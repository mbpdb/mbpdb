{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2825e76f-464c-4bc7-9897-1405534aa850",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23025,
     "status": "ok",
     "timestamp": 1712094448807,
     "user": {
      "displayName": "Russell Kuhfeld",
      "userId": "14760569517288879712"
     },
     "user_tz": 420
    },
    "id": "278kpom78fOP",
    "outputId": "95f300e9-4d05-4fe1-a725-f5c3eea6cf80",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import pandas as pd\n",
    "import csv, json, re, os, sys, math, base64, io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd1d10b6-d743-41a6-acdf-f843ce447d74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Create the DataManager class and update function\n",
    "class DataManager:\n",
    "    def __init__(self):\n",
    "        self.merged_df = None\n",
    "        self.group_data = None\n",
    "        \n",
    "    def set_merged_df(self, df):\n",
    "        self.merged_df = df\n",
    "        \n",
    "    def set_group_data(self, data):\n",
    "        self.group_data = data\n",
    "        \n",
    "    def has_merged_data(self):\n",
    "        return self.merged_df is not None and not self.merged_df.empty\n",
    "        \n",
    "    def has_group_data(self):\n",
    "        return self.group_data is not None and len(self.group_data) > 0\n",
    "        \n",
    "    def has_all_data(self):\n",
    "        return self.has_merged_data() and self.has_group_data()\n",
    "\n",
    "# Create the data manager instance\n",
    "data_manager = DataManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932c11e1-ea25-4348-94c8-af1d357ed25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0863a243b89b44c9a6e3a9f2ca6c9275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='<h2>Data Upload</h2>'), FileUpload(value=(), accept='.csv,.txt,.tsv,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_group_data(json_data):\n",
    "    \"\"\"Process and validate the group data, setting defaults if needed\"\"\"\n",
    "    try:\n",
    "        # If group 2 doesn't have a grouping_variable, set it to 'New_Formula'\n",
    "        if '2' in json_data and 'grouping_variable' not in json_data['2']:\n",
    "            json_data['2']['grouping_variable'] = 'New_Formula'\n",
    "            \n",
    "        # Validate that each group has the required fields\n",
    "        for group_id, group_info in json_data.items():\n",
    "            if 'grouping_variable' not in group_info:\n",
    "                raise ValueError(f\"Group {group_id} missing grouping_variable\")\n",
    "            if 'abundance_columns' not in group_info:\n",
    "                raise ValueError(f\"Group {group_id} missing abundance_columns\")\n",
    "                \n",
    "        return json_data\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing group data: {str(e)}\")\n",
    "\n",
    "def load_data_files():\n",
    "    \"\"\"\n",
    "    Create file upload widgets for merged data and group definition files.\n",
    "    Returns widgets and registers callbacks for data loading.\n",
    "    \"\"\"\n",
    "    # Create file upload widgets\n",
    "    merged_uploader = widgets.FileUpload(\n",
    "        accept='.csv,.txt,.tsv,.xlsx',\n",
    "        multiple=False,\n",
    "        description='Upload Merged Data File',\n",
    "        layout=widgets.Layout(width='300px'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    group_uploader = widgets.FileUpload(\n",
    "        accept='.json',\n",
    "        multiple=False,\n",
    "        description='Upload Group Definition',\n",
    "        layout=widgets.Layout(width='300px'),\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    def on_merged_upload_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with output_area:\n",
    "                output_area.clear_output()\n",
    "                uploaded_files = change['new']\n",
    "                if len(uploaded_files) > 0:\n",
    "                    file_data = uploaded_files[0]\n",
    "                    merged_df = load_merged_data(file_data)\n",
    "                    if merged_df is not None:\n",
    "                        data_manager.set_merged_df(merged_df)\n",
    "                        display(HTML(f'<b style=\"color:green;\">Merged data imported with {merged_df.shape[0]} rows and {merged_df.shape[1]} columns.</b>'))\n",
    "                        update_visualizations()\n",
    "    \n",
    "    def on_group_upload_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with output_area:\n",
    "                output_area.clear_output()\n",
    "                uploaded_files = change['new']\n",
    "                if len(uploaded_files) > 0:\n",
    "                    file_data = uploaded_files[0]\n",
    "                    try:\n",
    "                        # Convert memoryview to bytes then to string\n",
    "                        content = bytes(file_data.content).decode('utf-8')\n",
    "                        group_data_raw = json.loads(content)\n",
    "                        \n",
    "                        # Process and validate group data\n",
    "                        processed_group_data = process_group_data(group_data_raw)\n",
    "                        data_manager.set_group_data(processed_group_data)\n",
    "                        \n",
    "                        display(HTML(f'<b style=\"color:green;\">Group definition file imported successfully with {len(processed_group_data)} groups.</b>'))\n",
    "                        print(\"Loaded groups:\", [g['grouping_variable'] for g in processed_group_data.values()])\n",
    "                        update_visualizations()\n",
    "                    except Exception as e:\n",
    "                        display(HTML(f'<b style=\"color:red;\">Error loading group definition file: {str(e)}</b>'))\n",
    "                        print(f\"Detailed error: {str(e)}\")\n",
    "                        print(f\"Type of content: {type(file_data.content)}\")\n",
    "    \n",
    "    merged_uploader.observe(on_merged_upload_change, names='value')\n",
    "    group_uploader.observe(on_group_upload_change, names='value')\n",
    "    \n",
    "    return merged_uploader, group_uploader, output_area\n",
    "\n",
    "def load_merged_data(file_obj):\n",
    "    \"\"\"\n",
    "    Load and validate the merged data file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        content = bytes(file_obj.content)  # Convert memoryview to bytes\n",
    "        filename = file_obj.name\n",
    "        extension = filename.split('.')[-1].lower()\n",
    "        \n",
    "        file_stream = io.BytesIO(content)\n",
    "        \n",
    "        if extension == 'csv':\n",
    "            df = pd.read_csv(file_stream)\n",
    "        elif extension in ['txt', 'tsv']:\n",
    "            df = pd.read_csv(file_stream, delimiter='\\t')\n",
    "        elif extension == 'xlsx':\n",
    "            df = pd.read_excel(file_stream)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format.\")\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_columns = ['Master Protein Accessions', 'Positions in Proteins']\n",
    "        if not set(required_columns).issubset(df.columns):\n",
    "            missing = set(required_columns) - set(df.columns)\n",
    "            raise ValueError(f\"Missing required columns: {', '.join(missing)}\")\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        display(HTML(f'<b style=\"color:red;\">Error loading data: {str(e)}</b>'))\n",
    "        return None\n",
    "        \n",
    "def setup_correlation_exports():\n",
    "    \"\"\"\n",
    "    Setup correlation analysis exports with styled download buttons\n",
    "    \"\"\"\n",
    "    if not data_manager.has_all_data():\n",
    "        display(HTML(f'<b style=\"color:blue;\">Please load both data files to enable export functionality.</b>'))\n",
    "        return\n",
    "     \n",
    "    def generate_download_link(content, filename, filetype='text/csv'):\n",
    "        \"\"\"Generate a download link for any content\"\"\"\n",
    "        if isinstance(content, pd.DataFrame):\n",
    "            if filetype == 'text/csv':\n",
    "                content = content.to_csv(index=False)  # Remove index only for CSV\n",
    "            else:\n",
    "                content = content.to_csv(index=True)  # Keep index for other formats\n",
    "        if isinstance(content, str):\n",
    "            content = content.encode()\n",
    "        b64 = base64.b64encode(content).decode()\n",
    "        return f\"\"\"\n",
    "            <a download=\"{filename}\" href=\"data:{filetype};base64,{b64}\" class=\"download-link\" \n",
    "               title=\"Click to download\">\n",
    "                Download {filename}\n",
    "            </a>\n",
    "        \"\"\"\n",
    "\n",
    "    def calculate_all_correlations():\n",
    "        \"\"\"Calculate correlations between all group pairs\"\"\"\n",
    "        correlation_results = []\n",
    "        Avg_columns = {\n",
    "            group_info['grouping_variable']: f\"Avg_{group_info['grouping_variable']}\"\n",
    "            for group_info in group_data.values()\n",
    "            if f\"Avg_{group_info['grouping_variable']}\" in df.columns\n",
    "        }\n",
    "        \n",
    "        for (group1, col1), (group2, col2) in combinations(Avg_columns.items(), 2):\n",
    "            # Filter for positive values and calculate correlation\n",
    "            mask = (df[col1] > 0) & (df[col2] > 0)\n",
    "            if mask.sum() > 1:\n",
    "                log_col1 = np.log10(df.loc[mask, col1])\n",
    "                log_col2 = np.log10(df.loc[mask, col2])\n",
    "                correlation = log_col1.corr(log_col2)\n",
    "                correlation = round(correlation, 3)\n",
    "                correlation_results.append((group1, group2, correlation))\n",
    "        \n",
    "        return pd.DataFrame(correlation_results, columns=['Group 1', 'Group 2', 'Correlation'])\n",
    "\n",
    "    def calculate_group_correlations():\n",
    "        \"\"\"Calculate within-group correlations and statistics\"\"\"\n",
    "        correlation_dfs = {}\n",
    "        \n",
    "        for key, value in group_data.items():\n",
    "            grouping_variable = value['grouping_variable']\n",
    "            abundance_columns = value['abundance_columns']\n",
    "            \n",
    "            data = df[abundance_columns].copy()\n",
    "            data = data[data.gt(0).all(axis=1)]\n",
    "            \n",
    "            if len(data) > 1:\n",
    "                data = np.log10(data)\n",
    "                correlation_matrix = data.corr(method='pearson')\n",
    "                lower_triangle = correlation_matrix.where(\n",
    "                    np.tril(np.ones(correlation_matrix.shape), k=-1).astype(bool)\n",
    "                )\n",
    "                \n",
    "                pairs = []\n",
    "                values = []\n",
    "                for i in range(len(abundance_columns)):\n",
    "                    for j in range(i):\n",
    "                        pair_name = f\"{abundance_columns[j]} vs {abundance_columns[i]}\"\n",
    "                        pairs.append(pair_name)\n",
    "                        values.append(round(lower_triangle.iloc[i,j], 3))\n",
    "                \n",
    "                correlation_dfs[grouping_variable] = pd.Series(values)\n",
    "        \n",
    "        return correlation_dfs\n",
    "\n",
    "    # Calculate correlations\n",
    "    cross_group_correlations = calculate_all_correlations()\n",
    "    within_group_correlations = calculate_group_correlations()\n",
    "    \n",
    "    # Create Excel for within-group correlations\n",
    "    buffer = io.BytesIO()\n",
    "    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:\n",
    "        # Combine correlations into DataFrame with numeric index\n",
    "        combined_correlation_df = pd.concat(within_group_correlations, axis=1)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        min_values = combined_correlation_df.min().round(3)\n",
    "        max_values = combined_correlation_df.max().round(3)\n",
    "        mean_values = combined_correlation_df.mean().round(3)\n",
    "        \n",
    "        # Create summary DataFrame with statistics\n",
    "        summary_df = pd.DataFrame({\n",
    "            'Min': min_values,\n",
    "            'Max': max_values,\n",
    "            'Average': mean_values\n",
    "        }).T\n",
    "        \n",
    "        # Combine correlations and summary, keeping numeric index\n",
    "        combined_correlation_with_summary = pd.concat([combined_correlation_df, summary_df], axis=0)\n",
    "        \n",
    "        # Write summary sheet with numeric index\n",
    "        combined_correlation_with_summary.to_excel(writer, sheet_name='Summary')\n",
    "        \n",
    "        # Write individual group sheets\n",
    "        for key, value in group_data.items():\n",
    "            grouping_variable = value['grouping_variable']\n",
    "            abundance_columns = value['abundance_columns']\n",
    "            \n",
    "            if grouping_variable in within_group_correlations:\n",
    "                values = within_group_correlations[grouping_variable]\n",
    "                pairs = []\n",
    "                for i in range(len(abundance_columns)):\n",
    "                    for j in range(i):\n",
    "                        pair_name = f\"{abundance_columns[j]} vs {abundance_columns[i]}\"\n",
    "                        pairs.append(pair_name)\n",
    "                \n",
    "                group_df = pd.DataFrame({\n",
    "                    'Pair': pairs,\n",
    "                    'Correlation': values\n",
    "                })\n",
    "                group_df.to_excel(writer, sheet_name=grouping_variable, index=False)\n",
    "        \n",
    "        # Remove borders and adjust column widths\n",
    "        for sheet_name in writer.sheets:\n",
    "            worksheet = writer.sheets[sheet_name]\n",
    "            for row in worksheet.iter_rows():\n",
    "                for cell in row:\n",
    "                    cell.border = None\n",
    "            for column_cells in worksheet.columns:\n",
    "                max_length = max(len(str(cell.value)) if cell.value is not None else 0 \n",
    "                               for cell in column_cells)\n",
    "                worksheet.column_dimensions[column_cells[0].column_letter].width = max_length + 2\n",
    "\n",
    "    # Generate download sections\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Cross-group correlations CSV (without index)\n",
    "    csv_filename = f\"cross_group_correlations_{timestamp}.csv\"\n",
    "    csv_link = generate_download_link(cross_group_correlations, csv_filename)\n",
    "    \n",
    "    # Within-group correlations Excel (with index)\n",
    "    excel_filename = f\"within_group_correlations_{timestamp}.xlsx\"\n",
    "    excel_link = generate_download_link(\n",
    "        buffer.getvalue(), \n",
    "        excel_filename, \n",
    "        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "    )\n",
    "    \n",
    "    html_content = f\"\"\"\n",
    "    {style}\n",
    "    <div class=\"export-section\">\n",
    "        <h3>Cross-Group Correlations</h3>\n",
    "        <div class=\"export-description\">\n",
    "            Download correlations between different sample groups (CSV format)\n",
    "        </div>\n",
    "        {csv_link}\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"export-section\">\n",
    "        <h3>Within-Group Correlations</h3>\n",
    "        <div class=\"export-description\">\n",
    "            Download detailed correlation analysis for each group with summary statistics (Excel format)\n",
    "        </div>\n",
    "        {excel_link}\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(html_content))\n",
    "\n",
    "\n",
    "def plot_interactive_correlation():\n",
    "    \"\"\"\n",
    "    Creates an interactive correlation plot with simplified function filtering.\n",
    "    \"\"\"\n",
    "    if not data_manager.has_all_data():\n",
    "        display(HTML(f'<b style=\"color:blue;\">Please load both data files to begin visualization.</b>'))\n",
    "        return\n",
    "        \n",
    "    df = data_manager.merged_df.copy()\n",
    "    group_data = data_manager.group_data\n",
    "    \n",
    "    if group_data is not None:\n",
    "        group_names = [group_info['grouping_variable'] for group_info in group_data.values()]\n",
    "\n",
    "    # Initialize function options\n",
    "    function_options = ['All', 'Unknown']\n",
    "    if not df.empty and 'function' in df.columns:\n",
    "        all_functions = df['function'].dropna().unique()\n",
    "        function_counts = {}\n",
    "        \n",
    "        # Count occurrences of each function component\n",
    "        for func in df['function'].dropna():\n",
    "            components = [f.strip() for f in func.replace(';', ',').split(',')]\n",
    "            for component in components:\n",
    "                function_counts[component] = function_counts.get(component, 0) + 1\n",
    "        \n",
    "        # Filter for functions with more than 1 occurrence\n",
    "        valid_functions = [func for func, count in function_counts.items() if count > 1]\n",
    "        function_options.extend(sorted(valid_functions))\n",
    "\n",
    "    # Create widgets\n",
    "    # Create plotting widgets\n",
    "    group1_widget = widgets.Dropdown(\n",
    "        options=group_names,\n",
    "        description='Group 1:',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    group2_widget = widgets.Dropdown(\n",
    "        options=group_names,\n",
    "        description='Group 2:',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    \n",
    "    function_widget = widgets.Dropdown(\n",
    "        options=function_options,\n",
    "        description='Function:',\n",
    "        value='All',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    color_widget = widgets.ColorPicker(\n",
    "        concise=False,\n",
    "        description='Plot Color:',\n",
    "        value='#0072C6',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    xlabel_widget = widgets.Text(\n",
    "        description='X Label:',\n",
    "        placeholder='Enter x-axis label',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    ylabel_widget = widgets.Text(\n",
    "        description='Y Label:',\n",
    "        placeholder='Enter y-axis label',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "\n",
    "    # Output widgets\n",
    "    info_output = widgets.Output()\n",
    "    plot_output = widgets.Output()\n",
    "\n",
    "\n",
    "    def create_plot(change=None):\n",
    "        group1 = group1_widget.value\n",
    "        group2 = group2_widget.value\n",
    "        function_filter = function_widget.value\n",
    "        color = color_widget.value\n",
    "        xlabel = xlabel_widget.value\n",
    "        ylabel = ylabel_widget.value\n",
    "        \n",
    "        with plot_output:\n",
    "            plot_output.clear_output(wait=True)\n",
    "            \n",
    "            if group1 == group2:\n",
    "                display(HTML(f'<b style=\"color:red;\">Error: Please select different groups for comparison.</b>'))\n",
    "                return\n",
    "                \n",
    "            col1 = f\"Avg_{group1}\"\n",
    "            col2 = f\"Avg_{group2}\"\n",
    "            \n",
    "            if col1 not in df.columns or col2 not in df.columns:\n",
    "                display(HTML(f'<b style=\"color:red;\">Error: Skipping plot for {group1} vs {group2} as one or both columns are missing.</b>'))\n",
    "                return\n",
    "            \n",
    "            # Filter data based on selected function\n",
    "            working_df = df.copy()\n",
    "            if function_filter == 'Unknown':\n",
    "                working_df = working_df[working_df['function'].isna()]\n",
    "            elif function_filter != 'All':\n",
    "                # Filter for rows where the function contains the selected component\n",
    "                working_df = working_df[\n",
    "                    working_df['function'].fillna('').str.contains(\n",
    "                        function_filter, \n",
    "                        case=False, \n",
    "                        na=False\n",
    "                    )\n",
    "                ]\n",
    "            \n",
    "            # Filter for positive values\n",
    "            filtered_df = working_df[(working_df[col1] > 0) & (working_df[col2] > 0)].copy()\n",
    "            \n",
    "            if len(filtered_df) == 0:\n",
    "                display(HTML(f'<b style=\"color:red;\">No data points available for the selected combination.</b>'))\n",
    "                return\n",
    "            \n",
    "            # Update info display\n",
    "            with info_output:\n",
    "                info_output.clear_output()\n",
    "                print(f\"Displaying {len(filtered_df)} peptides\")\n",
    "                if function_filter != 'All':\n",
    "                    print(f\"Function filter: {function_filter}\")\n",
    "            \n",
    "            # Calculate log values\n",
    "            filtered_df['log_x'] = np.log10(filtered_df[col1])\n",
    "            filtered_df['log_y'] = np.log10(filtered_df[col2])\n",
    "            \n",
    "            # Calculate correlation\n",
    "            if len(filtered_df) > 1:\n",
    "                corr, _ = pearsonr(filtered_df['log_x'], filtered_df['log_y'])\n",
    "            else:\n",
    "                corr = float('nan')\n",
    "            \n",
    "            # Create scatter plot\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Add scatter points\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=filtered_df['log_x'],\n",
    "                y=filtered_df['log_y'],\n",
    "                mode='markers',\n",
    "                name='Data Points',\n",
    "                marker=dict(\n",
    "                    color=color,\n",
    "                    size=8,\n",
    "                    line=dict(\n",
    "                        color=color,\n",
    "                        width=1\n",
    "                    )\n",
    "                ),\n",
    "                hovertemplate=\n",
    "                f'<b>Peptide ID:</b> %{{customdata[2]}}<br>' +\n",
    "                f'<b>Function:</b> %{{customdata[3]}}<br>' +\n",
    "                f'<b>{group1}:</b> %{{customdata[0]:.2e}}<br>' +\n",
    "                f'<b>{group2}:</b> %{{customdata[1]:.2e}}<br>' +\n",
    "                '<extra></extra>',\n",
    "                customdata=np.column_stack((\n",
    "                    filtered_df[col1], \n",
    "                    filtered_df[col2],\n",
    "                    filtered_df['unique ID'],\n",
    "                    filtered_df['function'].fillna('Unknown')\n",
    "                ))\n",
    "            ))\n",
    "            \n",
    "            # Add trendline if we have enough points\n",
    "            if len(filtered_df) > 1:\n",
    "                z = np.polyfit(filtered_df['log_x'], filtered_df['log_y'], 1)\n",
    "                p = np.poly1d(z)\n",
    "                x_range = np.linspace(filtered_df['log_x'].min(), filtered_df['log_x'].max(), 100)\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=x_range,\n",
    "                    y=p(x_range),\n",
    "                    mode='lines',\n",
    "                    name='Trendline',\n",
    "                    line=dict(color=color, dash='dash'),\n",
    "                    hovertemplate='<extra></extra>'\n",
    "                ))\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                title=dict(\n",
    "                    text=f'Correlation Plot (r = {corr:.2f})',\n",
    "                    x=0.5,\n",
    "                    xanchor='center'\n",
    "                ),\n",
    "                xaxis_title=xlabel if xlabel else group1,\n",
    "                yaxis_title=ylabel if ylabel else group2,\n",
    "                xaxis=dict(\n",
    "                    ticktext=[f'10^{int(i)}' for i in range(int(np.floor(filtered_df['log_x'].min())), \n",
    "                                                          int(np.ceil(filtered_df['log_x'].max())) + 1)],\n",
    "                    tickvals=list(range(int(np.floor(filtered_df['log_x'].min())), \n",
    "                                      int(np.ceil(filtered_df['log_x'].max())) + 1)),\n",
    "                ),\n",
    "                yaxis=dict(\n",
    "                    ticktext=[f'10^{int(i)}' for i in range(int(np.floor(filtered_df['log_y'].min())), \n",
    "                                                          int(np.ceil(filtered_df['log_y'].max())) + 1)],\n",
    "                    tickvals=list(range(int(np.floor(filtered_df['log_y'].min())), \n",
    "                                      int(np.ceil(filtered_df['log_y'].max())) + 1)),\n",
    "                ),\n",
    "                showlegend=False,\n",
    "                width=800,\n",
    "                height=800,\n",
    "                template='simple_white',\n",
    "                hoverlabel=dict(\n",
    "                    bgcolor=\"white\",\n",
    "                    font_size=14,\n",
    "                    font_family=\"Arial\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Make aspect ratio equal\n",
    "            fig.update_layout(yaxis=dict(scaleanchor=\"x\", scaleratio=1))\n",
    "            \n",
    "            # Show plot\n",
    "            fig.show(config={\n",
    "                'displayModeBar': True,\n",
    "                'scrollZoom': True,\n",
    "                'modeBarButtonsToRemove': ['select2d', 'lasso2d']\n",
    "            })\n",
    "    \n",
    "    # Create widget container with vertical layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.VBox([\n",
    "            group1_widget,\n",
    "            group2_widget,\n",
    "            function_widget,\n",
    "            color_widget,\n",
    "            xlabel_widget,\n",
    "            ylabel_widget\n",
    "        ]),\n",
    "        info_output\n",
    "    ])\n",
    "    \n",
    "    # Observe widget changes\n",
    "    group1_widget.observe(create_plot, names='value')\n",
    "    group2_widget.observe(create_plot, names='value')\n",
    "    function_widget.observe(create_plot, names='value')\n",
    "    color_widget.observe(create_plot, names='value')\n",
    "    xlabel_widget.observe(create_plot, names='value')\n",
    "    ylabel_widget.observe(create_plot, names='value')\n",
    "    \n",
    "    # Display widgets and create initial plot\n",
    "    display(controls)\n",
    "    display(plot_output)\n",
    "    create_plot()\n",
    "    \n",
    "\n",
    "def update_visualizations():\n",
    "    \"\"\"\n",
    "    Update all visualizations when data changes\n",
    "    \"\"\"\n",
    "    plot_interactive_correlation()\n",
    "    setup_correlation_exports()\n",
    "\n",
    "# Cell 2: Define the initialization and display function\n",
    "def initialize_notebook():\n",
    "    # Initialize everything\n",
    "    merged_uploader, group_uploader, output_area = load_data_files()\n",
    "    \n",
    "    # Create container for visualization and export sections\n",
    "    visualization_output = widgets.Output()\n",
    "    export_output = widgets.Output()\n",
    "    \n",
    "    with visualization_output:\n",
    "        plot_interactive_correlation()\n",
    "    \n",
    "    with export_output:\n",
    "        setup_correlation_exports()\n",
    "    \n",
    "    # Create section headers\n",
    "    upload_header = widgets.HTML(\"<h2>Data Upload</h2>\")\n",
    "    plot_header = widgets.HTML(\"<h2>Correlation Plot</h2>\")\n",
    "    export_header = widgets.HTML(\"<h2>Export Options</h2>\")\n",
    "    \n",
    "    # Create upload section with both uploaders\n",
    "    upload_section = widgets.VBox([\n",
    "        upload_header,\n",
    "        merged_uploader,  # Direct widget, not wrapped in HTML\n",
    "        group_uploader,   # Direct widget, not wrapped in HTML\n",
    "        output_area\n",
    "    ])\n",
    "    \n",
    "    # Create main container\n",
    "    main_container = widgets.VBox([\n",
    "        upload_section,\n",
    "        plot_header,\n",
    "        visualization_output,\n",
    "        export_header,\n",
    "        export_output\n",
    "    ])\n",
    "    \n",
    "    # Display the main container\n",
    "    display(main_container)\n",
    "\n",
    "# Cell 3: Run everything\n",
    "# Create an instance of the data manager (if not already created)\n",
    "if 'data_manager' not in globals():\n",
    "    data_manager = DataManager()\n",
    "\n",
    "# Initialize the notebook interface\n",
    "initialize_notebook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
