{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278kpom78fOP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23025,
     "status": "ok",
     "timestamp": 1712094448807,
     "user": {
      "displayName": "Russell Kuhfeld",
      "userId": "14760569517288879712"
     },
     "user_tz": 420
    },
    "id": "278kpom78fOP",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "95f300e9-4d05-4fe1-a725-f5c3eea6cf80",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Install/Import packages & define key varribles and functions\n",
    "# Run install script\n",
    "# %chmod +x setup_jupyterlab.sh\n",
    "# %./setup_jupyterlab.sh\n",
    "\n",
    "# Import necessary libraries for the script to function.\n",
    "import pandas as pd\n",
    "import tempfile, csv, json, re, os, shutil, io, base64, time, subprocess, sqlite3, zipfile, base64\n",
    "from io import StringIO, BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from django.conf import settings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.formula.api import ols\n",
    "#from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import warnings\n",
    "\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import combinations\n",
    "from ipydatagrid import DataGrid\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "import traitlets\n",
    "from traitlets import HasTraits, Instance, observe\n",
    "\n",
    "# Global variable declaration\n",
    "\n",
    "import _settings as settings\n",
    "global spec_translate_list\n",
    "spec_translate_list = settings.SPEC_TRANSLATE_LIST\n",
    "# Set the default font to Calibri\n",
    "#matplotlib.rcParams['font.family'] = 'Calibri'\n",
    "\n",
    "def find_species(header, spec_translate_list):\n",
    "    \"\"\"Search for a species in the header and return the first element (species name) from the list.\"\"\"\n",
    "    header_lower = header.lower()\n",
    "    for spec_group in spec_translate_list:\n",
    "        for term in spec_group[1:]:  # Iterate over possible species names/terms except the first element\n",
    "            if term.lower() in header_lower:\n",
    "                return spec_group[0]  # Return the first element of the list (main species name)\n",
    "    return \"unknown\"  # Return unknown if no species match is found\n",
    "\n",
    "def parse_headers():\n",
    "    fasta_dict = {}\n",
    "    with open(\"protein_headers.txt\", 'r') as file:\n",
    "        protein_id = \"\"\n",
    "        protein_name = \"\"\n",
    "        species = \"\"\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if protein_id:\n",
    "                    # Save the previous protein entry in the dictionary\n",
    "                    fasta_dict[protein_id] = {\n",
    "                        \"name\": protein_name,\n",
    "                        \"species\": species\n",
    "                    }\n",
    "                sequence = \"\"\n",
    "                header_parts = line[1:].split('|')\n",
    "                if len(header_parts) > 2:\n",
    "                    protein_id = header_parts[1]\n",
    "                    protein_name_full = re.split(r' OS=', header_parts[2])[0]\n",
    "                    if ' ' in protein_name_full:\n",
    "\n",
    "                        protein_name = protein_name_full#.split()[1]\n",
    "                    else:\n",
    "                        protein_name = protein_name_full\n",
    "                    # Find species in the header\n",
    "                    species = find_species(line, spec_translate_list)\n",
    "\n",
    "        if protein_id:\n",
    "            # Save the last protein entry in the dictionary\n",
    "            fasta_dict[protein_id] = {\n",
    "                \"name\": protein_name,\n",
    "                \"species\": species\n",
    "            }\n",
    "    return fasta_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d06c8cd-9a5f-45cb-bf09-1a96f1eb77bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DataTransformation(HasTraits):\n",
    "    pd_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    mbpdb_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    #pd_results_cleaned = Instance(pd.DataFrame, allow_none=True)\n",
    "    search_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pd_results = pd.DataFrame()\n",
    "        #self.pd_results_cleaned = pd.DataFrame()\n",
    "        self.mbpdb_results = pd.DataFrame()\n",
    "        self.search_results = pd.DataFrame()\n",
    "        self.proteins_dic = parse_headers()\n",
    "        self.output_area = None\n",
    "        self.mbpdb_uploader = None\n",
    "        self.pd_uploader = None\n",
    "        self.fasta_uploader = None\n",
    "        self.reset_button = None\n",
    "        self.search_widget = None\n",
    "        self.search_progress = None\n",
    "             \n",
    "    def setup_search_ui(self, peptides):\n",
    "        \"\"\"Initialize and display the search UI\"\"\"\n",
    "        # Create dropdown for similarity threshold\n",
    "        self.threshold_dropdown = widgets.Dropdown(\n",
    "            options=list(range(0, 101, 10)),\n",
    "            value=80,\n",
    "            description='Similarity Threshold:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        # Create search button\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search Peptides',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        \n",
    "        # Progress indicator\n",
    "        self.search_progress = widgets.HTML(\n",
    "            value=\"\",\n",
    "            layout=widgets.Layout(margin='10px 0px')\n",
    "        )\n",
    "        \n",
    "        # Connect button click to handler\n",
    "        self.search_button.on_click(lambda b: self._on_search_click(b,))\n",
    "        \n",
    "        # Create layout\n",
    "        self.search_widget = widgets.VBox([\n",
    "            widgets.HBox([\n",
    "                self.threshold_dropdown, \n",
    "                self.search_button\n",
    "            ], layout=widgets.Layout(align_items='center')),\n",
    "            self.search_progress\n",
    "        ])\n",
    "        \n",
    "        display(self.search_widget)\n",
    "\n",
    "    def _on_search_click(self, b):\n",
    "        \"\"\"Handle search button click\"\"\"\n",
    "        with self.search_output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            if self.pd_results is None or self.pd_results.empty:\n",
    "                display(HTML(\"<b style='color:red'>Please upload peptidomic data first.</b>\"))\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                # Extract sequences from peptidomic data\n",
    "                peptides = self._extract_sequences(self.pd_results)\n",
    "                \n",
    "                if not peptides:\n",
    "                    display(HTML(\"<b style='color:red'>No valid sequences found in peptidomic data.</b>\"))\n",
    "                    return\n",
    "                    \n",
    "                display(HTML(f\"<b style='color:blue'>Found {len(peptides)} sequences. Searching database...</b>\"))\n",
    "                \n",
    "                # Perform search\n",
    "                results = self._search_peptides_comprehensive(\n",
    "                    peptides, \n",
    "                    similarity_threshold=self.threshold_dropdown.value\n",
    "                )          \n",
    "                # Format results if we have any matches\n",
    "                if not results.empty:\n",
    "                    self.mbpdb_results = self._format_search_results_with_matches(results)\n",
    "                    display(HTML(f\"<b style='color:green'>Search complete! Found {len(self.mbpdb_results)} matches</b>\"))\n",
    "                else:\n",
    "                    self.mbpdb_results = results\n",
    "                    display(HTML(\"<b style='color:orange'>No matches found in the database.</b>\"))\n",
    "                \n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red'>Error: {str(e)}</b>\"))\n",
    "                self.mbpdb_results = pd.DataFrame()\n",
    "    \n",
    "    def _search_peptides_comprehensive(self, peptides, similarity_threshold=100):\n",
    "        \"\"\"Search for peptides with BLAST-based similarity matching\"\"\"\n",
    "        \n",
    "        #WORK_DIRECTORY = '/home/kuhfeldrf/mbpdb/include/peptide/uploads/temp'\n",
    "        #conn = sqlite3.connect('/home/kuhfeldrf/mbpdb/include/peptide/db.sqlite3')\n",
    "        \n",
    "        WORK_DIRECTORY = '../../uploads/temp'\n",
    "        conn = sqlite3.connect('../../db.sqlite3')\n",
    "        work_path = self._create_work_directory(WORK_DIRECTORY)\n",
    "        \n",
    "        fasta_db_path = os.path.join(work_path, \"db.fasta\")\n",
    "        results = []\n",
    "        extra_info = defaultdict(list)\n",
    "        \n",
    "        # Create database with all peptides for BLAST\n",
    "        query = \"SELECT p.id, p.peptide FROM peptide_peptideinfo p\"\n",
    "        db_peptides = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        # Create BLAST database\n",
    "        with open(fasta_db_path, 'w') as f:\n",
    "            for _, row in db_peptides.iterrows():\n",
    "                f.write(f\">{row['id']}\\n{row['peptide']}\\n\")\n",
    "                \n",
    "        self._make_blast_db(fasta_db_path)\n",
    "        \n",
    "        for peptide in peptides:\n",
    "            if similarity_threshold == 100:\n",
    "                query = \"\"\"\n",
    "                SELECT DISTINCT\n",
    "                    ? as search_peptide,\n",
    "                    pi.pid as protein_id,\n",
    "                    p.id as peptide_id,\n",
    "                    p.peptide,\n",
    "                    pi.desc as protein_description,\n",
    "                    pi.species,\n",
    "                    p.intervals,\n",
    "                    f.function,\n",
    "                    r.additional_details,\n",
    "                    r.ic50,\n",
    "                    r.inhibition_type,\n",
    "                    r.inhibited_microorganisms,\n",
    "                    r.ptm,\n",
    "                    r.title,\n",
    "                    r.authors,\n",
    "                    r.abstract,\n",
    "                    r.doi,\n",
    "                    'sequence' as search_type,\n",
    "                    'IDENTITY' as scoring_matrix\n",
    "                FROM peptide_peptideinfo p\n",
    "                JOIN peptide_proteininfo pi ON p.protein_id = pi.id\n",
    "                LEFT JOIN peptide_function f ON f.pep_id = p.id\n",
    "                LEFT JOIN peptide_reference r ON r.func_id = f.id\n",
    "                WHERE p.peptide = ?\n",
    "                \"\"\"\n",
    "                df = pd.read_sql_query(query, conn, params=[peptide, peptide])\n",
    "                results.append(df)\n",
    "            else:\n",
    "                # Run BLASTP search for similarity matching\n",
    "                query_path = os.path.join(work_path, \"query.fasta\")\n",
    "                with open(query_path, \"w\") as query_file:\n",
    "                    query_file.write(f\">pep_query\\n{peptide}\\n\")\n",
    "                    \n",
    "                output_path = os.path.join(work_path, \"blastp_short.out\")\n",
    "                blast_args = [\n",
    "                    \"blastp\",\n",
    "                    \"-query\", query_path,\n",
    "                    \"-db\", fasta_db_path,\n",
    "                    \"-outfmt\", \"6 std ppos qcovs qlen slen positive\",\n",
    "                    \"-evalue\", \"1000\",\n",
    "                    \"-word_size\", \"2\",\n",
    "                    \"-matrix\", \"IDENTITY\",\n",
    "                    \"-threshold\", \"1\",\n",
    "                    \"-task\", \"blastp-short\",\n",
    "                    \"-out\", output_path\n",
    "                ]\n",
    "                \n",
    "                subprocess.check_output(blast_args, stderr=subprocess.STDOUT)\n",
    "                \n",
    "                # Process BLAST results\n",
    "                search_ids = self._process_blast_results(output_path, similarity_threshold, extra_info)\n",
    "                \n",
    "                if search_ids:\n",
    "                    df = self._fetch_peptide_data(conn, peptide, search_ids)\n",
    "                    self._add_blast_details(df, extra_info)\n",
    "                    results.append(df)\n",
    "        \n",
    "        conn.close()\n",
    "        self._cleanup_work_directory(WORK_DIRECTORY)\n",
    "        \n",
    "        return self._combine_results(results)\n",
    "    \n",
    "    def _create_work_directory(self, base_dir):\n",
    "        \"\"\"Create a working directory for BLAST operations\"\"\"\n",
    "        path = os.path.join(base_dir, f'work_{int(round(time.time() * 1000))}')\n",
    "        os.makedirs(path)\n",
    "        return path\n",
    "    \n",
    "    def _make_blast_db(self, library_fasta_path):\n",
    "        \"\"\"Create BLAST database from FASTA file\"\"\"\n",
    "        subprocess.check_output(\n",
    "            ['makeblastdb', '-in', library_fasta_path, '-dbtype', 'prot'],\n",
    "            stderr=subprocess.STDOUT\n",
    "        )\n",
    "    \n",
    "    def _process_blast_results(self, output_path, similarity_threshold, extra_info):\n",
    "        \"\"\"Process BLAST results and collect search IDs\"\"\"\n",
    "        search_ids = []\n",
    "        csv.register_dialect('blast_dialect', delimiter='\\t')\n",
    "        \n",
    "        with open(output_path, \"r\") as output_file:\n",
    "            blast_data = csv.DictReader(\n",
    "                output_file,\n",
    "                fieldnames=['query', 'subject', 'percid', 'align_len', 'mismatches', \n",
    "                           'gaps', 'qstart', 'qend', 'sstart', 'send', 'evalue', \n",
    "                           'bitscore', 'ppos', 'qcov', 'qlen', 'slen', 'numpos'],\n",
    "                dialect='blast_dialect'\n",
    "            )\n",
    "            \n",
    "            for row in blast_data:\n",
    "                tlen = float(row['slen']) if float(row['slen']) > float(row['qlen']) else float(row['qlen'])\n",
    "                simcalc = 100 * ((float(row['numpos']) - float(row['gaps'])) / tlen)\n",
    "                \n",
    "                if simcalc >= similarity_threshold:\n",
    "                    search_ids.append(row['subject'])\n",
    "                    extra_info[row['subject']] = [\n",
    "                        f\"{simcalc:.2f}\", row['qstart'], row['qend'], row['sstart'],\n",
    "                        row['send'], row['evalue'], row['align_len'], row['mismatches'],\n",
    "                        row['gaps']\n",
    "                    ]\n",
    "        \n",
    "        return search_ids\n",
    "    \n",
    "    def _fetch_peptide_data(self, conn, peptide, search_ids):\n",
    "        \"\"\"Fetch peptide data from database\"\"\"\n",
    "        placeholders = ','.join(['?' for _ in search_ids])\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            ? as search_peptide,\n",
    "            pi.pid as protein_id,\n",
    "            p.id as peptide_id,\n",
    "            p.peptide,\n",
    "            pi.desc as protein_description,\n",
    "            pi.species,\n",
    "            p.intervals,\n",
    "            f.function,\n",
    "            r.additional_details,\n",
    "            r.ic50,\n",
    "            r.inhibition_type,\n",
    "            r.inhibited_microorganisms,\n",
    "            r.ptm,\n",
    "            r.title,\n",
    "            r.authors,\n",
    "            r.abstract,\n",
    "            r.doi,\n",
    "            'sequence' as search_type,\n",
    "            'IDENTITY' as scoring_matrix\n",
    "        FROM peptide_peptideinfo p\n",
    "        JOIN peptide_proteininfo pi ON p.protein_id = pi.id\n",
    "        LEFT JOIN peptide_function f ON f.pep_id = p.id\n",
    "        LEFT JOIN peptide_reference r ON r.func_id = f.id\n",
    "        WHERE p.id IN ({placeholders})\n",
    "        \"\"\"\n",
    "        \n",
    "        return pd.read_sql_query(query, conn, params=[peptide] + search_ids)\n",
    "    \n",
    "    def _add_blast_details(self, df, extra_info):\n",
    "        \"\"\"Add BLAST details to DataFrame\"\"\"\n",
    "        for idx, row in df.iterrows():\n",
    "            if str(row['peptide_id']) in extra_info:\n",
    "                blast_details = extra_info[str(row['peptide_id'])]\n",
    "                df.at[idx, '% Alignment'] = blast_details[0]\n",
    "                df.at[idx, 'Query start'] = blast_details[1]\n",
    "                df.at[idx, 'Query end'] = blast_details[2]\n",
    "                df.at[idx, 'Subject start'] = blast_details[3]\n",
    "                df.at[idx, 'Subject end'] = blast_details[4]\n",
    "                df.at[idx, 'e-value'] = blast_details[5]\n",
    "                df.at[idx, 'Alignment length'] = blast_details[6]\n",
    "                df.at[idx, 'Mismatches'] = blast_details[7]\n",
    "                df.at[idx, 'Gap opens'] = blast_details[8]\n",
    "    \n",
    "    def _cleanup_work_directory(self, work_directory):\n",
    "        \"\"\"Clean up old work directories\"\"\"\n",
    "        try:\n",
    "            dirs = [f for f in os.scandir(work_directory) if f.is_dir()]\n",
    "            dirs.sort(key=lambda x: os.path.getmtime(x.path), reverse=True)\n",
    "            \n",
    "            for dir_entry in dirs[25:]:\n",
    "                try:\n",
    "                    shutil.rmtree(dir_entry.path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    def _combine_results(self, results):\n",
    "        \"\"\"Combine and format final results\"\"\"\n",
    "        if not results:\n",
    "            mbpdb_columns = [\n",
    "                'search_peptide', 'protein_id', 'peptide', 'protein_description',\n",
    "                'species', 'intervals', 'function', 'additional_details', 'ic50',\n",
    "                'inhibition_type', 'inhibited_microorganisms', 'ptm', 'title',\n",
    "                'authors', 'abstract', 'doi', 'search_type', 'scoring_matrix'\n",
    "            ]\n",
    "            return pd.DataFrame(columns=mbpdb_columns)\n",
    "        \n",
    "        final_results = pd.concat(results, ignore_index=True)\n",
    "        \n",
    "        if 'peptide_id' in final_results.columns:\n",
    "            final_results = final_results.drop('peptide_id', axis=1)\n",
    "            \n",
    "        sort_columns = ['search_peptide']\n",
    "        if '% Alignment' in final_results.columns:\n",
    "            sort_columns.append('% Alignment')\n",
    "            \n",
    "        return final_results.sort_values(\n",
    "            sort_columns,\n",
    "            ascending=[True] + [False] * (len(sort_columns) - 1)\n",
    "        )\n",
    "    \n",
    "    def _format_search_results_with_matches(self, final_results):\n",
    "        \"\"\"Format search results with matches\"\"\"\n",
    "        if '% Alignment' in final_results.columns:\n",
    "            final_results['% Alignment'] = pd.to_numeric(\n",
    "                final_results['% Alignment'], \n",
    "                errors='coerce'\n",
    "            )\n",
    "\n",
    "        grouped = final_results.groupby([\"search_peptide\", \"function\"], as_index=False)\n",
    "        aggregated_results = []\n",
    "        processed_indices = set()\n",
    "\n",
    "        for _, group in grouped:\n",
    "            if len(group) > 1:\n",
    "                aggregated_row = self._aggregate_group_data(group)\n",
    "                aggregated_results.append(aggregated_row)\n",
    "                processed_indices.update(group.index)\n",
    "\n",
    "        remaining_rows = final_results.loc[~final_results.index.isin(processed_indices)]\n",
    "        aggregated_df = pd.DataFrame(aggregated_results)\n",
    "        \n",
    "        return pd.concat([aggregated_df, remaining_rows], ignore_index=True)\n",
    "    \n",
    "    def _aggregate_group_data(self, group):\n",
    "        \"\"\"Aggregate data for a group of results\"\"\"\n",
    "        def enumerate_field(field):\n",
    "            if field in group.columns and not group[field].dropna().empty:\n",
    "                valid_values = set(group[field].dropna().astype(str).str.strip())\n",
    "                valid_values = {val for val in valid_values if val != ''}\n",
    "                if len(valid_values) > 1:\n",
    "                    return \"; \".join([f\"{i+1}) {val}\" for i, val in enumerate(valid_values)])\n",
    "                elif len(valid_values) == 1:\n",
    "                    return next(iter(valid_values))\n",
    "                return ''\n",
    "            return ''\n",
    "\n",
    "        return {col: enumerate_field(col) for col in group.columns}   \n",
    "                \n",
    "    def setup_data_loading_ui(self):\n",
    "        \"\"\"Initialize and display the data loading UI with integrated search and help tooltips\"\"\"\n",
    "        \n",
    "        def create_help_icon(tooltip_text):\n",
    "            \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "            help_icon = widgets.HTML(\n",
    "                value='<i class=\"fa fa-question-circle\" style=\"color: #007bff;\"></i>',\n",
    "                layout=widgets.Layout(width='25px', margin='2px 5px')\n",
    "            )\n",
    "            help_icon.add_class('jupyter-widgets')\n",
    "            help_icon.add_class('widget-html')\n",
    "            return widgets.HTML(\n",
    "                f'<div title=\"{tooltip_text}\" style=\"display: inline-block;\">{help_icon.value}</div>'\n",
    "            )\n",
    "    \n",
    "        def create_labeled_uploader(widget, label, tooltip):\n",
    "            \"\"\"Create an uploader with label and help icon\"\"\"\n",
    "            return widgets.HBox([\n",
    "                widget,\n",
    "                create_help_icon(tooltip)\n",
    "            ], layout=widgets.Layout(align_items='center'))\n",
    "    \n",
    "        # Create file upload widgets with the same configurations\n",
    "        self.mbpdb_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload MBPDB File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.pd_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload Peptidomic File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.fasta_uploader = widgets.FileUpload(\n",
    "            accept='.fasta',\n",
    "            multiple=True,\n",
    "            description='Upload FASTA Files',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Create search interface\n",
    "        self.threshold_dropdown = widgets.Dropdown(\n",
    "            options=list(range(0, 101, 10)),\n",
    "            value=80,\n",
    "            description='Similarity Threshold (%):',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='225px')\n",
    "        )\n",
    "        \n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search Database',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        \n",
    "        # Reset button\n",
    "        self.reset_button = widgets.Button(\n",
    "            description='Reset',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='100px')\n",
    "        )\n",
    "        \n",
    "        # Create output areas\n",
    "        self.output_area = widgets.Output()\n",
    "        self.search_output_area = widgets.Output()\n",
    "    \n",
    "        # Create the MBPDB section with side-by-side options\n",
    "        mbpdb_options = widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(\"<div style='font-weight: bold; margin-bottom: 5px;'>Option 1: Upload File</div>\"),\n",
    "                create_labeled_uploader(\n",
    "                    self.mbpdb_uploader,\n",
    "                    \"MBPDB File\",\n",
    "                    \"Upload your own MBPDB file (optional)\"\n",
    "                )\n",
    "            ]),\n",
    "            widgets.HTML(\"<div style='margin: 0 20px; line-height: 100px;'>OR</div>\"),\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(\"<div style='font-weight: bold; margin-bottom: 5px;'>Option 2: Search Database</div>\"),\n",
    "                widgets.HBox([\n",
    "                    self.threshold_dropdown,\n",
    "                    self.search_button,\n",
    "                    create_help_icon(\"Search peptides against the MBPDB (optional)\")\n",
    "                ], layout=widgets.Layout(align_items='center'))\n",
    "            ])\n",
    "        ], layout=widgets.Layout(align_items='center', margin='-10px 0 0 0'))        \n",
    "        # Create main container\n",
    "        main_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Upload Peptidomic Data Files:</u></h3>\"),\n",
    "            create_labeled_uploader(\n",
    "                self.pd_uploader,\n",
    "                \"Peptidomic File\",\n",
    "                \"Upload peptide groups data from Proteome Discover export file (required)\"\n",
    "            ),\n",
    "            #widgets.HTML(\"<h3><u>MBPDB Data (Optional):</u></h3>\"),\n",
    "            mbpdb_options,\n",
    "            widgets.HTML(\"<h3><u>Upload Protein FASTA Files (Optional):</u></h3>\"),\n",
    "            create_labeled_uploader(\n",
    "                self.fasta_uploader,\n",
    "                \"FASTA Files\",\n",
    "                \"Upload Protein FASTA file used in Proteome Discoverer Search (optional - This helps label proteins in data transformation)\"\n",
    "            ),\n",
    "            widgets.HTML(\"<br>\"),\n",
    "            widgets.HBox([\n",
    "                self.reset_button,\n",
    "                create_help_icon(\"Reset all uploaded files\")\n",
    "            ], layout=widgets.Layout(align_items='center')),\n",
    "            widgets.HTML(\"<div style='margin-top: 10px;'></div>\"),  # Spacing\n",
    "            self.output_area,\n",
    "            self.search_output_area\n",
    "        ])\n",
    "        \n",
    "        # Register observers\n",
    "        self.pd_uploader.observe(self._on_pd_upload_change, names='value')\n",
    "        self.mbpdb_uploader.observe(self._on_mbpdb_upload_change, names='value')\n",
    "        self.fasta_uploader.observe(self._on_fasta_upload_change, names='value')\n",
    "        self.reset_button.on_click(self._reset_ui)\n",
    "        self.search_button.on_click(self._on_search_click())\n",
    "        \n",
    "        # Add Font Awesome CSS for help icons\n",
    "        display(widgets.HTML(\"\"\"\n",
    "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\">\n",
    "        \"\"\"))\n",
    "        \n",
    "        display(main_container)\n",
    "        \n",
    "    def setup_data_loading_ui(self):\n",
    "        \"\"\"Initialize and display the data loading UI with integrated search and help tooltips\"\"\"\n",
    "        \n",
    "        def create_help_icon(tooltip_text):\n",
    "            \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "            help_icon = widgets.HTML(\n",
    "                value='<i class=\"fa fa-question-circle\" style=\"color: #007bff;\"></i>',\n",
    "                layout=widgets.Layout(width='25px', margin='2px 5px')\n",
    "            )\n",
    "            help_icon.add_class('jupyter-widgets')\n",
    "            help_icon.add_class('widget-html')\n",
    "            return widgets.HTML(\n",
    "                f'<div title=\"{tooltip_text}\" style=\"display: inline-block;\">{help_icon.value}</div>'\n",
    "            )\n",
    "    \n",
    "        def create_labeled_uploader(widget, label, tooltip):\n",
    "            \"\"\"Create an uploader with label and help icon\"\"\"\n",
    "            return widgets.HBox([\n",
    "                widget,\n",
    "                create_help_icon(tooltip)\n",
    "            ], layout=widgets.Layout(align_items='center'))\n",
    "    \n",
    "        # Create file upload widgets with the same configurations\n",
    "        self.mbpdb_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload MBPDB File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.pd_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload Peptidomic File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.fasta_uploader = widgets.FileUpload(\n",
    "            accept='.fasta',\n",
    "            multiple=True,\n",
    "            description='Upload FASTA Files',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Create search interface\n",
    "        self.threshold_dropdown = widgets.Dropdown(\n",
    "            options=list(range(0, 101, 10)),\n",
    "            value=80,\n",
    "            description='Similarity Threshold (%):',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='225px')\n",
    "        )\n",
    "        \n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search Database',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        \n",
    "        # Reset button\n",
    "        self.reset_button = widgets.Button(\n",
    "            description='Reset',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='100px')\n",
    "        )\n",
    "        \n",
    "        # Create output areas\n",
    "        self.output_area = widgets.Output()\n",
    "        self.search_output_area = widgets.Output()\n",
    "    \n",
    "        # Create the MBPDB section with side-by-side options\n",
    "        mbpdb_options = widgets.HBox([\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(\"<div style='font-weight: bold; margin-bottom: 5px;'>Option 1: Upload File</div>\"),\n",
    "                create_labeled_uploader(\n",
    "                    self.mbpdb_uploader,\n",
    "                    \"MBPDB File\",\n",
    "                    \"Upload your own MBPDB file (optional)\"\n",
    "                )\n",
    "            ]),\n",
    "            widgets.HTML(\"<div style='margin: 0 20px; line-height: 100px;'><b>OR</b></div>\"),\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(\"<div style='font-weight: bold; margin-bottom: 5px;'>Option 2: Search Database</div>\"),\n",
    "                widgets.HBox([\n",
    "                    self.threshold_dropdown,\n",
    "                    self.search_button,\n",
    "                    create_help_icon(\"Search peptides against the MBPDB (optional)\")\n",
    "                ], layout=widgets.Layout(align_items='center'))\n",
    "            ])\n",
    "        ], layout=widgets.Layout(align_items='center', margin='0'))\n",
    "        \n",
    "        # Create main container\n",
    "        main_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Upload Peptidomic Data Files:</u></h3>\"),\n",
    "            create_labeled_uploader(\n",
    "                self.pd_uploader,\n",
    "                \"Peptidomic File\",\n",
    "                \"Upload peptide groups data from Proteome Discover export file (required)\"\n",
    "            ),\n",
    "            widgets.HTML(\"<h3 style='margin-bottom: 0;'><u>MBPDB Data (Optional):</u></h3>\"),\n",
    "            mbpdb_options,\n",
    "            widgets.HTML(\"<h3><u>Upload Protein FASTA Files (Optional):</u></h3>\"),\n",
    "            create_labeled_uploader(\n",
    "                self.fasta_uploader,\n",
    "                \"FASTA Files\",\n",
    "                \"Upload Protein FASTA file used in Proteome Discoverer Search (optional - This helps label proteins in data transformation)\"\n",
    "            ),\n",
    "            widgets.HTML(\"<br>\"),\n",
    "            widgets.HBox([\n",
    "                self.reset_button,\n",
    "                create_help_icon(\"Reset all uploaded files\")\n",
    "            ], layout=widgets.Layout(align_items='center')),\n",
    "            widgets.HTML(\"<div style='margin-top: 10px;'></div>\"),  # Spacing\n",
    "            self.output_area,\n",
    "            self.search_output_area\n",
    "        ])\n",
    "        \n",
    "        # Register observers\n",
    "        self.pd_uploader.observe(self._on_pd_upload_change, names='value')\n",
    "        self.mbpdb_uploader.observe(self._on_mbpdb_upload_change, names='value')\n",
    "        self.fasta_uploader.observe(self._on_fasta_upload_change, names='value')\n",
    "        self.reset_button.on_click(self._reset_ui)\n",
    "        self.search_button.on_click(self._on_search_click)\n",
    "        \n",
    "        # Add Font Awesome CSS for help icons\n",
    "        display(widgets.HTML(\"\"\"\n",
    "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\">\n",
    "        \"\"\"))\n",
    "        display(main_container)\n",
    "\n",
    "    \n",
    "    def _extract_sequences(self, df):\n",
    "        \"\"\"Extract sequences from peptidomic data\"\"\"\n",
    "        if 'Sequence' not in df.columns:\n",
    "            if 'Annotated Sequence' in df.columns:\n",
    "                sequences = df['Annotated Sequence'].str.split('.', expand=False).str[1]\n",
    "                df = df.assign(Sequence=sequences)\n",
    "            elif 'Positions in Proteins' in df.columns:  # Add any other potential column names\n",
    "                df['Sequence'] = df['Positions in Proteins']\n",
    "        return df['Sequence'].dropna().unique().tolist()\n",
    "    \n",
    "    \n",
    "    def _reset_ui(self, b):\n",
    "        \"\"\"Reset the UI state\"\"\"\n",
    "        self.mbpdb_uploader._counter = 0\n",
    "        self.pd_uploader._counter = 0\n",
    "        self.fasta_uploader._counter = 0\n",
    "        self.mbpdb_uploader.value = ()\n",
    "        self.pd_uploader.value = ()\n",
    "        self.fasta_uploader.value = ()\n",
    "        self.pd_results = pd.DataFrame()\n",
    "        self.mbpdb_results = pd.DataFrame()\n",
    "        self.search_results = pd.DataFrame()\n",
    "        self.proteins_dic = parse_headers()\n",
    "        \n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "            display(HTML('<b style=\"color:blue;\">All uploads cleared.</b>'))\n",
    "        \n",
    "        with self.search_output_area:\n",
    "            clear_output()\n",
    "            display(HTML('<b style=\"color:blue;\">Search results cleared.</b>')) \n",
    "            \n",
    "    def _on_pd_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    self.pd_results, pd_status = self._load_data(\n",
    "                        file_data,\n",
    "                        required_columns=['Positions in Proteins'],\n",
    "                        file_type='Peptidomic'\n",
    "                    )\n",
    "                    if pd_status == 'yes' and self.pd_results is not None:\n",
    "                        display(HTML(f'<b style=\"color:green;\">Peptidomic data imported with {self.pd_results.shape[0]} rows and {self.pd_results.shape[1]} columns.</b>'))\n",
    "\n",
    "    def _on_mbpdb_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    self.mbpdb_results, mbpdb_status = self._load_data(\n",
    "                        file_data,\n",
    "                        required_columns=['Search peptide', 'Protein ID', 'Peptide'],\n",
    "                        file_type='MBPDB'\n",
    "                    )\n",
    "                    if mbpdb_status == 'yes' and self.mbpdb_results is not None:\n",
    "                        self.mbpdb_results.rename(columns={\n",
    "                            'Search peptide': 'search_peptide',\n",
    "                            'Protein ID': 'protein_id',\n",
    "                            'Peptide': 'peptide',\n",
    "                            'Protein description': 'protein_description',\n",
    "                            'Species': 'species',\n",
    "                            'Intervals': 'intervals',\n",
    "                            'Function': 'function',\n",
    "                            'Additional details': 'additional_details',\n",
    "                            'IC50 (Î¼M)': 'ic50',\n",
    "                            'Inhibition type': 'inhibition_type',\n",
    "                            'Inhibited microorganisms': 'inhibited_microorganisms',\n",
    "                            'PTM': 'ptm',\n",
    "                            'Title': 'title',\n",
    "                            'Authors': 'authors',\n",
    "                            'Abstract': 'abstract',\n",
    "                            'DOI': 'doi',\n",
    "                            'Search type': 'search_type',\n",
    "                            'Scoring matrix': 'scoring_matrix',\n",
    "                            }, inplace=True)\n",
    "                        display(HTML(f'<b style=\"color:green;\">MBPDB file imported with {self.mbpdb_results.shape[0]} rows and {self.mbpdb_results.shape[1]} columns</b>'))\n",
    "    \n",
    "    def _on_fasta_upload_change(self, change):\n",
    "        \n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            self.proteins_dic = {}\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    for file_data in change['new']:\n",
    "                        try:\n",
    "                            file_name = getattr(file_data, 'name', None)\n",
    "                            if file_name and file_name.endswith('.fasta'):\n",
    "                                new_proteins = self._parse_uploaded_fasta(file_data)\n",
    "                                self.proteins_dic.update(new_proteins)\n",
    "                                display(HTML(f'<b style=\"color:green;\">Successfully imported FASTA file: {file_name} ({len(new_proteins)} proteins)</b>'))\n",
    "                        except Exception as e:\n",
    "                            display(HTML(f'<b style=\"color:red;\">Error processing FASTA file: {str(e)}</b>'))       \n",
    "    def _load_data(self, file_obj, required_columns, file_type):\n",
    "        \"\"\"\n",
    "        Load and validate uploaded data files, cleaning empty rows and validating data.\n",
    "        \n",
    "        Args:\n",
    "            file_obj: Uploaded file object\n",
    "            required_columns (list): List of required column names (either single names or pairs)\n",
    "            file_type (str): Type of file being loaded ('MBPDB' or 'Peptidomic')\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (DataFrame or None, status string 'yes'/'no')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            content = file_obj.content\n",
    "            filename = file_obj.name\n",
    "            extension = filename.split('.')[-1].lower()\n",
    "            \n",
    "            file_stream = io.BytesIO(content)\n",
    "            \n",
    "            # Load data based on file extension\n",
    "            if extension == 'csv':\n",
    "                df = pd.read_csv(file_stream)\n",
    "            elif extension in ['txt', 'tsv']:\n",
    "                df = pd.read_csv(file_stream, delimiter='\\t')\n",
    "            elif extension == 'xlsx':\n",
    "                df = pd.read_excel(file_stream)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Please upload .csv, .txt, .tsv, or .xlsx files.\")\n",
    "            \n",
    "            # Clean column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Drop empty rows\n",
    "            df = df.dropna(how='all')\n",
    "            df = df[~(df.astype(str).apply(lambda x: x.str.strip().eq('')).all(axis=1))]\n",
    "            \n",
    "            # Handle validation differently based on file type\n",
    "            if file_type == 'MBPDB':\n",
    "                # Use column pairs for MBPDB validation\n",
    "                column_pairs = {\n",
    "                    'Search peptide': 'search_peptide',\n",
    "                    'Protein ID': 'protein_id',\n",
    "                    'Peptide': 'peptide'\n",
    "                }\n",
    "                \n",
    "                # Check for required columns in either format\n",
    "                missing_pairs = []\n",
    "                for orig_col, std_col in column_pairs.items():\n",
    "                    if not (orig_col in df.columns or std_col in df.columns):\n",
    "                        missing_pairs.append(f\"'{orig_col}' or '{std_col}'\")\n",
    "                \n",
    "                if missing_pairs:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Missing required columns: {\", \".join(missing_pairs)}</b>'))\n",
    "                    return None, 'no'\n",
    "                \n",
    "                # Validate non-empty required columns\n",
    "                empty_pairs = []\n",
    "                for orig_col, std_col in column_pairs.items():\n",
    "                    col_to_check = orig_col if orig_col in df.columns else std_col\n",
    "                    if df[col_to_check].isna().all() or (df[col_to_check].astype(str).str.strip() == '').all():\n",
    "                        empty_pairs.append(f\"'{orig_col}' or '{std_col}'\")\n",
    "                \n",
    "                if empty_pairs:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Required columns are empty: {\", \".join(empty_pairs)}</b>'))\n",
    "                    return None, 'no'\n",
    "                    \n",
    "            else:\n",
    "                # Standard validation for other file types\n",
    "                if not set(required_columns).issubset(df.columns):\n",
    "                    missing = set(required_columns) - set(df.columns)\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Missing required columns: {\", \".join(missing)}</b>'))\n",
    "                    return None, 'no'\n",
    "                \n",
    "                # Validate non-empty required columns\n",
    "                empty_required = []\n",
    "                for col in required_columns:\n",
    "                    if df[col].isna().all() or (df[col].astype(str).str.strip() == '').all():\n",
    "                        empty_required.append(col)\n",
    "                \n",
    "                if empty_required:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Required columns are empty: {\", \".join(empty_required)}</b>'))\n",
    "                    return None, 'no'\n",
    "            \n",
    "            # Show success message\n",
    "            display(HTML(f'<b style=\"color:green;\">{file_type} file loaded successfully with {len(df)} rows after cleaning.</b>'))\n",
    "            \n",
    "            return df, 'yes'\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(HTML(f'<b style=\"color:red;\">{file_type} File Error: {str(e)}</b>'))\n",
    "            return None, 'no'\n",
    "    \n",
    "    def _parse_uploaded_fasta(self, file_data):\n",
    "        \"\"\"Parse uploaded FASTA file content\"\"\"\n",
    "        fasta_dict = {}\n",
    "        fasta_text = bytes(file_data.content).decode('utf-8')\n",
    "        lines = fasta_text.split('\\n')\n",
    "        \n",
    "        protein_id = \"\"\n",
    "        protein_name = \"\"\n",
    "        sequence = \"\"\n",
    "        species = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if protein_id:\n",
    "                    fasta_dict[protein_id] = {\n",
    "                        \"name\": protein_name,\n",
    "                        \"sequence\": sequence,\n",
    "                        \"species\": species\n",
    "                    }\n",
    "                sequence = \"\"\n",
    "                header_parts = line[1:].split('|')\n",
    "                if len(header_parts) > 2:\n",
    "                    protein_id = header_parts[1]\n",
    "                    protein_name_full = re.split(r' OS=', header_parts[2])[0]\n",
    "                    if ' ' in protein_name_full:\n",
    "                        protein_name = protein_name_full\n",
    "                    else:\n",
    "                        protein_name = protein_name_full\n",
    "                    species = self._find_species(line)\n",
    "            else:\n",
    "                sequence += line\n",
    "                \n",
    "        if protein_id:\n",
    "            fasta_dict[protein_id] = {\n",
    "                \"name\": protein_name,\n",
    "                \"sequence\": sequence,\n",
    "                \"species\": species\n",
    "            }\n",
    "        \n",
    "        return fasta_dict\n",
    "\n",
    "    def _find_species(self, header):\n",
    "        \"\"\"Find species in FASTA header\"\"\"\n",
    "        header_lower = header.lower()\n",
    "        for spec_group in spec_translate_list:\n",
    "            for term in spec_group[1:]:\n",
    "                if term.lower() in header_lower:\n",
    "                    return spec_group[0]\n",
    "        return \"unknown\"\n",
    "\n",
    "    # Then to use it, we can create an observe function:\n",
    "    def observe_data_changes(change):\n",
    "        if hasattr(change, 'new'):\n",
    "            combiner.update_data(data_transformer.pd_results, data_transformer.mbpdb_results)\n",
    "            setup_data.update_data(data_transformer.pd_results)#, data_transformer.pd_results_cleaned)\n",
    "    \n",
    "        \n",
    "    \n",
    "    # Add this to DataTransformation class:\n",
    "    def attach_observers(self, group_processor):\n",
    "        \"\"\"\n",
    "        Attach observers to monitor changes in pd_results #and pd_results_cleaned\n",
    "        \n",
    "        Args:\n",
    "            group_processor: Instance of GroupProcessing class\n",
    "        \"\"\"\n",
    "        def observe_data_changes(change):\n",
    "            if change.name in ['pd_results']:#, 'pd_results_cleaned']:\n",
    "                group_processor.update_data(self.pd_results)#, self.pd_results_cleaned)\n",
    "        \n",
    "        self.observe(observe_data_changes, names=['pd_results'])#, 'pd_results_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0ca14a-1665-43ec-abde-8adb7fc5c835",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GroupProcessing:\n",
    "    def __init__(self):\n",
    "        self.group_data = {}\n",
    "        self.group_number = 1\n",
    "        self.filtered_columns = []\n",
    "        self.group_uploader = widgets.FileUpload(\n",
    "        accept='.json',\n",
    "        multiple=False,\n",
    "        description='Upload Groups File',\n",
    "        layout=widgets.Layout(width='300px'),\n",
    "        style={'description_width': 'initial'}\n",
    "        )\n",
    "        self.group_uploader.observe(self._on_group_upload_change, names='value')\n",
    "        \n",
    "        # Initialize output areas\n",
    "        self.output = widgets.Output()\n",
    "        self.gd_output_area = widgets.Output()\n",
    "        \n",
    "        # Initialize widgets for group selection\n",
    "        self.column_dropdown = widgets.SelectMultiple(\n",
    "            description='Absorbance',\n",
    "            style={'description_width': 'initial'},\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width='90%', height='300px')\n",
    "        )\n",
    "        \n",
    "        self.grouping_variable_text = widgets.Text(\n",
    "            description='Group Name',\n",
    "            layout=widgets.Layout(width='90%'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Initialize buttons\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 0')\n",
    "        )\n",
    "        \n",
    "        self.add_group_button = widgets.Button(\n",
    "            description='Add Group',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 0')\n",
    "        )\n",
    "        \n",
    "        self.reset_file_button = widgets.Button(\n",
    "            description='Reset Selection',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 75px')\n",
    "        )\n",
    "        \n",
    "        # Set up button callbacks\n",
    "        self.search_button.on_click(self._search_columns)\n",
    "        self.add_group_button.on_click(self._add_group)\n",
    "        self.reset_file_button.on_click(self._reset_selection)\n",
    "        \n",
    "\n",
    "    def update_data(self, pd_results):\n",
    "        \"\"\"Update data and refresh filtered columns\"\"\"\n",
    "        self.pd_results = pd_results\n",
    "        \n",
    "        # Only update if we have valid data\n",
    "        if pd_results is not None:\n",
    "            self.setup_data()\n",
    "            \n",
    "            # Update the dropdown with new filtered columns\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:green;\">Data updated successfully. Column selection refreshed.</b>'))\n",
    "    \n",
    "            \n",
    "    def setup_data(self):\n",
    "        \"\"\"Initialize data and filters for the analysis\"\"\"\n",
    "        # Define columns to exclude with more flexible matching\n",
    "        columns_to_exclude = [\n",
    "            'Marked as', 'Number of Missed Cleavages', 'Missed Cleavages',\n",
    "            'Checked', 'Confidence', 'Annotated Sequence', 'Unnamed: 3', \n",
    "            'Modifications', 'Protein Groups', 'Proteins', 'PSMs', \n",
    "            'Master Protein Accessions', 'Positions in Proteins', \n",
    "            'Modifications in Proteins',\n",
    "            'Theo MHplus in Da', 'Quan Info', \n",
    "            'Confidence by Search Engine', \n",
    "            'q-Value by Search Engine',\n",
    "            'PEP by Search Engine',\n",
    "            'SVM Score by Search Engine',\n",
    "            'XCorr by Search Engine',\n",
    "            'PEP', 'q-Value', 'Top Apex RT', 'RT in min',\n",
    "            'Sequence', 'search_peptide', 'Peptide', 'protein_id', \n",
    "            'protein_description', 'Alignment', 'Species', \n",
    "            'Intervals', 'function', 'unique ID'\n",
    "            ]\n",
    "        \n",
    "        exclude_substrings = [\n",
    "            'Abundances by Bio Rep', \n",
    "            'Count', \n",
    "            'Origin',\n",
    "            'Average_Abundance',\n",
    "            'Avg_',\n",
    "            'SEM_'\n",
    "        ]\n",
    "    \n",
    "        # Use cleaned data if available, otherwise use original\n",
    "        df = self.pd_results_cleaned if (hasattr(self, 'pd_results_cleaned') and \n",
    "                                       not self.pd_results_cleaned.empty) else self.pd_results\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            # More flexible column filtering\n",
    "            self.filtered_columns = []\n",
    "            for col in df.columns:\n",
    "                # Check if any exclusion pattern matches the column name\n",
    "                should_exclude = any(excl.lower() in col.lower() for excl in columns_to_exclude)\n",
    "                # Check if any substring pattern matches\n",
    "                has_excluded_substring = any(sub.lower() in col.lower() for sub in exclude_substrings)\n",
    "                \n",
    "                if not should_exclude and not has_excluded_substring:\n",
    "                    self.filtered_columns.append(col)\n",
    "              \n",
    "            # Update dropdown options\n",
    "            self.column_dropdown.options = self.filtered_columns\n",
    "            self._reset_inputs()\n",
    "        else:\n",
    "            self.filtered_columns = []\n",
    "            self.column_dropdown.options = []\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:red;\">No valid data available for processing.</b>'))\n",
    "\n",
    "    def display_group_selector(self):\n",
    "        \"\"\"Display the JSON file selector for group dictionaries\"\"\"\n",
    "        display(widgets.HTML(\"<h3><u>Upload Existing Group Dictionary:</u></h3>\"))\n",
    "        display(self.group_uploader, self.gd_output_area)\n",
    "        \n",
    "\n",
    "    def display_widgets(self):\n",
    "        \"\"\"Display the main UI for group selection\"\"\"\n",
    "        # Create main grid container\n",
    "        grid = widgets.GridspecLayout(1, 2,  # Number of rows and columns\n",
    "            width='1000px', \n",
    "            grid_gap='5px',  # Adjust spacing between grid elements\n",
    "        )\n",
    "        \n",
    "        # Create input container with vertical scroll\n",
    "        input_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Select New Grouping of Data:</u></h3>\"),\n",
    "            widgets.HTML('Now select the <b>absorbance columns</b> and assign the name of the <b>grouping variable</b>:'),\n",
    "            self.column_dropdown,\n",
    "            self.grouping_variable_text,\n",
    "            # Create button layouts\n",
    "            widgets.HBox([self.search_button, self.add_group_button]),\n",
    "            widgets.HBox([self.reset_file_button])\n",
    "        ], layout=widgets.Layout(\n",
    "            width='95%',\n",
    "            height='600px',\n",
    "            overflow_y='auto'  # Add vertical scroll\n",
    "        ))\n",
    "        \n",
    "        # Create output container with vertical scroll\n",
    "        output_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Group Selection Results:</u></h3>\"),\n",
    "            self.output\n",
    "        ], layout=widgets.Layout(\n",
    "            width='95%',\n",
    "            height='600px',\n",
    "            overflow_y='auto',  # Add vertical scroll\n",
    "            padding='10px'\n",
    "        ))\n",
    "        \n",
    "        # Add to grid\n",
    "        grid[0, 0] = input_container  # Left column\n",
    "        grid[0, 1] = output_container  # Right column\n",
    "        \n",
    "        display(grid)\n",
    "    def _on_gd_submit(self, b, dropdown):\n",
    "        \"\"\"Handle JSON file submission\"\"\"\n",
    "        selected_file = dropdown.value\n",
    "        with self.gd_output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            if selected_file == 'Select an existing grouping dictionary file':\n",
    "                print(\"Please select a valid file.\")\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                # Load and process JSON file\n",
    "                with open(selected_file, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                self.group_data = {}\n",
    "                \n",
    "                # Process groups\n",
    "                with self.output:\n",
    "                    clear_output()\n",
    "                    for group_number, group_info in data.items():\n",
    "                        group_name = group_info.get('grouping_variable')\n",
    "                        selected_columns = group_info.get('abundance_columns')\n",
    "                        \n",
    "                        self.group_data[group_number] = {\n",
    "                            'grouping_variable': group_name,\n",
    "                            'abundance_columns': selected_columns\n",
    "                        }\n",
    "                        \n",
    "                        display(widgets.HTML(\n",
    "                            f\"<b>Group {group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"\n",
    "                        ))\n",
    "                        display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "                        display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "                        display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "                        \n",
    "                display(widgets.HTML(f'<b style=\"color:green;\">Successfully uploaded: {selected_file}</b>'))\n",
    "                \n",
    "            except Exception as e:\n",
    "                display(widgets.HTML(f\"<b style='color:red;'>An error occurred while processing the file: {str(e)}</b>\"))\n",
    "    \n",
    "    def _search_columns(self, b):\n",
    "        \"\"\"Search for columns based on group name\"\"\"\n",
    "        group_name = self.grouping_variable_text.value\n",
    "        if group_name:\n",
    "            matching_columns = [col for col in self.filtered_columns if group_name in col]\n",
    "            self.column_dropdown.value = matching_columns\n",
    "        else:\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:red;\">Please enter a group name to search.</b>'))\n",
    "    \n",
    "    def _add_group(self, b):\n",
    "        \"\"\"Add a new group to the data\"\"\"\n",
    "        group_name = self.grouping_variable_text.value\n",
    "        selected_columns = list(self.column_dropdown.value)\n",
    "        \n",
    "        if not (group_name and selected_columns):\n",
    "            with self.output:\n",
    "                display(widgets.HTML('<b style=\"color:red;\">Please enter a group name and select at least one column.</b>'))\n",
    "            return\n",
    "        \n",
    "        # If group_data exists, use next number, otherwise start at 1\n",
    "        if self.group_data:\n",
    "            # Convert existing keys to integers and find max\n",
    "            existing_numbers = [int(k) for k in self.group_data.keys()]\n",
    "            next_number = max(existing_numbers) + 1\n",
    "            self.group_number = str(next_number)\n",
    "        else:\n",
    "            self.group_data = {}\n",
    "            self.group_number = \"1\"\n",
    "        \n",
    "        # Add new group data to the dictionary\n",
    "        self.group_data[self.group_number] = {\n",
    "            'grouping_variable': group_name,\n",
    "            'abundance_columns': selected_columns\n",
    "        }\n",
    "        \n",
    "        # Display output\n",
    "        with self.output:\n",
    "            display(widgets.HTML(f\"<b>Group {self.group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"))\n",
    "            display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "            display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "            display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "        \n",
    "        self._reset_inputs()\n",
    "        \n",
    "    def _reset_selection(self, b):\n",
    "        \"\"\"Reset all selections and data\"\"\"\n",
    "        self.group_data = {}\n",
    "        self.group_number = 1\n",
    "        with self.gd_output_area:\n",
    "            clear_output()\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "        self._reset_inputs()\n",
    "    \n",
    "    def _reset_inputs(self):\n",
    "        \"\"\"Reset input fields\"\"\"\n",
    "        self.grouping_variable_text.value = ''\n",
    "        self.column_dropdown.value = ()\n",
    "\n",
    "    def _on_group_upload_change(self, change):\n",
    "        \"\"\"Handle JSON file upload\"\"\"\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.gd_output_area:\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    try:\n",
    "                        content = bytes(file_data.content).decode('utf-8')\n",
    "                        data = json.loads(content)\n",
    "                        \n",
    "                        # Process groups\n",
    "                        with self.output:\n",
    "                            for group_number, group_info in data.items():\n",
    "                                group_name = group_info.get('grouping_variable')\n",
    "                                selected_columns = group_info.get('abundance_columns')\n",
    "                                \n",
    "                                # Update group_data without clearing previous entries\n",
    "                                self.group_data[group_number] = {\n",
    "                                    'grouping_variable': group_name,\n",
    "                                    'abundance_columns': selected_columns\n",
    "                                }\n",
    "                                \n",
    "                                display(widgets.HTML(\n",
    "                                    f\"<b>Group {group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"\n",
    "                                ))\n",
    "                                display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "                                display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "                                display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "                                \n",
    "                        display(widgets.HTML(f'<b style=\"color:green;\">Successfully uploaded: {file_data.name}</b>'))\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        display(widgets.HTML(f\"<b style='color:red;'>An error occurred while processing the file: {str(e)}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b615d4b-664c-4726-9027-1b627f07058d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ProteinCombinationHandler(HasTraits):\n",
    "    def __init__(self, data_transformer):\n",
    "        super().__init__()\n",
    "        self.data_transformer = data_transformer\n",
    "        self.pd_results = data_transformer.pd_results\n",
    "        self.proteins_dic = data_transformer.proteins_dic\n",
    "        self.pd_results_cleaned = None\n",
    "        self.protein_output_area = None\n",
    "        self.user_decisions = {}\n",
    "        self.decision_inputs = []\n",
    "        self.multi_position_combinations = []\n",
    "        \n",
    "        # Remove these observers as the workflow will handle updates\n",
    "        # self.data_transformer.observe(self._handle_pd_results_change, names=['pd_results'])\n",
    "        # self.data_transformer.observe(self._handle_proteins_dic_change, names=['proteins_dic'])\n",
    "\n",
    "    # Remove these methods as they're no longer needed\n",
    "    # def _handle_pd_results_change(self, change):\n",
    "    # def _handle_proteins_dic_change(self, change):\n",
    "            \n",
    "    def handle_combinations(self):\n",
    "        \"\"\"Main method to handle protein combinations\"\"\"\n",
    "        if self.pd_results is None or self.pd_results.empty:\n",
    "            return None\n",
    "            \n",
    "        choice = widgets.RadioButtons(\n",
    "            options=[('Yes', True), ('No', False)],\n",
    "            description='Process peptides mapped to multiple proteins?',\n",
    "            style={'description_width': 'initial'},\n",
    "            value=None\n",
    "        )\n",
    "        output = widgets.Output()\n",
    "        \n",
    "        def process_choice(_):\n",
    "            with output:\n",
    "                clear_output()\n",
    "                if choice.value:\n",
    "                    self.pd_results_cleaned = self.process_protein_combinations()\n",
    "                    display(HTML(\"<b style='color:green;'>Processed peptides mapped to multiple proteins.</b>\"))\n",
    "                else:\n",
    "                    self.pd_results_cleaned = self.pd_results.copy()\n",
    "                    display(HTML(\"<b>Using original protein mappings.</b>\"))\n",
    "        \n",
    "        choice.observe(process_choice, 'value')\n",
    "        display(choice)\n",
    "        display(output)\n",
    "        \n",
    "        return self.pd_results_cleaned\n",
    "            \n",
    "    def _get_protein_combinations(self):\n",
    "       \"\"\"Extract unique protein combinations from the dataset\"\"\"\n",
    "       protein_combinations = set()\n",
    "       \n",
    "       for _, row in self.pd_results.iterrows():\n",
    "           if pd.isna(row['Positions in Proteins']) or pd.isna(row['Master Protein Accessions']):\n",
    "               continue\n",
    "               \n",
    "           position_proteins = [p.split()[0] for p in row['Positions in Proteins'].split('; ')]\n",
    "           master_acc = row['Master Protein Accessions']\n",
    "           \n",
    "           # Check species of proteins in Positions in Proteins\n",
    "           species_set = set()\n",
    "           for protein in position_proteins:\n",
    "               if protein in self.proteins_dic:\n",
    "                   species_set.add(self.proteins_dic[protein]['species'])\n",
    "           \n",
    "           # Add to combinations if:\n",
    "           # 1. Multiple proteins in master accession, OR\n",
    "           # 2. Multiple proteins in Positions in Proteins, OR \n",
    "           # 3. Single protein in master accession but proteins in positions are from different species\n",
    "           if ';' in master_acc or ';' in row['Positions in Proteins'] or len(species_set) > 1:\n",
    "               protein_combinations.add('; '.join(sorted(position_proteins)))\n",
    "       \n",
    "       self.multi_position_combinations = list(protein_combinations)\n",
    "       return self.multi_position_combinations\n",
    "            \n",
    "    def _count_combination_occurrences(self, df, proteins):\n",
    "        \"\"\"Count occurrences of a specific protein combination\"\"\"\n",
    "        count = 0\n",
    "        for _, row in df.iterrows():\n",
    "            if pd.isna(row['Positions in Proteins']):\n",
    "                continue\n",
    "                \n",
    "            row_proteins = set(p.split()[0] for p in row['Positions in Proteins'].split('; '))\n",
    "            if row_proteins == set(proteins):\n",
    "                count += 1\n",
    "        return count \n",
    "        \n",
    "    def _get_default_decision(self, protein, row):\n",
    "        \"\"\"\n",
    "        Determine default decision based on protein's presence in Master Protein Accessions\n",
    "        \n",
    "        Args:\n",
    "            protein: The protein ID to check\n",
    "            row: DataFrame row containing Master Protein Accessions\n",
    "            \n",
    "        Returns:\n",
    "            str: 'new' if protein is in Master Accessions, 'remove' if not\n",
    "        \"\"\"\n",
    "        if pd.isna(row['Master Protein Accessions']):\n",
    "            return 'remove'\n",
    "        \n",
    "        master_proteins = row['Master Protein Accessions'].split(';')\n",
    "        master_proteins = [p.strip() for p in master_proteins]\n",
    "        \n",
    "        return 'new' if protein in master_proteins else 'remove'\n",
    "    \n",
    "    def process_protein_combinations(self):\n",
    "        \"\"\"Process protein combinations in pd_results\"\"\"\n",
    "        if not self.pd_results.empty:\n",
    "            df = self.pd_results.copy()\n",
    "            \n",
    "            # Create main container\n",
    "            main_container = widgets.VBox([\n",
    "                widgets.HTML(\"\"\"\n",
    "                    <h3>Peptides Mapped to Multiple Proteins</h3>\n",
    "                    <div style='margin-bottom: 15px;'>\n",
    "                        Select how to handle each protein mapping combination in your dataset.\n",
    "                        These combinations come from either:\n",
    "                        <ul>\n",
    "                            <li>Multiple proteins in Master Protein Accessions</li>\n",
    "                            <li>Multiple proteins in Positions in Proteins</li>\n",
    "                            <li>Proteins from different species</li>\n",
    "                        </ul>\n",
    "                    </div>\n",
    "                \"\"\")\n",
    "            ], layout=widgets.Layout(width='100%', padding='20px'))\n",
    "    \n",
    "            # Get combinations\n",
    "            combinations = self._get_protein_combinations()\n",
    "                                    \n",
    "            def create_help_icon(self, tooltip_text):\n",
    "                \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "                return f'<div title=\"{tooltip_text}\" style=\"display: inline-block; margin-left: 4px;\">' \\\n",
    "                       '<i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>' \\\n",
    "                       '</div>'\n",
    "            \n",
    "            # In your process_protein_combinations method:\n",
    "            table_header = widgets.HTML(\"\"\"\n",
    "                            <div style=\"display: grid; grid-template-columns: 100px 100px 420px 200px auto; gap: 2px; margin-bottom: 10px; font-weight: bold; align-items: center;\">\n",
    "                                <div>\n",
    "                                    Protein ID\n",
    "                                    <span title=\"Unique identifier for the protein\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Species\n",
    "                                    <span title=\"Source organism of the protein\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Description\n",
    "                                    <span title=\"Full protein name or description\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Decision\n",
    "                                    <span title=\"Available options:\n",
    "            - 'new' - Create a separate row for this protein\n",
    "            - 'remove' - Remove this protein from combination\n",
    "            - 'asis' - Keep as part of current combination\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Status\n",
    "                                    <span title=\"Color indicators:\n",
    "            - Grey - Default option (not yet submitted)\n",
    "            - Green - Option has been submitted\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                            </div>\n",
    "                            <hr style=\"margin: 0 0 10px 0;\">\n",
    "                        \"\"\")\n",
    "            # Create input area\n",
    "            input_area = widgets.VBox([table_header], \n",
    "                                    layout=widgets.Layout(width='100%', margin='10px 0'))\n",
    "            \n",
    "            # Add rows for each combination\n",
    "            self.decision_inputs = []\n",
    "            self.status_displays = {}\n",
    "            \n",
    "            for combo_idx, combo in enumerate(combinations, 1):\n",
    "                proteins = combo.split('; ')\n",
    "                \n",
    "                # Find rows with this combination\n",
    "                combo_rows = []\n",
    "                for _, row in df.iterrows():\n",
    "                    if pd.isna(row['Positions in Proteins']):\n",
    "                        continue\n",
    "                    row_proteins = set(p.split()[0] for p in row['Positions in Proteins'].split('; '))\n",
    "                    if row_proteins == set(proteins):\n",
    "                        combo_rows.append(row)\n",
    "                \n",
    "                occurrences = len(combo_rows)\n",
    "                \n",
    "                # Add combination header\n",
    "                input_area.children += (widgets.HTML(f\"\"\"\n",
    "                    <div style=\"background-color: #f8f9fa; padding: 2px; margin: 5px 0; border-radius: 5px;\">\n",
    "                        <b>Combination {combo_idx}</b> ({occurrences} occurrences)\n",
    "                    </div>\n",
    "                \"\"\"),)\n",
    "                \n",
    "                # Process each protein in the combination\n",
    "                for protein in proteins:\n",
    "                    species = \"Unknown\"\n",
    "                    name = \"Unknown\"\n",
    "                    if protein in self.proteins_dic:\n",
    "                        species = self.proteins_dic[protein]['species']\n",
    "                        name = self.proteins_dic[protein]['name']\n",
    "                    \n",
    "                    # Set default decision based on Master Protein Accessions\n",
    "                    default_decision = 'asis'\n",
    "                    if combo_rows:\n",
    "                        first_row = combo_rows[0]\n",
    "                        if not pd.isna(first_row['Master Protein Accessions']):\n",
    "                            master_proteins = first_row['Master Protein Accessions'].split(';')\n",
    "                            master_proteins = [p.strip() for p in master_proteins]\n",
    "                            default_decision = 'new' if protein in master_proteins else 'remove'\n",
    "                    \n",
    "                    # Create decision input\n",
    "                    decision_input = widgets.Text(\n",
    "                        layout=widgets.Layout(width='125px'),\n",
    "                        value=default_decision\n",
    "                    )\n",
    "                    self.decision_inputs.append((combo, protein, decision_input))\n",
    "                    \n",
    "                    # Create status display with initial status\n",
    "                    status_text = {\n",
    "                        'new': \"Will be created as new row\",\n",
    "                        'remove': \"Will be removed\",\n",
    "                        'asis': \"Will keep as is\"\n",
    "                    }\n",
    "                    initial_status = status_text.get(default_decision, '')\n",
    "                    status_display = widgets.HTML(f'<span style=\"color: gray\">{initial_status}</span>')\n",
    "                    self.status_displays[(combo, protein)] = status_display\n",
    "                    \n",
    "                    # Create the row content\n",
    "                    row_content = widgets.HTML(f\"\"\"\n",
    "                    <div style=\"display: grid; grid-template-columns: 100px 100px 420px; gap: 2px; align-items: center;\">\n",
    "                            <div>{protein}</div>\n",
    "                            <div>{species}</div>\n",
    "                            <div>{name}</div>\n",
    "                        </div>\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    # Create container with all elements\n",
    "                    container = widgets.HBox([\n",
    "                        row_content,\n",
    "                        widgets.HBox([decision_input], layout=widgets.Layout(width='150px', padding='0')),\n",
    "                        widgets.HBox([status_display], layout=widgets.Layout(width='200px', padding='0'))\n",
    "                    ], layout=widgets.Layout(\n",
    "                        margin='2px 0',\n",
    "                        display='flex',\n",
    "                        align_items='center',\n",
    "                        overflow='hidden', \n",
    "                        width='100%'\n",
    "                    ))\n",
    "                    \n",
    "                    input_area.children += (container,)\n",
    "    \n",
    "            # Create buttons\n",
    "            button_box = self._create_buttons()\n",
    "            \n",
    "            # Add output area\n",
    "            self.protein_output_area = widgets.Output(\n",
    "                layout=widgets.Layout(width='100%', margin='5px 0')\n",
    "            )\n",
    "            \n",
    "            # Add all components\n",
    "            main_container.children += (input_area, button_box, self.protein_output_area)\n",
    "            \n",
    "            self.pd_results_cleaned = df\n",
    "            display(main_container)\n",
    "            return df\n",
    "            \n",
    "    def _handle_remove_decision(self, df, index, row, positions, protein_to_remove):\n",
    "        \"\"\"Handle 'REMOVE' decision by removing the entire row\"\"\"\n",
    "        # Simply drop the row\n",
    "        df = df.drop(index)\n",
    "        return df\n",
    "            \n",
    "    def _handle_new_decision(self, df, index, row, positions):\n",
    "        \"\"\"Handle 'NEW' decision for a row by removing original row and creating separate rows for each protein\"\"\"\n",
    "        # Create new rows for each protein's position\n",
    "        new_rows = []\n",
    "        for pos in positions:\n",
    "            new_row = row.copy()\n",
    "            new_row['Positions in Proteins'] = pos\n",
    "            protein_id = pos.split()[0]\n",
    "            new_row['Master Protein Accessions'] = protein_id\n",
    "            new_rows.append(new_row)\n",
    "        \n",
    "        # Remove the original row\n",
    "        df = df.drop(index)\n",
    "        \n",
    "        # Add all new rows\n",
    "        if new_rows:\n",
    "            df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "        \n",
    "        return df \n",
    "       \n",
    "    def _on_submit(self, button, df):\n",
    "        \"\"\"Handle submit button click\"\"\"\n",
    "        self.submit_button.disabled = True\n",
    "        self.reset_button.disabled = True\n",
    "        self.progress.value = 0\n",
    "        \n",
    "        with self.protein_output_area:\n",
    "            try:\n",
    "                self.protein_output_area.clear_output()\n",
    "                \n",
    "                # Process decisions by combination\n",
    "                decisions_by_combo = {}\n",
    "                rows_to_remove = set()  # Track indices of original rows to remove\n",
    "                new_rows = []  # Store all new rows to be added\n",
    "                total_inputs = len(self.decision_inputs)\n",
    "                \n",
    "                # First pass: collect all decisions\n",
    "                for i, (combo, protein, input_widget) in enumerate(self.decision_inputs):\n",
    "                    decision = input_widget.value.strip().upper()\n",
    "                    if decision:\n",
    "                        # Update status\n",
    "                        status_display = self.status_displays[(combo, protein)]\n",
    "                        status_display.value = f'<span style=\"color: green\">Decision: {decision}</span>'\n",
    "                        \n",
    "                        # Store decision\n",
    "                        if combo not in decisions_by_combo:\n",
    "                            decisions_by_combo[combo] = {}\n",
    "                        decisions_by_combo[combo][protein] = decision\n",
    "                    \n",
    "                    # Update progress\n",
    "                    self.progress.value = ((i + 1) / total_inputs * 50)  # First half of progress\n",
    "                \n",
    "                # Second pass: process the dataframe based on decisions\n",
    "                if decisions_by_combo:\n",
    "                    df_processed = df.copy()\n",
    "                    processed_count = 0\n",
    "                    total_combinations = len(decisions_by_combo)\n",
    "                    \n",
    "                    for combo, protein_decisions in decisions_by_combo.items():\n",
    "                        proteins = combo.split('; ')\n",
    "                        # Create exact pattern match for this combination\n",
    "                        pattern = ''.join(f'(?=.*{p})' for p in proteins)\n",
    "                        mask = df_processed['Positions in Proteins'].str.contains(pattern, regex=True)\n",
    "                        \n",
    "                        matched_indices = df_processed[mask].index\n",
    "                        for idx in matched_indices:\n",
    "                            row = df_processed.loc[idx]\n",
    "                            positions = row['Positions in Proteins'].split('; ')\n",
    "                            current_proteins = [p.split()[0] for p in positions]\n",
    "                            current_combo = '; '.join(sorted(current_proteins))\n",
    "                            \n",
    "                            # Only process if this exact combination matches\n",
    "                            if current_combo == combo:\n",
    "                                any_new_or_remove = False\n",
    "                                custom_updates = []\n",
    "                                \n",
    "                                # First check if we need to remove the original row\n",
    "                                for protein, decision in protein_decisions.items():\n",
    "                                    if decision in ['NEW', 'REMOVE']:\n",
    "                                        any_new_or_remove = True\n",
    "                                        break\n",
    "                                    elif decision != 'ASIS':\n",
    "                                        custom_updates.append((protein, decision))\n",
    "                                \n",
    "                                if any_new_or_remove:\n",
    "                                    # Mark original row for removal\n",
    "                                    rows_to_remove.add(idx)\n",
    "                                    \n",
    "                                    # If any decision is 'NEW', create individual rows\n",
    "                                    for protein, decision in protein_decisions.items():\n",
    "                                        if decision == 'NEW':\n",
    "                                            position = next(p for p in positions if protein in p)\n",
    "                                            new_row = row.copy()\n",
    "                                            new_row['Positions in Proteins'] = position\n",
    "                                            new_row['Master Protein Accessions'] = protein\n",
    "                                            new_rows.append(new_row)\n",
    "                                \n",
    "                                elif custom_updates:\n",
    "                                    # Handle custom protein IDs without removing original row\n",
    "                                    for protein, new_accession in custom_updates:\n",
    "                                        df_processed = self._handle_custom_protein_id(df_processed, idx, positions, new_accession)\n",
    "                        \n",
    "                        processed_count += 1\n",
    "                        self.progress.value = 50 + (processed_count / total_combinations * 50)\n",
    "                    \n",
    "                    # Remove marked rows\n",
    "                    df_processed = df_processed.drop(index=list(rows_to_remove))\n",
    "                    \n",
    "                    # Add all new rows\n",
    "                    if new_rows:\n",
    "                        df_processed = pd.concat([df_processed, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "                    \n",
    "                    self.pd_results_cleaned = df_processed\n",
    "                \n",
    "                display(HTML(\"<b style='color:green;'>Processing complete.</b>\"))\n",
    "                \n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red;'>Error: {str(e)}</b>\"))\n",
    "            \n",
    "            finally:\n",
    "                self.submit_button.disabled = False\n",
    "                self.reset_button.disabled = False\n",
    "                \n",
    "        return self.pd_results_cleaned\n",
    "\n",
    "                \n",
    "    def create_help_icon(self, tooltip_text):\n",
    "        \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "        return widgets.HTML(\n",
    "            f'<div title=\"{tooltip_text}\" style=\"display: inline-block;\">'\n",
    "            '<i class=\"fa fa-question-circle\" style=\"color: #007bff;\"></i>'\n",
    "            '</div>'\n",
    "        )\n",
    "    \n",
    "    def _create_input_fields(self, input_area):\n",
    "        \"\"\"Create input fields for each unique protein combination\"\"\"\n",
    "        self.user_decisions = {}\n",
    "        self.decision_inputs = []\n",
    "        \n",
    "        def create_labeled_decision(protein_info, description):\n",
    "            \"\"\"Create a labeled decision input with tooltip\"\"\"\n",
    "            container = widgets.VBox([\n",
    "                widgets.HTML(f\"\"\"\n",
    "                <div style='margin-bottom: 5px;'>\n",
    "                    <b>{protein_info}</b>\n",
    "                    {self.create_help_icon(description).value}\n",
    "                </div>\n",
    "                \"\"\"),\n",
    "                widgets.Text(\n",
    "                    description='Decision:',\n",
    "                    layout=widgets.Layout(width='300px')\n",
    "                )\n",
    "            ], layout=widgets.Layout(margin='10px 0px'))\n",
    "            return container\n",
    "        \n",
    "        for protein_combo in self.multi_position_combinations:\n",
    "            proteins = protein_combo.split('; ')\n",
    "            pattern = ''.join(f'(?=.*{p})' for p in proteins)\n",
    "            occurrences = len(self.pd_results[self.pd_results['Positions in Proteins'].str.contains(pattern, regex=True)])\n",
    "            \n",
    "            # Create header for this combination\n",
    "            combo_header = widgets.HTML(f\"\"\"\n",
    "                <h4 style=\"display: flex; align-items: center;\">\n",
    "                    {occurrences} occurrences of Multiple protein combination\n",
    "                    {self.create_help_icon(\"Multiple proteins were found mapping to the same peptide sequence\").value}\n",
    "                </h4>\n",
    "            \"\"\")\n",
    "            \n",
    "            # Create container for this combination's inputs\n",
    "            combo_container = widgets.VBox([combo_header])\n",
    "            \n",
    "            # Add individual protein inputs\n",
    "            for protein in proteins:\n",
    "                if protein in self.proteins_dic:\n",
    "                    species = self.proteins_dic[protein]['species']\n",
    "                    name = self.proteins_dic[protein]['name']\n",
    "                    protein_info = f\"{protein} ({species} - {name})\"\n",
    "                else:\n",
    "                    protein_info = protein\n",
    "                \n",
    "                # Create tooltip description\n",
    "                description = (\n",
    "                    f\"Select how to handle {protein}:\\n\"\n",
    "                    \"â¢ 'new' - Create a separate row for this protein\\n\"\n",
    "                    \"â¢ 'asis' - Keep as part of the current combination\\n\"\n",
    "                    \"â¢ Enter a custom ID to replace this protein\"\n",
    "                )\n",
    "                \n",
    "                # Create input container for this protein\n",
    "                protein_input = create_labeled_decision(protein_info, description)\n",
    "                combo_container.children += (protein_input,)\n",
    "                self.decision_inputs.append(protein_input.children[1])  # Store the Text widget\n",
    "            \n",
    "            combo_container.layout.border = '1px solid #ddd'\n",
    "            combo_container.layout.padding = '10px'\n",
    "            combo_container.layout.margin = '10px 0px'\n",
    "            combo_container.layout.border_radius = '5px'\n",
    "            \n",
    "            input_area.children += (combo_container,)\n",
    "    def _process_rows(self, df):\n",
    "        \"\"\"Process DataFrame rows based on user decisions\"\"\"\n",
    "        processed_df = df.copy()\n",
    "        new_rows = []\n",
    "        \n",
    "        for index, row in processed_df.iterrows():\n",
    "            positions_row = row['Positions in Proteins']\n",
    "            master_acc = row['Master Protein Accessions']\n",
    "            sequence = row['Annotated Sequence']\n",
    "            \n",
    "            # Get the decision for this specific entry\n",
    "            decision = self.grid_data[index]['Decision']\n",
    "            \n",
    "            if decision == 'asis':\n",
    "                continue\n",
    "                \n",
    "            elif decision == 'new':\n",
    "                # Create separate rows\n",
    "                positions = positions_row.split('; ')\n",
    "                proteins = [p.split()[0] for p in positions]\n",
    "                \n",
    "                # Update first position in current row\n",
    "                processed_df.at[index, 'Positions in Proteins'] = positions[0]\n",
    "                processed_df.at[index, 'Master Protein Accessions'] = proteins[0]\n",
    "                \n",
    "                # Create new rows for additional positions\n",
    "                for pos, prot in zip(positions[1:], proteins[1:]):\n",
    "                    new_row = row.copy()\n",
    "                    new_row['Positions in Proteins'] = pos\n",
    "                    new_row['Master Protein Accessions'] = prot\n",
    "                    new_rows.append(new_row)\n",
    "                    \n",
    "            elif decision.startswith('custom:'):\n",
    "                # Handle custom protein ID\n",
    "                new_protein_id = decision.split(':')[1]\n",
    "                positions = positions_row.split('; ')\n",
    "                new_positions = []\n",
    "                for pos in positions:\n",
    "                    num_range = pos[pos.index('['):] if '[' in pos else ''\n",
    "                    new_positions.append(f\"{new_protein_id} {num_range}\")\n",
    "                \n",
    "                processed_df.at[index, 'Master Protein Accessions'] = new_protein_id\n",
    "                processed_df.at[index, 'Positions in Proteins'] = '; '.join(new_positions)\n",
    "        \n",
    "        # Add all new rows\n",
    "        if new_rows:\n",
    "            processed_df = pd.concat([processed_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "        \n",
    "        return processed_df\n",
    "\n",
    "\n",
    "    def _handle_custom_protein_id(self, df, index, positions, new_accession):\n",
    "        \"\"\"Handle custom protein ID decision\"\"\"\n",
    "        new_positions = []\n",
    "        for pos in positions:\n",
    "            num_range = pos[pos.index('['):] if '[' in pos else ''\n",
    "            new_positions.append(f\"{new_accession} {num_range}\")\n",
    "        \n",
    "        df.at[index, 'Master Protein Accessions'] = new_accession\n",
    "        df.at[index, 'Positions in Proteins'] = '; '.join(new_positions)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _create_buttons(self):\n",
    "        \"\"\"Create submit and reset buttons\"\"\"\n",
    "        self.submit_button = widgets.Button(\n",
    "            description=\"Submit\", \n",
    "            button_style='success',\n",
    "            disabled=False\n",
    "        )\n",
    "        self.reset_button = widgets.Button(\n",
    "            description=\"Reset\", \n",
    "            button_style='warning',\n",
    "            disabled=False\n",
    "        )\n",
    "        self.progress = widgets.FloatProgress(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=100,\n",
    "            description='Processing:',\n",
    "            bar_style='info',\n",
    "            style={'bar_color': '#0080ff'},\n",
    "            orientation='horizontal',\n",
    "            layout=widgets.Layout(width='50%')\n",
    "        )\n",
    "        \n",
    "        button_box = widgets.VBox([\n",
    "            widgets.HBox([self.submit_button, self.reset_button]),\n",
    "            self.progress\n",
    "        ])\n",
    "        \n",
    "        self.reset_button.on_click(self._on_reset_button_clicked)\n",
    "        self.submit_button.on_click(lambda b: self._on_submit(b, self.pd_results.copy()))\n",
    "        \n",
    "        return button_box\n",
    "        \n",
    "    def _on_reset_button_clicked(self, b):\n",
    "        \"\"\"Handle reset button click by resetting options to default values\"\"\"\n",
    "        # Disable buttons during reset\n",
    "        self.submit_button.disabled = True\n",
    "        self.reset_button.disabled = True\n",
    "        \n",
    "        # Clear output area\n",
    "        with self.protein_output_area:\n",
    "            self.protein_output_area.clear_output()\n",
    "            display(HTML(\"<b style='color:blue;'>Resetting options to defaults...</b>\"))\n",
    "        \n",
    "        # Reset progress bar\n",
    "        self.progress.value = 0\n",
    "        \n",
    "        try:\n",
    "            # Reset each input field to its default value based on Master Protein Accessions\n",
    "            df = self.pd_results.copy()\n",
    "            processed = 0\n",
    "            total_inputs = len(self.decision_inputs)\n",
    "            \n",
    "            for combo, protein, input_field in self.decision_inputs:\n",
    "                # Find rows with this combination\n",
    "                proteins = combo.split('; ')\n",
    "                combo_rows = []\n",
    "                for _, row in df.iterrows():\n",
    "                    if pd.isna(row['Positions in Proteins']):\n",
    "                        continue\n",
    "                    row_proteins = set(p.split()[0] for p in row['Positions in Proteins'].split('; '))\n",
    "                    if row_proteins == set(proteins):\n",
    "                        combo_rows.append(row)\n",
    "                \n",
    "                # Determine default decision\n",
    "                default_decision = 'asis'\n",
    "                if combo_rows:\n",
    "                    first_row = combo_rows[0]\n",
    "                    if not pd.isna(first_row['Master Protein Accessions']):\n",
    "                        master_proteins = first_row['Master Protein Accessions'].split(';')\n",
    "                        master_proteins = [p.strip() for p in master_proteins]\n",
    "                        default_decision = 'new' if protein in master_proteins else 'remove'\n",
    "                \n",
    "                # Set input field value\n",
    "                input_field.value = default_decision\n",
    "                \n",
    "                # Update status display\n",
    "                status_display = self.status_displays[(combo, protein)]\n",
    "                status_text = {\n",
    "                    'new': \"Will be created as new row\",\n",
    "                    'remove': \"Will be removed\",\n",
    "                    'asis': \"Will keep as is\"\n",
    "                }\n",
    "                status_display.value = f'<span style=\"color: gray\">{status_text[default_decision]}</span>'\n",
    "                \n",
    "                # Update progress\n",
    "                processed += 1\n",
    "                self.progress.value = (processed / total_inputs) * 100\n",
    "            \n",
    "            # Reset internal state\n",
    "            self.user_decisions = {}\n",
    "            self.pd_results_cleaned = self.pd_results.copy()\n",
    "            \n",
    "            with self.protein_output_area:\n",
    "                self.protein_output_area.clear_output()\n",
    "                display(HTML(\"<b style='color:green;'>Reset complete. All options set to defaults.</b>\"))\n",
    "            \n",
    "        except Exception as e:\n",
    "            with self.protein_output_area:\n",
    "                self.protein_output_area.clear_output()\n",
    "                display(HTML(f\"<b style='color:red;'>Error during reset: {str(e)}</b>\"))\n",
    "        \n",
    "        finally:\n",
    "            # Re-enable buttons\n",
    "            self.submit_button.disabled = False\n",
    "            self.reset_button.disabled = False\n",
    "    \n",
    "\n",
    "    def _display_results(self):\n",
    "        \"\"\"Display processing results\"\"\"\n",
    "        for combo, decision in self.user_decisions.items():\n",
    "            if decision == 'NEW':\n",
    "                display(HTML(f'<b>{combo}</b> <b style=\"color:green;\">has been successfully processed.</b>'))\n",
    "                display(HTML(f'&nbsp;&nbsp;&nbsp;&nbsp;The positions have been updated with the new protein ID \"{decision}\".'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b05c11-89ca-4ea5-8b1f-7a2a62d99f93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb5a9c891f843c9b77add2535bf867a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.1â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac80cdef41c47f0bc8bcd6ae3385ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3><u>Upload Peptidomic Data Files:</u></h3>'), HBox(children=(FileUpload(value=()â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5433104a39d240bea1321cf3c439417a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778940983b2741f28a9ac8c1b2316131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3><u>Protein Mapping</u></h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ebaac620684b19b615974b184ea733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef38e33e3eb403083e3787b084c04dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2ef19abf254853921f2cc8b78f29e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell where we initialize the workflow\n",
    "# Create global output areas for protein mapping and group processing\n",
    "protein_mapping_output = widgets.Output()\n",
    "group_processing_output = widgets.Output()\n",
    "\n",
    "# Create ProcessingWorkflow class with persistent UI elements\n",
    "class ProcessingWorkflow:\n",
    "    def __init__(self):\n",
    "        self.data_transformer = DataTransformation()\n",
    "        self.protein_handler = ProteinCombinationHandler(self.data_transformer)\n",
    "        self.group_processor = GroupProcessing()\n",
    "        \n",
    "        # Set up observers\n",
    "        self.data_transformer.observe(self._handle_data_change, names=['pd_results'])\n",
    "        self.data_transformer.observe(self._handle_fasta_change, names=['proteins_dic'])\n",
    "            \n",
    "    def _handle_data_change(self, change):\n",
    "        \"\"\"Handle changes in proteomics data\"\"\"\n",
    "        if change.name == 'pd_results':\n",
    "            # Update protein mapping\n",
    "            with protein_mapping_output:\n",
    "                protein_mapping_output.clear_output()\n",
    "                if change.new is not None:\n",
    "                    display(HTML(\"<h3>Multiple Protein Mappings</h3>\"))\n",
    "                    self.protein_handler.pd_results = change.new\n",
    "                    self.protein_handler.handle_combinations()\n",
    "                else:\n",
    "                    display(HTML(\"<h3>Multiple Protein Mappings</h3>\"))\n",
    "                    display(HTML(\"<b style='color:orange;'>Waiting for proteomics data to be uploaded...</b>\"))\n",
    "            \n",
    "            # Update group processor\n",
    "            self.group_processor.update_data(change.new)\n",
    "            \n",
    "    def _handle_fasta_change(self, change):\n",
    "        \"\"\"Handle changes in FASTA data\"\"\"\n",
    "        if hasattr(self.protein_handler, 'proteins_dic'):\n",
    "            self.protein_handler.proteins_dic = self.data_transformer.proteins_dic\n",
    "\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the complete workflow interface\"\"\"\n",
    "        # Display data transformer interface\n",
    "        self.data_transformer.setup_data_loading_ui()\n",
    "        \n",
    "        # Add spacing\n",
    "        display(widgets.HTML(\"<br>\"))\n",
    "        \n",
    "        # Display protein mapping section (always visible)\n",
    "        display(widgets.HTML(\"<h3><u>Protein Mapping</u></h3>\"))\n",
    "        display(protein_mapping_output)\n",
    "        \n",
    "        # Initialize protein handler with empty state\n",
    "        if self.protein_handler is None:\n",
    "            with protein_mapping_output:\n",
    "                self.protein_handler = ProteinCombinationHandler(self.data_transformer)\n",
    "                self.protein_handler.handle_combinations()\n",
    "        \n",
    "        # Add spacing\n",
    "        display(widgets.HTML(\"<br>\"))\n",
    "        \n",
    "        # Display group processing section\n",
    "        with group_processing_output:\n",
    "            self.group_processor.display_group_selector()\n",
    "            self.group_processor.display_widgets()\n",
    "        display(group_processing_output)\n",
    "\n",
    "# Initialize and display workflow\n",
    "workflow = ProcessingWorkflow()\n",
    "workflow.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f50828-45c1-444e-be98-4ed3ff7a947e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CombineAverageDataframes:\n",
    "    def __init__(self, data_transformer, group_processor, protein_handler):\n",
    "        self.data_transformer = data_transformer\n",
    "        self.group_processor = group_processor\n",
    "        self.pd_results = data_transformer.pd_results\n",
    "        self.mbpdb_results = data_transformer.mbpdb_results\n",
    "        self.pd_results_cleaned = protein_handler.pd_results_cleaned if hasattr(protein_handler, 'pd_results_cleaned') and protein_handler.pd_results_cleaned is not None else pd.DataFrame()\n",
    "        self._merged_df = None\n",
    "        # Set up observer for data changes\n",
    "        self.data_transformer.observe(self._handle_data_change, names=['pd_results', 'mbpdb_results'])\n",
    "        \n",
    "    def _handle_data_change(self, change):\n",
    "        \"\"\"Handle changes in the input data.\"\"\"\n",
    "        if change.name == 'pd_results':\n",
    "            self.pd_results = change.new\n",
    "        elif change.name == 'mbpdb_results':\n",
    "            self.mbpdb_results = change.new\n",
    "        elif change.name == 'pd_results_cleaned':\n",
    "            self.pd_results_cleaned = change.new        # Re-run interactive display\n",
    "        clear_output()        \n",
    "    @property\n",
    "    def merged_df(self):\n",
    "        \"\"\"Property to access the merged DataFrame.\"\"\"\n",
    "        return self._merged_df\n",
    "\n",
    "    def extract_bioactive_peptides(self):\n",
    "        \"\"\"\n",
    "        Extracts the list of bioactive peptide matches from the imported MBPDB search.\n",
    "        \"\"\"\n",
    "        if not self.mbpdb_results.empty:\n",
    "            # Drop rows where protein_id is NaN or 'None'\n",
    "            mbpdb_results_cleaned = self.mbpdb_results.copy()\n",
    "            mbpdb_results_cleaned.dropna(subset=['search_peptide'], inplace=True)\n",
    "            mbpdb_results_cleaned = mbpdb_results_cleaned[mbpdb_results_cleaned['protein_id'] != 'None']\n",
    "\n",
    "            # Check if '% Alignment' column exists\n",
    "            if '% Alignment' in mbpdb_results_cleaned.columns:\n",
    "                agg_dict = {\n",
    "                    'peptide': 'first', \n",
    "                    'protein_id': 'first',\n",
    "                    'protein_description': 'first',\n",
    "                    '% Alignment': 'first',\n",
    "                    'species': 'first',\n",
    "                    'intervals': 'first',\n",
    "                    'function': lambda x: list(x.dropna().unique())\n",
    "                }\n",
    "            else:\n",
    "                agg_dict = {\n",
    "                    'peptide': 'first', \n",
    "                    'search_peptide': 'first',\n",
    "                    'protein_description': 'first',\n",
    "                    'species': 'first',\n",
    "                    'intervals': 'first',\n",
    "                    'function': lambda x: list(x.dropna().unique())\n",
    "                }\n",
    "\n",
    "            # Perform the groupby and aggregation\n",
    "            mbpdb_results_grouped = mbpdb_results_cleaned.groupby('search_peptide').agg(agg_dict).reset_index()\n",
    "\n",
    "            # Flatten the 'function' list\n",
    "            mbpdb_results_grouped['function'] = mbpdb_results_grouped['function'].apply(\n",
    "                lambda x: '; '.join(x) if isinstance(x, list) else x\n",
    "            )\n",
    "            return mbpdb_results_cleaned, mbpdb_results_grouped\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def create_unique_id(self, row):\n",
    "        \"\"\"Creates a unique ID for each peptide row.\"\"\"\n",
    "        if pd.notna(row['Modifications']):\n",
    "            unique_id = row['Sequence'] + \"_\" + row['Modifications'].strip()\n",
    "        else:\n",
    "            unique_id = row['Sequence']\n",
    "        return unique_id.rstrip('_')\n",
    "    \n",
    "    def process_pd_results(self, mbpdb_results_grouped):\n",
    "        \"\"\"Processes the PD results and merges with MBPDB results.\"\"\"\n",
    "        pd_results_cleaned = self.pd_results_cleaned\n",
    "        \n",
    "        # Process positions and accessions\n",
    "        #pd_results_cleaned['Positions in Proteins'] = pd_results_cleaned['Positions in Proteins'].str.split(';', expand=False).str[0]\n",
    "        #pd_results_cleaned['Master Protein Accessions'] = pd_results_cleaned['Master Protein Accessions'].str.split(';', expand=False).str[0]\n",
    "        \n",
    "        # Create sequence column if needed\n",
    "        if 'Sequence' not in pd_results_cleaned.columns:\n",
    "            pd_results_cleaned['Sequence'] = pd_results_cleaned['Annotated Sequence'].str.split('.', expand=False).str[1]\n",
    "        \n",
    "        # Create unique ID\n",
    "        pd_results_cleaned['unique ID'] = pd_results_cleaned.apply(self.create_unique_id, axis=1)\n",
    "        \n",
    "        #  Extract start and stop positions\n",
    "        try:\n",
    "            # Initialize start and stop columns with NaN\n",
    "            pd_results_cleaned['start'] = pd.NA\n",
    "            pd_results_cleaned['stop'] = pd.NA\n",
    "            \n",
    "            # Create mask for rows without semicolons (single positions)\n",
    "            single_position_mask = ~pd_results_cleaned['Positions in Proteins'].str.contains(';', na=False)\n",
    "            \n",
    "            # Process rows with single positions\n",
    "            single_positions = pd_results_cleaned.loc[single_position_mask, 'Positions in Proteins']\n",
    "            if not single_positions.empty:\n",
    "                extracted = single_positions.str.extract(r'\\[(\\d+)-(\\d+)\\]')\n",
    "                \n",
    "                # Convert to numeric and handle invalid values\n",
    "                pd_results_cleaned.loc[single_position_mask, 'start'] = pd.to_numeric(extracted[0], errors='coerce')\n",
    "                pd_results_cleaned.loc[single_position_mask, 'stop'] = pd.to_numeric(extracted[1], errors='coerce')\n",
    "            \n",
    "            # Convert to Int64 to handle missing values properly\n",
    "            pd_results_cleaned['start'] = pd_results_cleaned['start'].astype('Int64')\n",
    "            pd_results_cleaned['stop'] = pd_results_cleaned['stop'].astype('Int64')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing protein positions: {str(e)}\")\n",
    "        \n",
    "        # Reorder columns with unique ID and Sequence first\n",
    "        remaining_cols = [col for col in pd_results_cleaned.columns \n",
    "                         if col not in ['unique ID', 'Sequence', 'Master Protein Accessions', \n",
    "                                      'Positions in Proteins', 'start', 'stop']]\n",
    "        \n",
    "        columns_order = ['unique ID', 'Sequence', 'Master Protein Accessions', \n",
    "                        'Positions in Proteins', 'start', 'stop'] + remaining_cols\n",
    "        \n",
    "        pd_results_cleaned = pd_results_cleaned[columns_order]\n",
    "        \n",
    "        # Merge with MBPDB results if available\n",
    "        if mbpdb_results_grouped is not None and not mbpdb_results_grouped.empty:\n",
    "            merged_df = pd.merge(pd_results_cleaned, mbpdb_results_grouped, \n",
    "                               right_on='search_peptide', left_on='unique ID', how='left')\n",
    "            display(HTML(\"<b style='color:green;'>The MBPDB was successfully merged with the peptidomic data matching the Search Peptide and Unique ID columns.</b>\"))\n",
    "        else:\n",
    "            merged_df = pd_results_cleaned.copy()\n",
    "            merged_df['function'] = np.nan\n",
    "            display(HTML(\"<b style='color:orange;'>No MBPDB was uploaded.</b>\"))\n",
    "            display(HTML(\"<b style='color:orange;'>The merged Dataframe contains only peptidomic data.</b>\"))\n",
    "        \n",
    "        return merged_df\n",
    "    \n",
    "    def calculate_group_abundance_sem_averages(self, df, group_data):\n",
    "        \"\"\"Calculates group abundance averages and SEMs, organizing them with averages first, then SEMs.\"\"\"\n",
    "        # Check if all average abundance columns already exist\n",
    "        all_columns_exist = True\n",
    "        for group_number, details in group_data.items():\n",
    "            average_column_name = f\"Avg_{details['grouping_variable']}\"\n",
    "            if average_column_name not in df.columns:\n",
    "                all_columns_exist = False\n",
    "                break\n",
    "        \n",
    "        if all_columns_exist:\n",
    "            display(HTML('<b style=\"color:orange;\">All average abundance columns already exist. Returning original DataFrame.</b>'))\n",
    "            return df\n",
    "        \n",
    "        # If not all columns exist, proceed with calculations\n",
    "        average_columns = {}\n",
    "        sem_columns = {}\n",
    "        \n",
    "        # Calculate all averages and SEMs but store them separately\n",
    "        for group_number, details in group_data.items():\n",
    "            grouping_variable = details['grouping_variable']\n",
    "            abundance_columns = details['abundance_columns']\n",
    "            \n",
    "            # Convert abundance columns to numeric\n",
    "            for col in abundance_columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Define column names\n",
    "            average_column_name = f\"Avg_{grouping_variable}\"\n",
    "            sem_column_name = f\"SEM_{grouping_variable}\"\n",
    "        \n",
    "            # Calculate standard deviation\n",
    "            std = df[abundance_columns].std(axis=1, skipna=True)\n",
    "            \n",
    "            # Calculate number of non-NaN values for each row\n",
    "            n_samples = df[abundance_columns].notna().sum(axis=1)\n",
    "            \n",
    "            # Calculate SEM (standard deviation divided by square root of n)\n",
    "            sem = std / np.sqrt(n_samples)\n",
    "            \n",
    "            # Store results in separate dictionaries\n",
    "            average_columns[average_column_name] = df[abundance_columns].mean(axis=1, skipna=True)\n",
    "            sem_columns[sem_column_name] = sem\n",
    "        \n",
    "        # Combine the columns in the desired order (all averages, then all SEMs)\n",
    "        new_columns = {**average_columns, **sem_columns}\n",
    "        \n",
    "        # Add new columns to DataFrame\n",
    "        df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "        \n",
    "        if not df.empty:\n",
    "            display(HTML('<b style=\"color:green;\">Group average abundance and Standard Error of Mean (SEM) columns have been successfully added to the DataFrame.</b>'))\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def process_data(self, group_data):\n",
    "        \"\"\"Main method to process all data.\"\"\"\n",
    "        if hasattr(self, 'pd_results') and self.pd_results is not None and not self.pd_results.empty:\n",
    "            try:\n",
    "                # Extract and process bioactive peptides\n",
    "                mbpdb_results_cleaned, mbpdb_results_grouped = self.extract_bioactive_peptides()\n",
    "                \n",
    "                if not hasattr(self, 'pd_results_cleaned') or self.pd_results_cleaned is None:\n",
    "                    self.pd_results_cleaned = self.pd_results.copy()\n",
    "                \n",
    "                # Process PD results and merge with MBPDB\n",
    "                merged_df_temp = self.process_pd_results(mbpdb_results_grouped)\n",
    "                \n",
    "                # Calculate abundance averages if group_data exists\n",
    "                if group_data:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                        final_df = self.calculate_group_abundance_sem_averages(merged_df_temp, group_data)\n",
    "                else:\n",
    "                    final_df = merged_df_temp\n",
    "                    display(HTML(\"<b style='color:orange;'>No group data provided. Skipping abundance calculations.</b>\"))\n",
    "                \n",
    "                # Store the final DataFrame\n",
    "                self._merged_df = final_df\n",
    "                return final_df\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red;'>Error processing data: {str(e)}</b>\"))\n",
    "                return None\n",
    "        else:\n",
    "            display(HTML(\"<b style='color:red;'>No PD results data available for processing.</b>\"))\n",
    "            return None\n",
    "            \n",
    "    def update_data(self, pd_results):\n",
    "        \"\"\"Update data and refresh filtered columns\"\"\"\n",
    "        self.pd_results = pd_results\n",
    "        \n",
    "        # Only update if we have valid data\n",
    "        if pd_results is not None and not pd_results.empty:\n",
    "            self.setup_data()\n",
    "            \n",
    "            # Update the dropdown with new filtered columns\n",
    "            self.column_dropdown.options = self.filtered_columns\n",
    "            \n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:green;\">Data updated successfully. Column selection refreshed.</b>'))\n",
    "        else:\n",
    "            # Clear options if no data\n",
    "            self.column_dropdown.options = []\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:orange;\">No data available for column selection.</b>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20958467-4a02-4b10-afbb-1a3224430ce5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ExportManager:\n",
    "    \"\"\"Class to manage all export operations with notebook-compatible lazy loading\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output_style = \"\"\"\n",
    "            <style>\n",
    "            .download-link {\n",
    "                background-color: #4CAF50;\n",
    "                border: none;\n",
    "                color: white;\n",
    "                padding: 10px 20px;\n",
    "                text-align: center;\n",
    "                text-decoration: none;\n",
    "                display: inline-block;\n",
    "                font-size: 14px;\n",
    "                margin: 4px 2px;\n",
    "                cursor: pointer;\n",
    "                border-radius: 4px;\n",
    "            }\n",
    "            .download-link:hover {\n",
    "                background-color: #45a049;\n",
    "            }\n",
    "            .download-link:disabled {\n",
    "                background-color: #cccccc;\n",
    "                cursor: not-allowed;\n",
    "            }\n",
    "            .export-section {\n",
    "                margin-bottom: 20px;\n",
    "                padding: 15px;\n",
    "                border-radius: 5px;\n",
    "                background-color: #f8f9fa;\n",
    "            }\n",
    "            .export-description {\n",
    "                color: #666;\n",
    "                margin: 5px 0 15px 0;\n",
    "                font-style: italic;\n",
    "            }\n",
    "            </style>\n",
    "        \"\"\"\n",
    "\n",
    "    def _create_download_section(self, title, description, data_generator, mime_type):\n",
    "        \"\"\"Create a download section with direct data generation\"\"\"\n",
    "        try:\n",
    "            # Generate the data immediately but efficiently\n",
    "            content, filename = data_generator()\n",
    "            \n",
    "            if isinstance(content, str):\n",
    "                content = content.encode('utf-8')\n",
    "            \n",
    "            # Convert to base64\n",
    "            b64_data = base64.b64encode(content).decode('utf-8')\n",
    "            file_data = f\"data:{mime_type};base64,{b64_data}\"\n",
    "            \n",
    "            html_content = f\"\"\"\n",
    "            <div class=\"export-section\">\n",
    "                <h3><u>{title}</u></h3>\n",
    "                <div class=\"export-description\">\n",
    "                    {description}\n",
    "                </div>\n",
    "                <a href=\"{file_data}\" \n",
    "                   download=\"{filename}\" \n",
    "                   class=\"download-link\"\n",
    "                   title=\"Click to download\">\n",
    "                    Download Data\n",
    "                </a>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            display(HTML(self.output_style + html_content))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating download: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def export_mbpdb_results(self, df):\n",
    "        \"\"\"Export MBPDB results as TSV\"\"\"\n",
    "        if not 'function' in df.columns:\n",
    "            return\n",
    "            \n",
    "        def generate_mbpdb_data():\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"MBPDB_SEARCH_{timestamp}.tsv\"\n",
    "            content = df.to_csv(sep='\\t', index=False)\n",
    "            return content.encode(), filename\n",
    "            \n",
    "        self._create_download_section(\n",
    "            \n",
    "            \"Export MBPDB Search Results\",\n",
    "            \"Download the results from searching your peptides against the MBPDB database\",\n",
    "            generate_mbpdb_data,\n",
    "            'text/tab-separated-values'\n",
    "        )\n",
    "\n",
    "    def export_group_data(self, group_data):\n",
    "        \"\"\"Export group data as JSON\"\"\"\n",
    "        def generate_group_data():\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"Categorical_variable_definitions_{timestamp}.json\"\n",
    "            content = json.dumps(group_data, indent=4)\n",
    "            return content.encode(), filename\n",
    "            \n",
    "        self._create_download_section(\n",
    "            \"Export Group Data\",\n",
    "            \"Download the categorical variable definitions used for data grouping and analysis\",\n",
    "            generate_group_data,\n",
    "            'application/json'\n",
    "        )\n",
    "\n",
    "    def export_dataframe(self, df):\n",
    "        \"\"\"Export DataFrame as CSV\"\"\"\n",
    "        def generate_df_data():\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"Merged_Dataframe_{timestamp}.csv\"\n",
    "            content = df.to_csv(index=False)\n",
    "            return content.encode(), filename\n",
    "            \n",
    "        self._create_download_section(\n",
    "            \"Export Full Dataset\",\n",
    "            \"Download the complete merged dataset containing all processed data\",\n",
    "            generate_df_data,\n",
    "            'text/csv'\n",
    "        )\n",
    "\n",
    "    def setup_pivoted_data_export(self, merged_df, group_data):\n",
    "        \"\"\"Setup pivoted data export\"\"\"\n",
    "        def generate_pivoted_data():\n",
    "            def create_pivoted_df(df, abundance_columns):\n",
    "                melted_df = df.melt(\n",
    "                    id_vars=['unique ID'],\n",
    "                    value_vars=abundance_columns,\n",
    "                    var_name='Sample',\n",
    "                    value_name='Abundance'\n",
    "                )\n",
    "                pivoted = melted_df.pivot_table(\n",
    "                    index='Sample',\n",
    "                    columns='unique ID',\n",
    "                    values='Abundance'\n",
    "                )\n",
    "                pivoted.index.name = 'Abundance Values'\n",
    "                return pivoted\n",
    "\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"pivoted_data_{timestamp}.xlsx\"\n",
    "            \n",
    "            output = io.BytesIO()\n",
    "            with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
    "                for group_key, group_info in group_data.items():\n",
    "                    pivoted_df = create_pivoted_df(\n",
    "                        merged_df, \n",
    "                        group_info['abundance_columns']\n",
    "                    )\n",
    "                    if not pivoted_df.empty:\n",
    "                        pivoted_df.to_excel(\n",
    "                            writer, \n",
    "                            sheet_name=group_info['grouping_variable'],\n",
    "                            index=True\n",
    "                        )\n",
    "            \n",
    "            return output.getvalue(), filename\n",
    "            \n",
    "        self._create_download_section(\n",
    "            \"Pivoted Peptide Data Export\",\n",
    "            \"Download abundance values organized by sample and peptide ID\",\n",
    "            generate_pivoted_data,\n",
    "            'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "        )\n",
    "\n",
    "    def export_bioactive_data(self, merged_df, group_data):\n",
    "        \"\"\"Export bioactive function analysis\"\"\"\n",
    "        if not 'function' in merged_df.columns:\n",
    "            if group_data is None:\n",
    "                return\n",
    "            \n",
    "        def generate_bioactive_data():\n",
    "            results = self._bioactive_function_count_and_abundance_sum_avg(merged_df, group_data)\n",
    "            if not results:\n",
    "                raise ValueError(\"No bioactive data to export\")\n",
    "            \n",
    "            (summed_function_count, unique_function_counts, \n",
    "             unique_function_count_averages, unique_function_absorbance, \n",
    "             summed_function_abundance) = results\n",
    "\n",
    "            # Create DataFrames\n",
    "            peptide_count_df = pd.DataFrame.from_dict(\n",
    "                summed_function_count, \n",
    "                orient='index', \n",
    "                columns=['Counts of peptides']\n",
    "            )\n",
    "            function_count_df = pd.DataFrame.from_dict(\n",
    "                unique_function_counts, \n",
    "                orient='index'\n",
    "            ).fillna(0).astype(int)\n",
    "            combined_count_df = pd.concat([peptide_count_df, function_count_df], axis=1).T\n",
    "\n",
    "            peptide_absorbance_df = pd.DataFrame.from_dict(\n",
    "                summed_function_abundance, \n",
    "                orient='index', \n",
    "                columns=['Summed Abundance']\n",
    "            )\n",
    "            function_absorbance_df = pd.DataFrame.from_dict(\n",
    "                unique_function_absorbance, \n",
    "                orient='index'\n",
    "            ).fillna(0)\n",
    "            combined_absorbance_df = pd.concat(\n",
    "                [peptide_absorbance_df, function_absorbance_df], \n",
    "                axis=1\n",
    "            ).T\n",
    "\n",
    "            combined_df = pd.DataFrame(\n",
    "                index=combined_absorbance_df.index, \n",
    "                columns=combined_absorbance_df.columns\n",
    "            )\n",
    "            \n",
    "            for col in combined_absorbance_df.columns:\n",
    "                for idx in combined_absorbance_df.index:\n",
    "                    abundance = combined_absorbance_df.loc[idx, col]\n",
    "                    count = (combined_count_df.loc['Counts of peptides', col] \n",
    "                            if idx == 'Summed Abundance' \n",
    "                            else combined_count_df.loc[idx, col])\n",
    "                    combined_df.loc[idx, col] = \"-\" if (abundance == 0 and count == 0) else f\"{abundance:.2e} ({round(count)})\"\n",
    "            \n",
    "            combined_df.rename(index={'Summed Abundance': 'Total'}, inplace=True)\n",
    "\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"Processed_mbpdb_results_{timestamp}.xlsx\"\n",
    "            \n",
    "            output = io.BytesIO()\n",
    "            with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
    "                combined_df.to_excel(writer, sheet_name='combined', index=True)\n",
    "                combined_count_df.to_excel(writer, sheet_name='count', index=True)\n",
    "                combined_absorbance_df.to_excel(writer, sheet_name='absorbance', index=True)\n",
    "            \n",
    "            return output.getvalue(), filename\n",
    "        if 'function' in merged_df.columns:    \n",
    "            if group_data:\n",
    "                self._create_download_section(\n",
    "                    \"Export Bioactive Function Analysis\",\n",
    "                    \"Download the bioactive function analysis results in Excel format containing three sheets: combined, count, and absorbance\",\n",
    "                    generate_bioactive_data,\n",
    "                    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "                    )\n",
    "\n",
    "    def _bioactive_function_count_and_abundance_sum_avg(self, df, group_data):\n",
    "        \"\"\"Debug version of bioactive function counting and abundance calculation\"\"\"\n",
    "        \n",
    "        # Initialize result dictionaries\n",
    "        summed_function_count = {}\n",
    "        unique_function_counts = {}\n",
    "        unique_function_count_averages = {}\n",
    "        unique_function_absorbance = {}\n",
    "        summed_function_abundance = {}\n",
    "    \n",
    "\n",
    "    \n",
    "        # Iterate over each group\n",
    "        for group_id, group_info in group_data.items():\n",
    "            grouping_variable = group_info['grouping_variable']\n",
    "            abundance_column = f'Avg_{grouping_variable}'\n",
    "            \n",
    "            \n",
    "            # Check if abundance column exists\n",
    "            if abundance_column not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Filter and process data\n",
    "            temp_filter_df = df[['unique ID', 'function', abundance_column]].copy()\n",
    "            \n",
    "            # Filter non-zero and non-NaN values\n",
    "            temp_filter_df = temp_filter_df[\n",
    "                (temp_filter_df[abundance_column] != 0) & \n",
    "                temp_filter_df[abundance_column].notna()\n",
    "            ]\n",
    "            \n",
    "            # Drop duplicates\n",
    "            filtered_df = temp_filter_df.drop_duplicates(subset='unique ID')\n",
    "            \n",
    "            if filtered_df.empty:\n",
    "                continue\n",
    "                \n",
    "            # Calculate metrics\n",
    "            unique_peptide_count = filtered_df['unique ID'].nunique()\n",
    "            total_sum = filtered_df[abundance_column].sum()\n",
    "            \n",
    "            \n",
    "            # Store the totals\n",
    "            summed_function_abundance[grouping_variable] = total_sum\n",
    "            summed_function_count[grouping_variable] = unique_peptide_count\n",
    "            \n",
    "            # Process functions\n",
    "            filtered_df['function'] = filtered_df['function'].fillna('')\n",
    "            filtered_df['function'] = filtered_df['function'].str.split(';')\n",
    "            exploded_df = filtered_df.explode('function')\n",
    "            exploded_df['function'] = exploded_df['function'].str.strip()\n",
    "            exploded_df = exploded_df[exploded_df['function'] != '']\n",
    "            \n",
    "            if not exploded_df.empty:\n",
    "                \n",
    "                # Count functions\n",
    "                function_counts = exploded_df['function'].value_counts().to_dict()\n",
    "                unique_function_counts[grouping_variable] = function_counts\n",
    "                \n",
    "                # Calculate function abundances\n",
    "                function_grouped = exploded_df.groupby('function')[abundance_column].sum()\n",
    "                unique_function_absorbance[grouping_variable] = function_grouped.to_dict()\n",
    "                \n",
    "                # Calculate averages\n",
    "                num_columns_in_group = 1  # Since using averaged columns\n",
    "                function_averages = {func: count / num_columns_in_group \n",
    "                                   for func, count in function_counts.items()}\n",
    "                unique_function_count_averages[grouping_variable] = function_averages\n",
    "            \n",
    "        \n",
    "        return (summed_function_count, unique_function_counts, \n",
    "                unique_function_count_averages, unique_function_absorbance, \n",
    "                summed_function_abundance)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b506a35-a853-46ee-b193-3fce79decf57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139fbee310cc4e88becdcab244d1ab4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Process Data', style=ButtonStyle(), tooltip='Click â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d85e777f0f41b0896f44bc570f60c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4e7be3ad3c4a2bb4f1d1fb945604dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a68e22165f24a0dba8032d49d5f57db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f14f4fe3eca40dfb11941706f44e279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a934227ba20848eb9fb8ae80f737123e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DataProcessingController:\n",
    "    def __init__(self, workflow):\n",
    "        self.workflow = workflow  # Store reference to workflow\n",
    "        self.export_manager = ExportManager()\n",
    "        self.combiner = None\n",
    "        self.merged_df = None\n",
    "        \n",
    "        # Create processing button\n",
    "        self.process_button = widgets.Button(\n",
    "            description='Process Data',\n",
    "            button_style='success',\n",
    "            tooltip='Click to start data processing'\n",
    "        )\n",
    "        \n",
    "        # Create export button (initially disabled)\n",
    "        self.export_button = widgets.Button(\n",
    "            description='Export Data',\n",
    "            button_style='info',\n",
    "            tooltip='Process data first to enable export',\n",
    "            layout=widgets.Layout(margin='0 0 0 10px'),  # Add margin to separate buttons\n",
    "            disabled=True  # Start disabled\n",
    "        )\n",
    "        \n",
    "        # Create button container\n",
    "        self.button_container = widgets.HBox([self.process_button, self.export_button])\n",
    "        \n",
    "        # Create separate output areas\n",
    "        self.process_output = widgets.Output()\n",
    "        self.export_output = widgets.Output()\n",
    "        self.search_output = widgets.Output()\n",
    "        self.stats_output = widgets.Output()\n",
    "        self.grid_output = widgets.Output()\n",
    "        \n",
    "        # Set up button callbacks\n",
    "        self.process_button.on_click(self._on_process_clicked)\n",
    "        self.export_button.on_click(self._on_export_clicked)\n",
    "        \n",
    "    def display_interactive_results(self, df):\n",
    "        \"\"\"Display interactive grid with row search functionality\"\"\"\n",
    "        if df is not None:\n",
    "            # Create search widget\n",
    "            search_widget = widgets.Text(\n",
    "                placeholder='Search for data in rows...',\n",
    "                description='Search:',\n",
    "                layout=widgets.Layout(width='50%'),\n",
    "                style={'description_width': 'initial'}\n",
    "            )\n",
    "            \n",
    "            def get_column_category(col):\n",
    "                \"\"\"Determine category for each column\"\"\"\n",
    "                if col.startswith('Avg_'):\n",
    "                    return 'Average Abundance'\n",
    "                if col.startswith('SEM_'):\n",
    "                    return 'Standard Error Mean'\n",
    "                elif col in self.workflow.data_transformer.mbpdb_results.columns:\n",
    "                    return 'MBPDB Search Results'\n",
    "                else:\n",
    "                    return 'Peptidomic Data'\n",
    "\n",
    "            # Create multi-level columns while preserving order\n",
    "            column_tuples = [(get_column_category(col), col) for col in df.columns]\n",
    "            \n",
    "            df_display = df.copy()\n",
    "            df_display.columns = pd.MultiIndex.from_tuples(column_tuples)\n",
    "            \n",
    "            def create_grid(df_to_display):\n",
    "                grid = DataGrid(\n",
    "                    df_to_display, \n",
    "                    selection_mode='cell', \n",
    "                    editable=False,\n",
    "                    layout=widgets.Layout(height='600px')\n",
    "                )\n",
    "                grid.auto_fit_columns = True\n",
    "                grid.base_row_size = 25\n",
    "                grid.base_column_size = 150\n",
    "                grid.auto_fit_params = {'area': 'column', 'padding': 10}\n",
    "                return grid\n",
    "            \n",
    "            def on_search_change(change):\n",
    "                with self.grid_output:\n",
    "                    self.grid_output.clear_output()\n",
    "                    \n",
    "                    search_term = change['new'].strip()\n",
    "                    if search_term:\n",
    "                        str_df = df_display.astype(str)\n",
    "                        mask = str_df.apply(\n",
    "                            lambda row: row.str.contains(search_term, case=False, na=False).any(),\n",
    "                            axis=1\n",
    "                        )\n",
    "                        filtered_df = df_display[mask]\n",
    "                        \n",
    "                        with self.stats_output:\n",
    "                            self.stats_output.clear_output()\n",
    "                            print(f\"Found {len(filtered_df)} matching rows out of {len(df_display)} total rows\")\n",
    "                    else:\n",
    "                        filtered_df = df_display\n",
    "                        with self.stats_output:\n",
    "                            self.stats_output.clear_output()\n",
    "                    \n",
    "                    display(create_grid(filtered_df))\n",
    "            \n",
    "            search_widget.observe(on_search_change, names='value')\n",
    "\n",
    "            # Display search interface\n",
    "            with self.search_output:\n",
    "                self.search_output.clear_output()\n",
    "                display(search_widget)\n",
    "            \n",
    "            # Initialize grid display\n",
    "            with self.grid_output:\n",
    "                self.grid_output.clear_output()\n",
    "                display(create_grid(df_display))\n",
    "            \n",
    "        else:\n",
    "            print(\"No data to display\")\n",
    "\n",
    "        \n",
    "    def _on_process_clicked(self, b):\n",
    "        # Clear all outputs except export\n",
    "        self.process_output.clear_output()\n",
    "        self.search_output.clear_output()\n",
    "        self.stats_output.clear_output()\n",
    "        self.grid_output.clear_output()\n",
    "        \n",
    "        with self.process_output:           \n",
    "            # Pass the actual data_transformer, not the workflow\n",
    "            self.combiner = CombineAverageDataframes(\n",
    "                self.workflow.data_transformer,  # Pass the data_transformer directly\n",
    "                self.workflow.group_processor, \n",
    "                self.workflow.protein_handler\n",
    "            )\n",
    "            self.merged_df = self.combiner.process_data(self.workflow.group_processor.group_data)\n",
    "            \n",
    "            if self.merged_df is not None:\n",
    "                print(\"\\nData processing completed successfully!\")\n",
    "                print(f\"Final results row count: {self.merged_df.shape[0]}\")\n",
    "                print(f\"Final results column count: {self.merged_df.shape[1]}\")\n",
    "                \n",
    "                # Enable export button after successful processing\n",
    "                self.export_button.disabled = False\n",
    "                self.export_button.tooltip = 'Click to show export options'\n",
    "                \n",
    "                self.display_interactive_results(self.merged_df)\n",
    "            else:\n",
    "                print(\"Error: No data was processed\")\n",
    "                # Keep export button disabled if processing failed\n",
    "                self.export_button.disabled = True\n",
    "                    \n",
    "    def _on_export_clicked(self, b):\n",
    "        with self.export_output:\n",
    "            clear_output()\n",
    "            #display(HTML(\"<h2>Export:</h2>\"))\n",
    "            \n",
    "            # Use workflow's data_transformer instance\n",
    "            if (hasattr(self.workflow.data_transformer, 'mbpdb_results') and \n",
    "                self.workflow.data_transformer.mbpdb_results is not None):\n",
    "                self.export_manager.export_mbpdb_results(self.workflow.data_transformer.mbpdb_results)\n",
    "            \n",
    "            # Use workflow's group_processor instance\n",
    "            self.export_manager.export_group_data(self.workflow.group_processor.group_data)\n",
    "            \n",
    "            if self.merged_df is not None:\n",
    "                self.export_manager.export_bioactive_data(\n",
    "                    self.merged_df, \n",
    "                    self.workflow.group_processor.group_data\n",
    "                )\n",
    "                self.export_manager.export_dataframe(self.merged_df)\n",
    "                self.export_manager.setup_pivoted_data_export(\n",
    "                    self.merged_df,\n",
    "                    self.workflow.group_processor.group_data\n",
    "                )\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the complete interface\"\"\"\n",
    "        display(self.button_container)\n",
    "        display(self.process_output)\n",
    "        display(self.export_output)\n",
    "        display(self.search_output)\n",
    "        display(self.stats_output)\n",
    "        display(self.grid_output)\n",
    "\n",
    "# Initialize the controller\n",
    "controller = DataProcessingController(workflow)\n",
    "# Display the interface\n",
    "controller.display()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Vt5dV-pTHRWngML-2nDQAR6_P_KFsIx4",
     "timestamp": 1712158917217
    },
    {
     "file_id": "1l7fpCQepyE1pJq2O5QfOHVv9a4VFpr-B",
     "timestamp": 1712094574841
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
