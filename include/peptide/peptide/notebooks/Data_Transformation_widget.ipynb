{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "278kpom78fOP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23025,
     "status": "ok",
     "timestamp": 1712094448807,
     "user": {
      "displayName": "Russell Kuhfeld",
      "userId": "14760569517288879712"
     },
     "user_tz": 420
    },
    "id": "278kpom78fOP",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "95f300e9-4d05-4fe1-a725-f5c3eea6cf80",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Install/Import packages & define key varribles and functions\n",
    "# Run install script\n",
    "# %chmod +x setup_jupyterlab.sh\n",
    "# %./setup_jupyterlab.sh\n",
    "\n",
    "# Import necessary libraries for the script to function.\n",
    "import pandas as pd\n",
    "import tempfile, csv, json, re, os, shutil, io, base64, time, subprocess, sqlite3, zipfile, base64\n",
    "from io import StringIO, BytesIO\n",
    "import numpy as np\n",
    "\n",
    "#from django.conf import settings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.formula.api import ols\n",
    "#from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import warnings\n",
    "\n",
    "\n",
    "from itertools import combinations\n",
    "from ipydatagrid import DataGrid\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "import traitlets\n",
    "from traitlets import HasTraits, Instance, observe\n",
    "\n",
    "# Global variable declaration\n",
    "\n",
    "import _settings as settings\n",
    "global spec_translate_list\n",
    "spec_translate_list = settings.SPEC_TRANSLATE_LIST\n",
    "# Set the default font to Calibri\n",
    "#matplotlib.rcParams['font.family'] = 'Calibri'\n",
    "\n",
    "# Create global output areas\n",
    "protein_mapping_output = widgets.Output()\n",
    "group_processing_output = widgets.Output()\n",
    "\n",
    "def find_species(header, spec_translate_list):\n",
    "    \"\"\"Search for a species in the header and return the first element (species name) from the list.\"\"\"\n",
    "    header_lower = header.lower()\n",
    "    for spec_group in spec_translate_list:\n",
    "        for term in spec_group[1:]:  # Iterate over possible species names/terms except the first element\n",
    "            if term.lower() in header_lower:\n",
    "                return spec_group[0]  # Return the first element of the list (main species name)\n",
    "    return \"unknown\"  # Return unknown if no species match is found\n",
    "\n",
    "def parse_headers():\n",
    "    fasta_dict = {}\n",
    "    with open(\"protein_headers.txt\", 'r') as file:\n",
    "        protein_id = \"\"\n",
    "        protein_name = \"\"\n",
    "        species = \"\"\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if protein_id:\n",
    "                    # Save the previous protein entry in the dictionary\n",
    "                    fasta_dict[protein_id] = {\n",
    "                        \"name\": protein_name,\n",
    "                        \"species\": species\n",
    "                    }\n",
    "                sequence = \"\"\n",
    "                header_parts = line[1:].split('|')\n",
    "                if len(header_parts) > 2:\n",
    "                    protein_id = header_parts[1]\n",
    "                    protein_name_full = re.split(r' OS=', header_parts[2])[0]\n",
    "                    if ' ' in protein_name_full:\n",
    "\n",
    "                        protein_name = protein_name_full#.split()[1]\n",
    "                    else:\n",
    "                        protein_name = protein_name_full\n",
    "                    # Find species in the header\n",
    "                    species = find_species(line, spec_translate_list)\n",
    "\n",
    "        if protein_id:\n",
    "            # Save the last protein entry in the dictionary\n",
    "            fasta_dict[protein_id] = {\n",
    "                \"name\": protein_name,\n",
    "                \"species\": species\n",
    "            }\n",
    "    return fasta_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "4d06c8cd-9a5f-45cb-bf09-1a96f1eb77bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DataTransformation(HasTraits):\n",
    "    pd_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    mbpdb_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    # pd_results_cleaned = Instance(pd.DataFrame, allow_none=True)\n",
    "    search_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    protein_dict = {}  # Add explicit trait for protein_dict\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pd_results = pd.DataFrame()\n",
    "        # self.pd_results_cleaned = pd.DataFrame()\n",
    "        self.mbpdb_results = pd.DataFrame()\n",
    "        self.search_results = pd.DataFrame()\n",
    "        self.protein_dict = parse_headers()\n",
    "        self.output_area = None\n",
    "        self.mbpdb_uploader = None\n",
    "        self.pd_uploader = None\n",
    "        self.fasta_uploader = None\n",
    "        self.search_widget = None\n",
    "        self.search_progress = None\n",
    "\n",
    "    def create_download_link(self, file_path, label):\n",
    "        \"\"\"Create a download link for a file.\"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            # Read file content and encode it as base64\n",
    "            with open(file_path, 'rb') as f:\n",
    "                content = f.read()\n",
    "            b64_content = base64.b64encode(content).decode('utf-8')\n",
    "\n",
    "            # Generate the download link HTML\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <a download=\"{os.path.basename(file_path)}\" \n",
    "                   href=\"data:application/octet-stream;base64,{b64_content}\" \n",
    "                   style=\"color: #0366d6; text-decoration: none; margin-left: 20px; font-size: 14px;\">\n",
    "                    {label}\n",
    "                </a>\n",
    "            \"\"\")\n",
    "        else:\n",
    "            # Show an error message if the file does not exist\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <span style=\"color: red; margin-left: 20px; font-size: 14px;\">\n",
    "                    File \"{file_path}\" not found!\n",
    "                </span>\n",
    "            \"\"\")\n",
    "\n",
    "    def setup_search_ui(self, peptides):\n",
    "        \"\"\"Initialize and display the search UI\"\"\"\n",
    "        # Create dropdown for similarity threshold\n",
    "        self.threshold_dropdown = widgets.Dropdown(\n",
    "            options=list(range(0, 101, 10)),\n",
    "            value=80,\n",
    "            description='Similarity Threshold:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "\n",
    "        # Create search button\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search Peptides',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "\n",
    "        # Progress indicator\n",
    "        self.search_progress = widgets.HTML(\n",
    "            value=\"\",\n",
    "            layout=widgets.Layout(margin='10px 0px')\n",
    "        )\n",
    "\n",
    "        # Connect button click to handler\n",
    "        self.search_button.on_click(lambda b: self._on_search_click(b, ))\n",
    "\n",
    "        # Create layout\n",
    "        self.search_widget = widgets.VBox([\n",
    "            widgets.HBox([\n",
    "                self.threshold_dropdown,\n",
    "                self.search_button\n",
    "            ], layout=widgets.Layout(align_items='center')),\n",
    "            self.search_progress\n",
    "        ])\n",
    "\n",
    "        display(self.search_widget)\n",
    "\n",
    "    def _on_search_click(self, b):\n",
    "        \"\"\"Handle search button click\"\"\"\n",
    "        with self.search_output_area:\n",
    "            clear_output()\n",
    "\n",
    "            if self.pd_results is None or self.pd_results.empty:\n",
    "                display(HTML(\"<b style='color:red'>Please upload peptidomic data first.</b>\"))\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                # Extract sequences from peptidomic data\n",
    "                self.peptides = self._extract_sequences(self.pd_results)\n",
    "\n",
    "                if not self.peptides:\n",
    "                    display(HTML(\"<b style='color:red'>No valid sequences found in peptidomic data.</b>\"))\n",
    "                    return\n",
    "\n",
    "                display(HTML(f\"<b style='color:blue'>Found {len(self.peptides)} sequences. Searching database...</b>\"))\n",
    "\n",
    "                # Perform search\n",
    "                results = self._search_peptides_comprehensive(\n",
    "                    self.peptides,\n",
    "                    similarity_threshold=self.threshold_dropdown.value\n",
    "                )\n",
    "                # Format results if we have any matches\n",
    "                if not results.empty:\n",
    "                    self.mbpdb_results = self._format_search_results_with_matches(results)\n",
    "                    display(\n",
    "                        HTML(f\"<b style='color:green'>Search complete! Found {len(self.mbpdb_results)} matches</b>\"))\n",
    "                else:\n",
    "                    self.mbpdb_results = results\n",
    "                    display(HTML(\"<b style='color:orange'>No matches found in the database.</b>\"))\n",
    "\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red'>Error: {str(e)}</b>\"))\n",
    "                self.mbpdb_results = pd.DataFrame()\n",
    "\n",
    "    def _search_peptides_comprehensive(self, peptides, similarity_threshold=100):\n",
    "        \"\"\"Search for peptides with BLAST-based similarity matching\"\"\"\n",
    "\n",
    "        # WORK_DIRECTORY = '/home/kuhfeldrf/mbpdb/include/peptide/uploads/temp'\n",
    "        # conn = sqlite3.connect('/home/kuhfeldrf/mbpdb/include/peptide/db.sqlite3')\n",
    "\n",
    "        WORK_DIRECTORY = '../../uploads/temp'\n",
    "        conn = sqlite3.connect('../../db.sqlite3')\n",
    "        work_path = self._create_work_directory(WORK_DIRECTORY)\n",
    "\n",
    "        fasta_db_path = os.path.join(work_path, \"db.fasta\")\n",
    "        results = []\n",
    "        extra_info = defaultdict(list)\n",
    "\n",
    "        # Create database with all peptides for BLAST\n",
    "        query = \"SELECT p.id, p.peptide FROM peptide_peptideinfo p\"\n",
    "        db_peptides = pd.read_sql_query(query, conn)\n",
    "\n",
    "        # Create BLAST database\n",
    "        with open(fasta_db_path, 'w') as f:\n",
    "            for _, row in db_peptides.iterrows():\n",
    "                f.write(f\">{row['id']}\\n{row['peptide']}\\n\")\n",
    "\n",
    "        self._make_blast_db(fasta_db_path)\n",
    "\n",
    "        for peptide in self.peptides:\n",
    "            if similarity_threshold == 100 or len(peptide) <4:\n",
    "                query = \"\"\"\n",
    "                SELECT DISTINCT\n",
    "                    ? as search_peptide,\n",
    "                    pi.pid as protein_id,\n",
    "                    p.id as peptide_id,\n",
    "                    p.peptide,\n",
    "                    pi.desc as protein_description,\n",
    "                    pi.species,\n",
    "                    p.intervals,\n",
    "                    f.function,\n",
    "                    r.additional_details,\n",
    "                    r.ic50,\n",
    "                    r.inhibition_type,\n",
    "                    r.inhibited_microorganisms,\n",
    "                    r.ptm,\n",
    "                    r.title,\n",
    "                    r.authors,\n",
    "                    r.abstract,\n",
    "                    r.doi,\n",
    "                    'sequence' as search_type,\n",
    "                    'IDENTITY' as scoring_matrix\n",
    "                FROM peptide_peptideinfo p\n",
    "                JOIN peptide_proteininfo pi ON p.protein_id = pi.id\n",
    "                LEFT JOIN peptide_function f ON f.pep_id = p.id\n",
    "                LEFT JOIN peptide_reference r ON r.func_id = f.id\n",
    "                WHERE p.peptide = ?\n",
    "                \"\"\"\n",
    "                df = pd.read_sql_query(query, conn, params=[peptide, peptide])\n",
    "                results.append(df)\n",
    "            else:\n",
    "                # Run BLASTP search for similarity matching\n",
    "                query_path = os.path.join(work_path, \"query.fasta\")\n",
    "                with open(query_path, \"w\") as query_file:\n",
    "                    query_file.write(f\">pep_query\\n{peptide}\\n\")\n",
    "\n",
    "                output_path = os.path.join(work_path, \"blastp_short.out\")\n",
    "                blast_args = [\n",
    "                    \"blastp\",\n",
    "                    \"-query\", query_path,\n",
    "                    \"-db\", fasta_db_path,\n",
    "                    \"-outfmt\", \"6 std ppos qcovs qlen slen positive\",\n",
    "                    \"-evalue\", \"1000\",\n",
    "                    \"-word_size\", \"2\",\n",
    "                    \"-matrix\", \"IDENTITY\",\n",
    "                    \"-threshold\", \"1\",\n",
    "                    \"-task\", \"blastp-short\",\n",
    "                    \"-out\", output_path\n",
    "                ]\n",
    "\n",
    "                subprocess.check_output(blast_args, stderr=subprocess.STDOUT)\n",
    "\n",
    "                # Process BLAST results\n",
    "                search_ids = self._process_blast_results(output_path, similarity_threshold, extra_info)\n",
    "\n",
    "                if search_ids:\n",
    "                    df = self._fetch_peptide_data(conn, peptide, search_ids)\n",
    "                    self._add_blast_details(df, extra_info)\n",
    "                    results.append(df)\n",
    "\n",
    "        conn.close()\n",
    "        self._cleanup_work_directory(WORK_DIRECTORY)\n",
    "\n",
    "        return self._combine_results(results)\n",
    "\n",
    "    def _create_work_directory(self, base_dir):\n",
    "        \"\"\"Create a working directory for BLAST operations\"\"\"\n",
    "        path = os.path.join(base_dir, f'work_{int(round(time.time() * 1000))}')\n",
    "        os.makedirs(path)\n",
    "        return path\n",
    "\n",
    "    def _make_blast_db(self, library_fasta_path):\n",
    "        \"\"\"Create BLAST database from FASTA file\"\"\"\n",
    "        subprocess.check_output(\n",
    "            ['makeblastdb', '-in', library_fasta_path, '-dbtype', 'prot'],\n",
    "            stderr=subprocess.STDOUT\n",
    "        )\n",
    "\n",
    "    def _process_blast_results(self, output_path, similarity_threshold, extra_info):\n",
    "        \"\"\"Process BLAST results and collect search IDs\"\"\"\n",
    "        search_ids = []\n",
    "        csv.register_dialect('blast_dialect', delimiter='\\t')\n",
    "\n",
    "        with open(output_path, \"r\") as output_file:\n",
    "            blast_data = csv.DictReader(\n",
    "                output_file,\n",
    "                fieldnames=['query', 'subject', 'percid', 'align_len', 'mismatches',\n",
    "                            'gaps', 'qstart', 'qend', 'sstart', 'send', 'evalue',\n",
    "                            'bitscore', 'ppos', 'qcov', 'qlen', 'slen', 'numpos'],\n",
    "                dialect='blast_dialect'\n",
    "            )\n",
    "\n",
    "            for row in blast_data:\n",
    "                tlen = float(row['slen']) if float(row['slen']) > float(row['qlen']) else float(row['qlen'])\n",
    "                simcalc = 100 * ((float(row['numpos']) - float(row['gaps'])) / tlen)\n",
    "\n",
    "                if simcalc >= similarity_threshold:\n",
    "                    search_ids.append(row['subject'])\n",
    "                    extra_info[row['subject']] = [\n",
    "                        f\"{simcalc:.2f}\", row['qstart'], row['qend'], row['sstart'],\n",
    "                        row['send'], row['evalue'], row['align_len'], row['mismatches'],\n",
    "                        row['gaps']\n",
    "                    ]\n",
    "\n",
    "        return search_ids\n",
    "\n",
    "    def _fetch_peptide_data(self, conn, peptide, search_ids):\n",
    "        \"\"\"Fetch peptide data from database\"\"\"\n",
    "        placeholders = ','.join(['?' for _ in search_ids])\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            ? as search_peptide,\n",
    "            pi.pid as protein_id,\n",
    "            p.id as peptide_id,\n",
    "            p.peptide,\n",
    "            pi.desc as protein_description,\n",
    "            pi.species,\n",
    "            p.intervals,\n",
    "            f.function,\n",
    "            r.additional_details,\n",
    "            r.ic50,\n",
    "            r.inhibition_type,\n",
    "            r.inhibited_microorganisms,\n",
    "            r.ptm,\n",
    "            r.title,\n",
    "            r.authors,\n",
    "            r.abstract,\n",
    "            r.doi,\n",
    "            'sequence' as search_type,\n",
    "            'IDENTITY' as scoring_matrix\n",
    "        FROM peptide_peptideinfo p\n",
    "        JOIN peptide_proteininfo pi ON p.protein_id = pi.id\n",
    "        LEFT JOIN peptide_function f ON f.pep_id = p.id\n",
    "        LEFT JOIN peptide_reference r ON r.func_id = f.id\n",
    "        WHERE p.id IN ({placeholders})\n",
    "        \"\"\"\n",
    "\n",
    "        return pd.read_sql_query(query, conn, params=[peptide] + search_ids)\n",
    "\n",
    "    def _add_blast_details(self, df, extra_info):\n",
    "        \"\"\"Add BLAST details to DataFrame\"\"\"\n",
    "        for idx, row in df.iterrows():\n",
    "            if str(row['peptide_id']) in extra_info:\n",
    "                blast_details = extra_info[str(row['peptide_id'])]\n",
    "                df.at[idx, '% Alignment'] = blast_details[0]\n",
    "                df.at[idx, 'Query start'] = blast_details[1]\n",
    "                df.at[idx, 'Query end'] = blast_details[2]\n",
    "                df.at[idx, 'Subject start'] = blast_details[3]\n",
    "                df.at[idx, 'Subject end'] = blast_details[4]\n",
    "                df.at[idx, 'e-value'] = blast_details[5]\n",
    "                df.at[idx, 'Alignment length'] = blast_details[6]\n",
    "                df.at[idx, 'Mismatches'] = blast_details[7]\n",
    "                df.at[idx, 'Gap opens'] = blast_details[8]\n",
    "\n",
    "    def _cleanup_work_directory(self, work_directory):\n",
    "        \"\"\"Clean up old work directories\"\"\"\n",
    "        try:\n",
    "            dirs = [f for f in os.scandir(work_directory) if f.is_dir()]\n",
    "            dirs.sort(key=lambda x: os.path.getmtime(x.path), reverse=True)\n",
    "\n",
    "            for dir_entry in dirs[25:]:\n",
    "                try:\n",
    "                    shutil.rmtree(dir_entry.path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _combine_results(self, results):\n",
    "        \"\"\"Combine and format final results\"\"\"\n",
    "        if not results:\n",
    "            mbpdb_columns = [\n",
    "                'search_peptide', 'protein_id', 'peptide', 'protein_description',\n",
    "                'species', 'intervals', 'function', 'additional_details', 'ic50',\n",
    "                'inhibition_type', 'inhibited_microorganisms', 'ptm', 'title',\n",
    "                'authors', 'abstract', 'doi', 'search_type', 'scoring_matrix'\n",
    "            ]\n",
    "            return pd.DataFrame(columns=mbpdb_columns)\n",
    "\n",
    "        final_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "        if 'peptide_id' in final_results.columns:\n",
    "            final_results = final_results.drop('peptide_id', axis=1)\n",
    "\n",
    "        sort_columns = ['search_peptide']\n",
    "        if '% Alignment' in final_results.columns:\n",
    "            sort_columns.append('% Alignment')\n",
    "\n",
    "        return final_results.sort_values(\n",
    "            sort_columns,\n",
    "            ascending=[True] + [False] * (len(sort_columns) - 1)\n",
    "        )\n",
    "\n",
    "    def _format_search_results_with_matches(self, final_results):\n",
    "        \"\"\"Format search results with matches\"\"\"\n",
    "        if '% Alignment' in final_results.columns:\n",
    "            final_results['% Alignment'] = pd.to_numeric(\n",
    "                final_results['% Alignment'],\n",
    "                errors='coerce'\n",
    "            )\n",
    "\n",
    "        grouped = final_results.groupby([\"search_peptide\", \"function\"], as_index=False)\n",
    "        aggregated_results = []\n",
    "        processed_indices = set()\n",
    "\n",
    "        for _, group in grouped:\n",
    "            if len(group) > 1:\n",
    "                aggregated_row = self._aggregate_group_data(group)\n",
    "                aggregated_results.append(aggregated_row)\n",
    "                processed_indices.update(group.index)\n",
    "\n",
    "        remaining_rows = final_results.loc[~final_results.index.isin(processed_indices)]\n",
    "        aggregated_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "        return pd.concat([aggregated_df, remaining_rows], ignore_index=True)\n",
    "\n",
    "    def _aggregate_group_data(self, group):\n",
    "        \"\"\"Aggregate data for a group of results\"\"\"\n",
    "\n",
    "        def enumerate_field(field):\n",
    "            if field in group.columns and not group[field].dropna().empty:\n",
    "                valid_values = set(group[field].dropna().astype(str).str.strip())\n",
    "                valid_values = {val for val in valid_values if val != ''}\n",
    "                if len(valid_values) > 1:\n",
    "                    return \"; \".join([f\"{i + 1}) {val}\" for i, val in enumerate(valid_values)])\n",
    "                elif len(valid_values) == 1:\n",
    "                    return next(iter(valid_values))\n",
    "                return ''\n",
    "            return ''\n",
    "\n",
    "        return {col: enumerate_field(col) for col in group.columns}\n",
    "\n",
    "    def setup_data_loading_ui(self):\n",
    "        \"\"\"Initialize and display the data loading UI with integrated search and help tooltips\"\"\"\n",
    "\n",
    "        def create_help_icon(tooltip_text):\n",
    "            \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "            help_icon = widgets.HTML(\n",
    "                value='<i class=\"fa fa-question-circle\" style=\"color: #007bff;\"></i>',\n",
    "                layout=widgets.Layout(width='25px', margin='2px 5px')\n",
    "            )\n",
    "            help_icon.add_class('jupyter-widgets')\n",
    "            help_icon.add_class('widget-html')\n",
    "            return widgets.HTML(\n",
    "                f'<div title=\"{tooltip_text}\" style=\"display: inline-block;\">{help_icon.value}</div>'\n",
    "            )\n",
    "\n",
    "        def create_labeled_uploader(widget, label, tooltip):\n",
    "            \"\"\"Create an uploader with label and help icon\"\"\"\n",
    "            return widgets.HBox([\n",
    "                widget,\n",
    "                create_help_icon(tooltip)\n",
    "            ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        # Create file upload widgets with the same configurations\n",
    "        self.mbpdb_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload MBPDB File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        self.pd_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload Peptidomic File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        self.fasta_uploader = widgets.FileUpload(\n",
    "            accept='.fasta',\n",
    "            multiple=True,\n",
    "            description='Upload FASTA Files',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        # Create search interface\n",
    "        self.threshold_dropdown = widgets.Dropdown(\n",
    "            options=list(range(0, 101, 10)),\n",
    "            value=80,\n",
    "            description='Similarity Threshold (%):',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='225px')\n",
    "        )\n",
    "\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search Database',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "\n",
    "\n",
    "        # Create output areas\n",
    "        self.output_area = widgets.Output()\n",
    "        self.search_output_area = widgets.Output()\n",
    "\n",
    "        mbpdb_box = widgets.HBox([\n",
    "            widgets.HTML(\"\"\"\n",
    "                    <div margin-bottom: 5px;'>\n",
    "                        <b>Option 1: Upload File</b>\n",
    "                    </div>\n",
    "                \"\"\"),\n",
    "            self.create_download_link(\n",
    "                \"example_MBPDB_search.tsv\",\n",
    "                \"Example\"\n",
    "            )\n",
    "        ])\n",
    "        # Create MBPDB options section\n",
    "        mbpdb_options = widgets.HBox([widgets.VBox([\n",
    "            mbpdb_box,\n",
    "            create_labeled_uploader(\n",
    "                self.mbpdb_uploader,\n",
    "                \"MBPDB File\",\n",
    "                \"Upload your own MBPDB file (optional)\"\n",
    "            )\n",
    "        ]),\n",
    "            widgets.HTML(\"<div style='margin: 0 20px; line-height: 100px;'><b>OR</b></div>\"),\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(\"<div style='font-weight: bold; margin-bottom: 5px;'>Option 2: Search Database</div>\"),\n",
    "                widgets.HBox([\n",
    "                    self.threshold_dropdown,\n",
    "                    self.search_button,\n",
    "                    create_help_icon(\"Search peptides against the MBPDB (optional)\")\n",
    "                ], layout=widgets.Layout(align_items='center'))\n",
    "            ])\n",
    "        ], layout=widgets.Layout(align_items='center', margin='0'))\n",
    "\n",
    "        # Create peptide file uploader box with example link\n",
    "        peptide_box = widgets.HBox([\n",
    "            create_labeled_uploader(\n",
    "                self.pd_uploader,\n",
    "                \"Peptidomic File\",\n",
    "                \"Upload peptide groups data from Proteome Discover export file (required)\"\n",
    "            ),\n",
    "            self.create_download_link(\n",
    "                \"example_peptide_data.csv\",\n",
    "                \"Example\"\n",
    "            )\n",
    "        ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        # Create FASTA uploader box with example link\n",
    "        fasta_box = widgets.HBox([\n",
    "            create_labeled_uploader(\n",
    "                self.fasta_uploader,\n",
    "                \"FASTA Files\",\n",
    "                \"Upload Protein FASTA file used in Proteome Discoverer Search (optional)\"\n",
    "            ),\n",
    "            self.create_download_link(\n",
    "                \"example_fasta.fasta\",\n",
    "                \"Example\"\n",
    "            )\n",
    "        ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        # Create main container\n",
    "        main_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Upload Peptidomic Data Files:</u></h3>\"),\n",
    "            peptide_box,\n",
    "            widgets.HTML(\"<h3 style='margin-bottom: 0;'><u>MBPDB Data (Optional):</u></h3>\"),\n",
    "            mbpdb_options,\n",
    "            widgets.HTML(\"<h3><u>Upload Protein FASTA Files (Optional):</u></h3>\"),\n",
    "            fasta_box,\n",
    "            widgets.HTML(\"<br>\"),\n",
    "            widgets.HTML(\"<div style='margin-top: 10px;'></div>\"),\n",
    "            self.output_area,\n",
    "            self.search_output_area\n",
    "        ])\n",
    "\n",
    "        # Register observers\n",
    "        self.pd_uploader.observe(self._on_pd_upload_change, names='value')\n",
    "        self.mbpdb_uploader.observe(self._on_mbpdb_upload_change, names='value')\n",
    "        self.fasta_uploader.observe(self._on_fasta_upload_change, names='value')\n",
    "        self.search_button.on_click(self._on_search_click)\n",
    "\n",
    "        # Add Font Awesome CSS for help icons\n",
    "        display(widgets.HTML(\"\"\"\n",
    "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\">\n",
    "        \"\"\"))\n",
    "\n",
    "        display(main_container)\n",
    "    \n",
    "    def _extract_sequences(self, df):\n",
    "        \"\"\"Extract sequences from peptidomic data\"\"\"\n",
    "        if 'Sequence' not in df.columns:\n",
    "            # First create Sequence column with NaN values\n",
    "            df['Sequence'] = pd.NA\n",
    "            \n",
    "            def extract_sequence(annotated_seq):\n",
    "                if pd.isna(annotated_seq):\n",
    "                    return pd.NA\n",
    "                \n",
    "                # Split by comma if present to handle multiple sequences\n",
    "                if ',' in annotated_seq:\n",
    "                    sequences = []\n",
    "                    for seq in annotated_seq.split(','):\n",
    "                        seq = seq.strip()\n",
    "                        # Handle [X].SEQUENCE.[X] format\n",
    "                        if '.' in seq:\n",
    "                            parts = seq.split('.')\n",
    "                            if len(parts) > 1:\n",
    "                                sequences.append(parts[1])\n",
    "                        # Handle plain sequence\n",
    "                        else:\n",
    "                            sequences.append(seq)\n",
    "                    return sequences\n",
    "                \n",
    "                # Single sequence case\n",
    "                # Handle [X].SEQUENCE.[X] format\n",
    "                if '.' in annotated_seq:\n",
    "                    parts = annotated_seq.split('.')\n",
    "                    if len(parts) > 1:\n",
    "                        return parts[1]\n",
    "                \n",
    "                # Handle plain sequence\n",
    "                return annotated_seq\n",
    "            \n",
    "            # Apply the extraction function and explode the results\n",
    "            df['Sequence'] = df['Annotated Sequence'].apply(extract_sequence)\n",
    "            # Explode sequences if they're in a list (from comma separation)\n",
    "            df = df.explode('Sequence')\n",
    "            \n",
    "        return df['Sequence'].dropna().unique().tolist()\n",
    "\n",
    "    def _on_pd_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    self.pd_results, pd_status = self._load_data(\n",
    "                        file_data,\n",
    "                        required_columns=['Positions in Proteins'],\n",
    "                        file_type='Peptidomic'\n",
    "                    )\n",
    "                    if pd_status == 'yes' and self.pd_results is not None:\n",
    "                        display(HTML(\n",
    "                            f'<b style=\"color:green;\">Peptidomic data imported with {self.pd_results.shape[0]} rows and {self.pd_results.shape[1]} columns.</b>'))\n",
    "\n",
    "    def _on_mbpdb_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    self.mbpdb_results, mbpdb_status = self._load_data(\n",
    "                        file_data,\n",
    "                        required_columns=['Search peptide', 'Protein ID', 'Peptide'],\n",
    "                        file_type='MBPDB'\n",
    "                    )\n",
    "                    if mbpdb_status == 'yes' and self.mbpdb_results is not None:\n",
    "                        self.mbpdb_results.rename(columns={\n",
    "                            'Search peptide': 'search_peptide',\n",
    "                            'Protein ID': 'protein_id',\n",
    "                            'Peptide': 'peptide',\n",
    "                            'Protein description': 'protein_description',\n",
    "                            'Species': 'species',\n",
    "                            'Intervals': 'intervals',\n",
    "                            'Function': 'function',\n",
    "                            'Additional details': 'additional_details',\n",
    "                            'IC50 (Î¼M)': 'ic50',\n",
    "                            'Inhibition type': 'inhibition_type',\n",
    "                            'Inhibited microorganisms': 'inhibited_microorganisms',\n",
    "                            'PTM': 'ptm',\n",
    "                            'Title': 'title',\n",
    "                            'Authors': 'authors',\n",
    "                            'Abstract': 'abstract',\n",
    "                            'DOI': 'doi',\n",
    "                            'Search type': 'search_type',\n",
    "                            'Scoring matrix': 'scoring_matrix',\n",
    "                        }, inplace=True)\n",
    "                        display(HTML(\n",
    "                            f'<b style=\"color:green;\">MBPDB file imported with {self.mbpdb_results.shape[0]} rows and {self.mbpdb_results.shape[1]} columns</b>'))\n",
    "\n",
    "    def _on_fasta_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            new_proteins = {}\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    for file_data in change['new']:\n",
    "                        try:\n",
    "                            if file_data.name.endswith('.fasta'):\n",
    "                                parsed = self._parse_uploaded_fasta(file_data)\n",
    "                                new_proteins.update(parsed)\n",
    "                                #print(f\"Parsed {len(parsed)} proteins from {file_data.name}\")\n",
    "                                display(HTML(f'<b style=\"color:green;\">Successfully imported {file_data.name}</b>'))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error: {str(e)}\")\n",
    "\n",
    "                    # Update protein_dict with new data\n",
    "                    self.protein_dict = new_proteins\n",
    "                    #print(f\"Updated protein_dict with {len(new_proteins)} entries\")\n",
    "\n",
    "    def _load_data(self, file_obj, required_columns, file_type):\n",
    "        \"\"\"\n",
    "        Load and validate uploaded data files, cleaning empty rows and validating data.\n",
    "        \n",
    "        Args:\n",
    "            file_obj: Uploaded file object\n",
    "            required_columns (list): List of required column names (either single names or pairs)\n",
    "            file_type (str): Type of file being loaded ('MBPDB' or 'Peptidomic')\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (DataFrame or None, status string 'yes'/'no')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            content = file_obj.content\n",
    "            filename = file_obj.name\n",
    "            extension = filename.split('.')[-1].lower()\n",
    "            \n",
    "            file_stream = io.BytesIO(content)\n",
    "            \n",
    "            # Load data based on file extension with multiple delimiter attempts\n",
    "            if extension == 'csv':\n",
    "                # Try different delimiters in order of common usage\n",
    "                delimiters = [',', ';', '|', '\\t']\n",
    "                df = None\n",
    "                successful_delimiter = None\n",
    "                \n",
    "                for delimiter in delimiters:\n",
    "                    try:\n",
    "                        # Reset file stream position\n",
    "                        file_stream.seek(0)\n",
    "                        temp_df = pd.read_csv(file_stream, sep=delimiter)\n",
    "                        \n",
    "                        # Check if we got more than one column\n",
    "                        if len(temp_df.columns) > 1:\n",
    "                            df = temp_df\n",
    "                            successful_delimiter = delimiter\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "                if df is None:\n",
    "                    raise ValueError(\"Could not parse CSV file with any common delimiter (tried: comma, semicolon, pipe, tab)\")\n",
    "                \n",
    "                # Show which delimiter was used\n",
    "                #display(HTML(f'<b style=\"color:blue;\">File parsed using delimiter: {successful_delimiter}</b>'))\n",
    "                \n",
    "            elif extension in ['txt', 'tsv']:\n",
    "                # For txt/tsv files, try tab first, then other delimiters\n",
    "                delimiters = ['\\t', ',', ';', '|']\n",
    "                df = None\n",
    "                successful_delimiter = None\n",
    "                \n",
    "                for delimiter in delimiters:\n",
    "                    try:\n",
    "                        file_stream.seek(0)\n",
    "                        temp_df = pd.read_csv(file_stream, sep=delimiter)\n",
    "                        if len(temp_df.columns) > 1:\n",
    "                            df = temp_df\n",
    "                            successful_delimiter = delimiter\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "                if df is None:\n",
    "                    raise ValueError(\"Could not parse TXT/TSV file with any common delimiter\")\n",
    "                    \n",
    "                #display(HTML(f'<b style=\"color:blue;\">File parsed using delimiter: {successful_delimiter}</b>'))\n",
    "                \n",
    "            elif extension == 'xlsx':\n",
    "                df = pd.read_excel(file_stream)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Please upload .csv, .txt, .tsv, or .xlsx files.\")\n",
    "            \n",
    "            # Clean column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Drop empty rows\n",
    "            df = df.dropna(how='all')\n",
    "            df = df[~(df.astype(str).apply(lambda x: x.str.strip().eq('')).all(axis=1))]\n",
    "            \n",
    "            # Handle validation differently based on file type\n",
    "            if file_type == 'MBPDB':\n",
    "                # Use column pairs for MBPDB validation\n",
    "                column_pairs = {\n",
    "                    'Search peptide': 'search_peptide',\n",
    "                    'Protein ID': 'protein_id',\n",
    "                    'Peptide': 'peptide'\n",
    "                }\n",
    "                \n",
    "                # Check for required columns in either format\n",
    "                missing_pairs = []\n",
    "                for orig_col, std_col in column_pairs.items():\n",
    "                    if not (orig_col in df.columns or std_col in df.columns):\n",
    "                        missing_pairs.append(f\"'{orig_col}' or '{std_col}'\")\n",
    "                \n",
    "                if missing_pairs:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Missing required columns: {\", \".join(missing_pairs)}</b>'))\n",
    "                    return None, 'no'\n",
    "                \n",
    "                # Validate non-empty required columns\n",
    "                empty_pairs = []\n",
    "                for orig_col, std_col in column_pairs.items():\n",
    "                    col_to_check = orig_col if orig_col in df.columns else std_col\n",
    "                    if df[col_to_check].isna().all() or (df[col_to_check].astype(str).str.strip() == '').all():\n",
    "                        empty_pairs.append(f\"'{orig_col}' or '{std_col}'\")\n",
    "                \n",
    "                if empty_pairs:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Required columns are empty: {\", \".join(empty_pairs)}</b>'))\n",
    "                    return None, 'no'\n",
    "                    \n",
    "            else:\n",
    "                # Standard validation for other file types\n",
    "                if not set(required_columns).issubset(df.columns):\n",
    "                    missing = set(required_columns) - set(df.columns)\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Missing required columns: {\", \".join(missing)}</b>'))\n",
    "                    return None, 'no'\n",
    "                \n",
    "                # Validate non-empty required columns\n",
    "                empty_required = []\n",
    "                for col in required_columns:\n",
    "                    if df[col].isna().all() or (df[col].astype(str).str.strip() == '').all():\n",
    "                        empty_required.append(col)\n",
    "                \n",
    "                if empty_required:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Required columns are empty: {\", \".join(empty_required)}</b>'))\n",
    "                    return None, 'no'\n",
    "            \n",
    "            # Show success message\n",
    "            display(HTML(f'<b style=\"color:green;\">{file_type} file loaded successfully with {len(df)} rows after cleaning.</b>'))\n",
    "            \n",
    "            return df, 'yes'\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(HTML(f'<b style=\"color:red;\">{file_type} File Error: {str(e)}</b>'))\n",
    "            return None, 'no'\n",
    "    \n",
    "    def _parse_uploaded_fasta(self, file_data):\n",
    "        \"\"\"Parse uploaded FASTA file content\"\"\"\n",
    "        fasta_dict = {}\n",
    "        fasta_text = bytes(file_data.content).decode('utf-8')\n",
    "        lines = fasta_text.split('\\n')\n",
    "\n",
    "        protein_id = \"\"\n",
    "        protein_name = \"\"\n",
    "        sequence = \"\"\n",
    "        species = \"\"\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if protein_id:\n",
    "                    fasta_dict[protein_id] = {\n",
    "                        \"name\": protein_name,\n",
    "                        \"sequence\": sequence,\n",
    "                        \"species\": species\n",
    "                    }\n",
    "                sequence = \"\"\n",
    "                header_parts = line[1:].split('|')\n",
    "                if len(header_parts) > 2:\n",
    "                    protein_id = header_parts[1]\n",
    "                    protein_name_full = re.split(r' OS=', header_parts[2])[0]\n",
    "                    if ' ' in protein_name_full:\n",
    "                        protein_name = protein_name_full\n",
    "                    else:\n",
    "                        protein_name = protein_name_full\n",
    "                    species = self._find_species(line)\n",
    "            else:\n",
    "                sequence += line\n",
    "\n",
    "        if protein_id:\n",
    "            fasta_dict[protein_id] = {\n",
    "                \"name\": protein_name,\n",
    "                \"sequence\": sequence,\n",
    "                \"species\": species\n",
    "            }\n",
    "\n",
    "        return fasta_dict\n",
    "\n",
    "    def _find_species(self, header):\n",
    "        \"\"\"Find species in FASTA header\"\"\"\n",
    "        header_lower = header.lower()\n",
    "        for spec_group in spec_translate_list:\n",
    "            for term in spec_group[1:]:\n",
    "                if term.lower() in header_lower:\n",
    "                    return spec_group[0]\n",
    "        return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "4e0ca14a-1665-43ec-abde-8adb7fc5c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupProcessing:\n",
    "    def __init__(self):\n",
    "        self.group_data = {}\n",
    "        self.group_number = 1\n",
    "        self.filtered_columns = []\n",
    "        self.group_uploader = widgets.FileUpload(\n",
    "        accept='.json',\n",
    "        multiple=False,\n",
    "        description='Upload Groups File',\n",
    "        layout=widgets.Layout(width='300px'),\n",
    "        style={'description_width': 'initial'}\n",
    "        )\n",
    "        self.group_uploader.observe(self._on_group_upload_change, names='value')\n",
    "        \n",
    "        # Initialize output areas\n",
    "        self.output = widgets.Output()\n",
    "        self.gd_output_area = widgets.Output()\n",
    "        \n",
    "        # Initialize widgets for group selection\n",
    "        self.column_dropdown = widgets.SelectMultiple(\n",
    "            #description='Absorbance ',\n",
    "            style={'description_width': 'initial'},\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width='225px', height='300px')\n",
    "        )\n",
    "\n",
    "        column_dropdown_box = widgets.HBox([\n",
    "            widgets.HTML(\"<b>Absorbance Columns:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</b>\"),\n",
    "            self.column_dropdown\n",
    "            ],layout=widgets.Layout(width='400px', height='305px')\n",
    "            )\n",
    "\n",
    "        self.grouping_variable_text = widgets.Text(\n",
    "            #description='Assign New Group Name',\n",
    "            layout=widgets.Layout(width='230px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        text_box = widgets.HBox([\n",
    "            widgets.HTML(\"<b>Search or Assigned Name:</b>\"),\n",
    "            self.grouping_variable_text\n",
    "        ])\n",
    "        self.group_box = widgets.VBox([\n",
    "            column_dropdown_box,\n",
    "            text_box], layout=widgets.Layout(width='100%'))\n",
    "        \n",
    "        # Initialize buttons\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 0')\n",
    "        )\n",
    "        \n",
    "        self.add_group_button = widgets.Button(\n",
    "            description='Add Group',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 0')\n",
    "        )\n",
    "        \n",
    "        self.reset_file_button = widgets.Button(\n",
    "            description='Reset Selection',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 75px')\n",
    "        )\n",
    "        \n",
    "        # Set up button callbacks\n",
    "        self.search_button.on_click(self._search_columns)\n",
    "        self.add_group_button.on_click(self._add_group)\n",
    "        self.reset_file_button.on_click(self._reset_selection)\n",
    "        \n",
    "    def setup_data(self):\n",
    "        \"\"\"Initialize data and filters for the analysis\"\"\"\n",
    "        # Define columns to exclude with more flexible matching\n",
    "        columns_to_exclude = [\n",
    "            'Marked as', 'Number of Missed Cleavages', 'Missed Cleavages',\n",
    "            'Checked', 'Confidence', 'Annotated Sequence', 'Unnamed: 3', \n",
    "            'Modifications', 'Protein Groups', 'Proteins', 'PSMs', \n",
    "            'Master Protein Accessions', 'Master Protein Descriptions', 'Description',\n",
    "            'Positions in Master Proteins', 'Positions in Proteins' 'Modifications in Master Proteins',\n",
    "            'Modifications in Master Proteins all Sites',\n",
    "            'Theo MHplus in Da', 'Quan Info', \n",
    "            'Confidence by Search Engine', \n",
    "            'q-Value by Search Engine',\n",
    "            'XCorr by Search Engine',\n",
    "            'Percolator PEP by Search Engine', 'Percolator q-Value by Search Engine',\n",
    "            'Percolator SVMScore by Search Engine',\n",
    "            'PEP', 'q-Value', 'RT in min', 'RT in min by Search Engine',\n",
    "            'Sequence', 'Sequence Length', 'search_peptide', 'Peptide', 'protein_id', \n",
    "            'protein_description', 'Alignment', 'Species', \n",
    "            'Intervals', 'function', 'unique ID', 'PEP (by Search Engine): Sequest HT',\n",
    "            'SVM Score (by Search Engine): Sequest HT', 'SVM_Score',\n",
    "            'XCorr (by Search Engine): Sequest HT', 'PEP', 'q-Value', 'Qvality PEP', 'Qvality q-value',\n",
    "            'Top Apex RT [min]', 'Top Apex RT in min', 'start', 'stop',\n",
    "            'Abundance Ratio', 'Abundance Ratio Adj P-Value', 'Abundance Ratio log2', \n",
    "            'Abundance Ratio P-Value', 'Abundances', 'Abundances Counts', \n",
    "            'Abundances Grouped', 'Abundances Grouped Count', 'Abundances Grouped CV',\n",
    "            'Abundances Normalized', 'Abundances Scaled', 'Charge by Search Engine',\n",
    "            'Concatenated Rank by Search Engine', 'Delta Cn by Search Engine',\n",
    "            'Delta M in ppm by Search Engine', 'Delta mz in Da by Search Engine',\n",
    "            'Delta Score by Search Engine', 'Found in Sample Groups', 'Found in Samples',\n",
    "            'Modifications all possible sites', 'mz in Da by Search Engine',\n",
    "            'Number of Isoforms', 'Number of Protein Groups', 'Number of Proteins',\n",
    "            'Protein Accessions', 'PSM Ambiguity', 'Rank by Search Engine',\n",
    "            'Search Engine Rank by Search Engine', 'Score CHIMERYS Identification (by search engine)'\n",
    "            ]\n",
    "        \n",
    "        exclude_substrings = [\n",
    "            'Abundances by Bio Rep', \n",
    "            'Count', \n",
    "            'Origin',\n",
    "            'Average_Abundance',\n",
    "            'Avg_',\n",
    "            'PEP by Search Engine',\n",
    "            'SVM Score by Search Engine',\n",
    "            'XCorr by Search Engine',\n",
    "            'Top Apex RT'\n",
    "            \n",
    "        ]\n",
    "    \n",
    "        # Use cleaned data if available, otherwise use original\n",
    "        df = self.pd_results_cleaned if (hasattr(self, 'pd_results_cleaned') and \n",
    "                                       not self.pd_results_cleaned.empty) else self.pd_results\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            # More flexible column filtering\n",
    "            self.filtered_columns = []\n",
    "            for col in df.columns:\n",
    "                # Check if any exclusion pattern matches the column name\n",
    "                should_exclude = any(excl.lower() in col.lower() for excl in columns_to_exclude)\n",
    "                # Check if any substring pattern matches\n",
    "                has_excluded_substring = any(sub.lower() in col.lower() for sub in exclude_substrings)\n",
    "                \n",
    "                if not should_exclude and not has_excluded_substring:\n",
    "                    self.filtered_columns.append(col)\n",
    "              \n",
    "            # Update dropdown options\n",
    "            self.column_dropdown.options = self.filtered_columns\n",
    "            self._reset_inputs()\n",
    "        else:\n",
    "            self.filtered_columns = []\n",
    "            self.column_dropdown.options = []\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:red;\">No valid data available for processing.</b>'))\n",
    "   \n",
    "    def update_data(self, pd_results):\n",
    "        \"\"\"Update data and refresh filtered columns\"\"\"\n",
    "        self.pd_results = pd_results\n",
    "        \n",
    "        # Only update if we have valid data\n",
    "        if pd_results is not None:\n",
    "            self.setup_data()\n",
    "            \n",
    "            # Update the dropdown with new filtered columns\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:green;\">Data updated successfully. Column selection refreshed.</b>'))\n",
    "    \n",
    "    def create_download_link(self, file_path, label):\n",
    "        \"\"\"Create a download link for a file.\"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            # Read file content and encode it as base64\n",
    "            with open(file_path, 'rb') as f:\n",
    "                content = f.read()\n",
    "            b64_content = base64.b64encode(content).decode('utf-8')\n",
    "    \n",
    "            # Generate the download link HTML\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <a download=\"{os.path.basename(file_path)}\" \n",
    "                   href=\"data:application/octet-stream;base64,{b64_content}\" \n",
    "                   style=\"color: #0366d6; text-decoration: none; margin-left: 20px; font-size: 14px;\">\n",
    "                    {label}\n",
    "                </a>\n",
    "            \"\"\")\n",
    "        else:\n",
    "            # Show an error message if the file does not exist\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <span style=\"color: red; margin-left: 20px; font-size: 14px;\">\n",
    "                    File \"{file_path}\" not found!\n",
    "                </span>\n",
    "            \"\"\")\n",
    "            \n",
    "    def display_group_selector(self):\n",
    "        \"\"\"Display the JSON file selector for group dictionaries\"\"\"\n",
    "        group_box_uploader = widgets.HBox([\n",
    "            self.group_uploader,\n",
    "            self.create_download_link(\"example_group_definition.json\", \"Example\")\n",
    "        ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        display(widgets.HTML(\"<h3><u>Upload Existing Group Dictionary:</u></h3>\"))\n",
    "        display(group_box_uploader, self.gd_output_area)\n",
    "        \n",
    "    def display_widgets(self):\n",
    "        \"\"\"Display the main UI for group selection\"\"\"\n",
    "        # Create main grid container\n",
    "        grid = widgets.GridspecLayout(1, 2,  # Number of rows and columns\n",
    "            width='800px', \n",
    "            grid_gap='5px',  # Adjust spacing between grid elements\n",
    "        )\n",
    "        \n",
    "        # Create input container with vertical scroll\n",
    "        input_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Select New Grouping of Data:</u></h3>\"),\n",
    "            widgets.HTML('Now select the <b>absorbance columns</b> and assign the name of the <b>grouping variable</b>:'),\n",
    "            self.group_box,\n",
    "            # Create button layouts\n",
    "            widgets.HBox([self.search_button, self.add_group_button], layout=widgets.Layout(width='100%')),\n",
    "            widgets.HBox([self.reset_file_button], layout=widgets.Layout(width='100%'))\n",
    "        ], layout=widgets.Layout(\n",
    "            width='400px',\n",
    "            height='600px',\n",
    "            overflow_y='auto'  # Add vertical scroll\n",
    "        ))\n",
    "        \n",
    "        # Create output container with vertical scroll\n",
    "        output_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Group Selection Results:</u></h3>\"),\n",
    "            self.output\n",
    "        ], layout=widgets.Layout(\n",
    "            width='400px',\n",
    "            height='600px',\n",
    "            overflow_y='auto',  # Add vertical scroll\n",
    "            padding='10px'\n",
    "        ))\n",
    "        \n",
    "        # Add to grid\n",
    "        grid[0, 0] = input_container  # Left column\n",
    "        grid[0, 1] = output_container  # Right column\n",
    "        \n",
    "        display(grid)\n",
    "    \n",
    "    def _on_gd_submit(self, b, dropdown):\n",
    "        \"\"\"Handle JSON file submission\"\"\"\n",
    "        selected_file = dropdown.value\n",
    "        with self.gd_output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            if selected_file == 'Select an existing grouping dictionary file':\n",
    "                print(\"Please select a valid file.\")\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                # Load and process JSON file\n",
    "                with open(selected_file, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                self.group_data = {}\n",
    "                \n",
    "                # Process groups\n",
    "                with self.output:\n",
    "                    clear_output()\n",
    "                    for group_number, group_info in data.items():\n",
    "                        group_name = group_info.get('grouping_variable')\n",
    "                        selected_columns = group_info.get('abundance_columns')\n",
    "                        \n",
    "                        self.group_data[group_number] = {\n",
    "                            'grouping_variable': group_name,\n",
    "                            'abundance_columns': selected_columns\n",
    "                        }\n",
    "                        \n",
    "                        display(widgets.HTML(\n",
    "                            f\"<b>Group {group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"\n",
    "                        ))\n",
    "                        display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "                        display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "                        display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "                        \n",
    "                display(widgets.HTML(f'<b style=\"color:green;\">Successfully uploaded: {selected_file}</b>'))\n",
    "                \n",
    "            except Exception as e:\n",
    "                display(widgets.HTML(f\"<b style='color:red;'>An error occurred while processing the file: {str(e)}</b>\"))\n",
    "    \n",
    "    def _search_columns(self, b):\n",
    "        \"\"\"Search for columns based on group name\"\"\"\n",
    "        group_name = self.grouping_variable_text.value\n",
    "        if group_name:\n",
    "            matching_columns = [col for col in self.filtered_columns if group_name in col]\n",
    "            self.column_dropdown.value = matching_columns\n",
    "        else:\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:red;\">Please enter a group name to search.</b>'))\n",
    "    \n",
    "    def _add_group(self, b):\n",
    "        \"\"\"Add a new group to the data\"\"\"\n",
    "        group_name = self.grouping_variable_text.value\n",
    "        selected_columns = list(self.column_dropdown.value)\n",
    "        \n",
    "        if not (group_name and selected_columns):\n",
    "            with self.output:\n",
    "                display(widgets.HTML('<b style=\"color:red;\">Please enter a group name and select at least one column.</b>'))\n",
    "            return\n",
    "        \n",
    "        # If group_data exists, use next number, otherwise start at 1\n",
    "        if self.group_data:\n",
    "            # Convert existing keys to integers and find max\n",
    "            existing_numbers = [int(k) for k in self.group_data.keys()]\n",
    "            next_number = max(existing_numbers) + 1\n",
    "            self.group_number = str(next_number)\n",
    "        else:\n",
    "            self.group_data = {}\n",
    "            self.group_number = \"1\"\n",
    "        \n",
    "        # Add new group data to the dictionary\n",
    "        self.group_data[self.group_number] = {\n",
    "            'grouping_variable': group_name,\n",
    "            'abundance_columns': selected_columns\n",
    "        }\n",
    "        \n",
    "        # Display output\n",
    "        with self.output:\n",
    "            display(widgets.HTML(f\"<b>Group {self.group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"))\n",
    "            display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "            display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "            display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "        \n",
    "        self._reset_inputs()\n",
    "        \n",
    "    def _reset_selection(self, b):\n",
    "        \"\"\"Reset all selections and data\"\"\"\n",
    "        self.group_data = {}\n",
    "        self.group_number = 1\n",
    "        with self.gd_output_area:\n",
    "            clear_output()\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "        self._reset_inputs()\n",
    "    \n",
    "    def _reset_inputs(self):\n",
    "        \"\"\"Reset input fields\"\"\"\n",
    "        self.grouping_variable_text.value = ''\n",
    "        self.column_dropdown.value = ()\n",
    "\n",
    "    def _on_group_upload_change(self, change):\n",
    "        \"\"\"Handle JSON file upload\"\"\"\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.gd_output_area:\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    try:\n",
    "                        content = bytes(file_data.content).decode('utf-8')\n",
    "                        simplified_data = json.loads(content)\n",
    "                        \n",
    "                        # Convert simplified format to enumerated format\n",
    "                        enumerated_data = {}\n",
    "                        for i, (group_name, abundance_cols) in enumerate(simplified_data.items(), 1):\n",
    "                            group_number = str(i)  # Convert to string to match original format\n",
    "                            enumerated_data[group_number] = {\n",
    "                                'grouping_variable': group_name,\n",
    "                                'abundance_columns': abundance_cols\n",
    "                            }\n",
    "                        \n",
    "                        # Process groups using the enumerated format\n",
    "                        with self.output:\n",
    "                            for group_number, group_info in enumerated_data.items():\n",
    "                                group_name = group_info['grouping_variable']\n",
    "                                selected_columns = group_info['abundance_columns']\n",
    "                                \n",
    "                                # Update group_data without clearing previous entries\n",
    "                                self.group_data[group_number] = {\n",
    "                                    'grouping_variable': group_name,\n",
    "                                    'abundance_columns': selected_columns\n",
    "                                }\n",
    "                                \n",
    "                                display(widgets.HTML(\n",
    "                                    f\"<b>Group {group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"\n",
    "                                ))\n",
    "                                display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "                                display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "                                display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "                                \n",
    "                        display(widgets.HTML(f'<b style=\"color:green;\">Successfully uploaded: {file_data.name}</b>'))\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        display(widgets.HTML(f\"<b style='color:red;'>An error occurred while processing the file: {str(e)}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "1f7fd494-b273-4018-8b3a-9e0e31867530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinCombinationHandler(HasTraits):\n",
    "    def __init__(self, data_transformer):\n",
    "        super().__init__()\n",
    "        self.data_transformer = data_transformer  # Store reference to data_transformer\n",
    "        self.pd_results = data_transformer.pd_results\n",
    "        self.pd_results_cleaned = None\n",
    "        self.protein_output_area = None\n",
    "        self.user_decisions = {}\n",
    "        self.decision_inputs = []\n",
    "        self.multi_position_combinations = []\n",
    "        self.submit_button = None\n",
    "        self.reset_button = None\n",
    "        self.progress = None\n",
    "        self.protein_mapping_output_area = widgets.Output()\n",
    "        \n",
    "        self.protein_mapping_widget = widgets.RadioButtons(\n",
    "            options=[('Yes', True), ('No (skip)', False)],\n",
    "            description='Process peptides mapped to multiple proteins?',\n",
    "            disabled=True, \n",
    "            style={'description_width': 'initial'},\n",
    "            value=None\n",
    "        )\n",
    "        \n",
    "        self.protein_mapping_widget.observe(self.process_protein_mapping, names='value')\n",
    "\n",
    "\n",
    "    @property  # Make protein_dict a property that always reads from data_transformer\n",
    "    def protein_dict(self):\n",
    "        return self.data_transformer.protein_dict\n",
    "\n",
    "    def _get_protein_combinations(self):\n",
    "        \"\"\"Extract unique protein combinations from the dataset with NaN handling\"\"\"\n",
    "        if self.pd_results is None or self.pd_results.empty:\n",
    "            return []\n",
    "\n",
    "        protein_combinations = set()\n",
    "        nan_warnings = {\n",
    "            'positions': 0,\n",
    "            'master_acc': 0,\n",
    "            'unknown_added': 0\n",
    "        }\n",
    "\n",
    "        # Create a working copy of the dataframe\n",
    "        working_df = self.pd_results.copy()\n",
    "\n",
    "        # Track NaN counts before modification\n",
    "        nan_warnings['positions'] = working_df['Positions in Proteins'].isna().sum()\n",
    "        nan_warnings['master_acc'] = working_df['Master Protein Accessions'].isna().sum()\n",
    "\n",
    "        # Replace NaN values with \"Unknown\" instead of dropping\n",
    "        working_df['Positions in Proteins'] = working_df['Positions in Proteins'].fillna('Unknown')\n",
    "        working_df['Master Protein Accessions'] = working_df['Master Protein Accessions'].fillna('Unknown')\n",
    "\n",
    "        for _, row in working_df.iterrows():\n",
    "            try:\n",
    "                # Handle \"Unknown\" case specially\n",
    "                if row['Positions in Proteins'] == 'Unknown':\n",
    "                    position_proteins = ['Unknown']\n",
    "                else:\n",
    "                    position_proteins = [p.split()[0] for p in row['Positions in Proteins'].split('; ')]\n",
    "\n",
    "                master_acc = row['Master Protein Accessions']\n",
    "\n",
    "                # Check species of proteins in Positions in Proteins\n",
    "                species_set = set()\n",
    "                for protein in position_proteins:\n",
    "                    if protein in self.protein_dict:\n",
    "                        species_set.add(self.protein_dict[protein]['species'])\n",
    "                    elif protein == 'Unknown':\n",
    "                        species_set.add('Unknown')\n",
    "\n",
    "                if (';' in master_acc or\n",
    "                        ';' in row['Positions in Proteins'] or\n",
    "                        len(species_set) > 1 or\n",
    "                        'Unknown' in species_set):  # Include Unknown combinations\n",
    "                    protein_combinations.add('; '.join(sorted(position_proteins)))\n",
    "                    if 'Unknown' in position_proteins:\n",
    "                        nan_warnings['unknown_added'] += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing row {_}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Store warning message for display\n",
    "        warning_message = []\n",
    "\n",
    "        if nan_warnings['positions'] > 0:\n",
    "            warning_message.append(\n",
    "                f\"{nan_warnings['positions']} rows with missing 'Positions in Proteins' were marked as Unknown\")\n",
    "        if nan_warnings['master_acc'] > 0:\n",
    "            warning_message.append(\n",
    "                f\"{nan_warnings['master_acc']} rows with missing 'Master Protein Accessions' were marked as Unknown\")\n",
    "        if nan_warnings['unknown_added'] > 0:\n",
    "            warning_message.append(f\"{nan_warnings['unknown_added']} combinations now include Unknown proteins\")\n",
    "\n",
    "        # if warning_message:\n",
    "        #   print(\"Warning: \" + \"; \".join(warning_message))\n",
    "\n",
    "        self.multi_position_combinations = list(protein_combinations)\n",
    "        return self.multi_position_combinations\n",
    "\n",
    "    def process_protein_mapping(self, change):\n",
    "\n",
    "\n",
    "        with self.protein_mapping_output_area:\n",
    "            self.protein_mapping_output_area.clear_output()\n",
    "            if self.protein_mapping_widget.value == True:\n",
    "                display(HTML(\"<b style='color:green;'>Using user-defined protein mappings.</b>\"))\n",
    "                self.pd_results_cleaned = self.process_protein_combinations()\n",
    "            else:\n",
    "                self.pd_results_cleaned = self.pd_results.copy()\n",
    "                display(HTML(\"<b style='color:green;'>Using original protein mappings.</b>\"))\n",
    "\n",
    "\n",
    "        return self.pd_results_cleaned\n",
    "\n",
    "    def process_protein_combinations(self):\n",
    "        \"\"\"Process protein combinations in pd_results with Unknown handling\"\"\"\n",
    "        if not self.pd_results.empty:\n",
    "            df = self.pd_results.copy()\n",
    "\n",
    "            # Fill NaN values with \"Unknown\"\n",
    "            df['Positions in Proteins'] = df['Positions in Proteins'].fillna('Unknown')\n",
    "            df['Master Protein Accessions'] = df['Master Protein Accessions'].fillna('Unknown')\n",
    "\n",
    "            # Create warning display area\n",
    "            warning_area = widgets.HTML(\n",
    "                layout=widgets.Layout(\n",
    "                    margin='10px 0',\n",
    "                    padding='10px',\n",
    "                    border='1px solid #ffeeba',\n",
    "                    background_color='#fff3cd',\n",
    "                    border_radius='4px'\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Get combinations and track Unknown statistics\n",
    "            combinations = self._get_protein_combinations()\n",
    "\n",
    "            # Update warning area with statistics\n",
    "            unknown_positions = (df['Positions in Proteins'] == 'Unknown').sum()\n",
    "            unknown_master_acc = (df['Master Protein Accessions'] == 'Unknown').sum()\n",
    "\n",
    "            if unknown_positions > 0 or unknown_master_acc > 0:\n",
    "                warning_html = \"<div><b>â¹ï¸ Notice:</b><ul style='margin: 5px 0'>\"\n",
    "                if unknown_positions > 0:\n",
    "                    warning_html += f\"<li>{unknown_positions} rows with missing 'Positions in Proteins' are marked as Unknown</li>\"\n",
    "                if unknown_master_acc > 0:\n",
    "                    warning_html += f\"<li>{unknown_master_acc} rows with missing 'Master Protein Accessions' are marked as Unknown</li>\"\n",
    "                warning_html += \"</ul>These peptides will be preserved in the output.</div>\"\n",
    "                warning_area.value = warning_html\n",
    "            else:\n",
    "                warning_area = widgets.HTML(f\"<br>\")\n",
    "\n",
    "            # Main container with warning area\n",
    "            main_container = widgets.VBox([\n",
    "                warning_area,\n",
    "                widgets.HTML(\"\"\"\n",
    "                    <h3>Peptides Mapped to Multiple Proteins</h3>\n",
    "                    <div style='margin-bottom: 15px;'>\n",
    "                        Select how to handle each protein mapping combination in your dataset.\n",
    "                        These combinations come from either:\n",
    "                        <ul>\n",
    "                            <li>Multiple proteins in Master Protein Accessions</li>\n",
    "                            <li>Multiple proteins in Positions in Proteins</li>\n",
    "                            <li>Proteins from different species</li>\n",
    "                            <li>Unknown protein mappings (from missing values)</li>\n",
    "                        </ul>\n",
    "                    </div>\n",
    "                \"\"\")\n",
    "            ], layout=widgets.Layout(width='100%', padding='20px'))\n",
    "\n",
    "            # Get combinations\n",
    "            combinations = self._get_protein_combinations()\n",
    "\n",
    "            def create_help_icon(self, tooltip_text):\n",
    "                \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "                return f'<div title=\"{tooltip_text}\" style=\"display: inline-block; margin-left: 4px;\">' \\\n",
    "                       '<i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>' \\\n",
    "                       '</div>'\n",
    "\n",
    "            table_header = widgets.HTML(\"\"\"\n",
    "                            <div style=\"display: grid; grid-template-columns: 100px 100px 420px 200px auto; gap: 2px; margin-bottom: 10px; font-weight: bold; align-items: center;\">\n",
    "                                <div>\n",
    "                                    Protein ID\n",
    "                                    <span title=\"Unique identifier for the protein\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Species\n",
    "                                    <span title=\"Source organism of the protein\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Description\n",
    "                                    <span title=\"Full protein name or description\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Decision\n",
    "                                    <span title=\"Available options:\\n\n",
    "            - 'new' - Create a separate row for this protein\\n\n",
    "            - 'remove' - Remove this protein from combination\\n\n",
    "            - 'asis' - Keep as part of current combination\\n\n",
    "            - 'Custom: (protein ID)': ie. Custom: P02666A1\"\n",
    "            style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Status\n",
    "                                    <span title=\"Color indicators:\\n\n",
    "            - Grey - Default option (not yet submitted)\\n\n",
    "            - Green - Option has been submitted\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                            </div>\n",
    "                            <hr style=\"margin: 0 0 10px 0;\">\n",
    "                        \"\"\")\n",
    "\n",
    "            # Create input area\n",
    "            input_area = widgets.VBox([table_header],\n",
    "                                      layout=widgets.Layout(width='100%', margin='10px 0'))\n",
    "\n",
    "            # Add rows for each combination\n",
    "            self.decision_inputs = []\n",
    "            self.status_displays = {}\n",
    "\n",
    "            for combo_idx, combo in enumerate(combinations, 1):\n",
    "                proteins = combo.split('; ')\n",
    "\n",
    "                # Find rows with this combination\n",
    "                combo_rows = []\n",
    "                for _, row in df.iterrows():\n",
    "                    if pd.isna(row['Positions in Proteins']):\n",
    "                        continue\n",
    "                    row_proteins = set(p.split()[0] for p in row['Positions in Proteins'].split('; '))\n",
    "                    if row_proteins == set(proteins):\n",
    "                        combo_rows.append(row)\n",
    "\n",
    "                occurrences = len(combo_rows)\n",
    "\n",
    "                # Add combination header\n",
    "                input_area.children += (widgets.HTML(f\"\"\"\n",
    "                    <div style=\"background-color: #f8f9fa; padding: 2px; margin: 5px 0; border-radius: 5px;\">\n",
    "                        <b>Combination {combo_idx}</b> ({occurrences} occurrences)\n",
    "                    </div>\n",
    "                \"\"\"),)\n",
    "\n",
    "                # Process each protein in the combination\n",
    "                for protein in proteins:\n",
    "                    species = \"Unknown\" if protein == 'Unknown' else self.protein_dict.get(protein, {}).get('species',\n",
    "                                                                                                            \"Unknown\")\n",
    "                    name = \"Unknown Protein\" if protein == 'Unknown' else self.protein_dict.get(protein, {}).get('name',\n",
    "                                                                                                                 \"Unknown\")\n",
    "\n",
    "                    # Set default decision based on Master Protein Accessions\n",
    "                    default_decision = 'asis'  # Always keep Unknown proteins as-is\n",
    "                    if protein != 'Unknown' and combo_rows:\n",
    "                        first_row = combo_rows[0]\n",
    "                        if not pd.isna(first_row['Master Protein Accessions']):\n",
    "                            master_proteins = first_row['Master Protein Accessions'].split(';')\n",
    "                            master_proteins = [p.strip() for p in master_proteins]\n",
    "                            default_decision = 'new' if protein in master_proteins else 'remove'\n",
    "\n",
    "                    # Create decision input\n",
    "                    decision_input = widgets.Text(\n",
    "                        layout=widgets.Layout(width='125px'),\n",
    "                        value=default_decision\n",
    "                    )\n",
    "                    self.decision_inputs.append((combo, protein, decision_input))\n",
    "\n",
    "                    # Create status display with initial status\n",
    "                    status_text = {\n",
    "                        'new': \"Will be created as new row\",\n",
    "                        'remove': \"Will be removed\",\n",
    "                        'asis': \"Will keep as is\",\n",
    "                        'Custom: (protein ID)': \"ie. Custom: P02666A1\"\n",
    "                    }\n",
    "                    initial_status = status_text.get(default_decision, '')\n",
    "                    status_display = widgets.HTML(f'<span style=\"color: gray\">{initial_status}</span>')\n",
    "                    self.status_displays[(combo, protein)] = status_display\n",
    "\n",
    "                    # Create the row content\n",
    "                    row_content = widgets.HTML(f\"\"\"\n",
    "                    <div style=\"display: grid; grid-template-columns: 100px 100px 420px; gap: 2px; align-items: center;\">\n",
    "                            <div>{protein}</div>\n",
    "                            <div>{species}</div>\n",
    "                            <div>{name}</div>\n",
    "                        </div>\n",
    "                    \"\"\")\n",
    "\n",
    "                    # Create container with all elements\n",
    "                    container = widgets.HBox([\n",
    "                        row_content,\n",
    "                        widgets.HBox([decision_input], layout=widgets.Layout(width='150px', padding='0')),\n",
    "                        widgets.HBox([status_display], layout=widgets.Layout(width='200px', padding='0'))\n",
    "                    ], layout=widgets.Layout(\n",
    "                        margin='2px 0',\n",
    "                        display='flex',\n",
    "                        align_items='center',\n",
    "                        overflow='hidden',\n",
    "                        width='100%'\n",
    "                    ))\n",
    "\n",
    "                    input_area.children += (container,)\n",
    "\n",
    "            # Create buttons\n",
    "            button_box = self._create_buttons()\n",
    "\n",
    "            # Add output area\n",
    "            self.protein_output_area = widgets.Output(\n",
    "                layout=widgets.Layout(width='100%', margin='5px 0')\n",
    "            )\n",
    "\n",
    "            # Add all components\n",
    "            main_container.children += (input_area, button_box, self.protein_output_area)\n",
    "\n",
    "            self.pd_results_cleaned = df\n",
    "            display(main_container)\n",
    "            return df\n",
    "\n",
    "    def _on_submit(self, button, df):\n",
    "        \"\"\"Handle submit button click with enhanced position handling\"\"\"\n",
    "        self.submit_button.disabled = True\n",
    "        self.reset_button.disabled = True\n",
    "        self.progress.value = 0\n",
    "        \n",
    "        with self.protein_output_area:\n",
    "            try:\n",
    "                self.protein_output_area.clear_output()\n",
    "                decisions_by_combo = {}\n",
    "                rows_to_remove = set()\n",
    "                new_rows = []\n",
    "                total_inputs = len(self.decision_inputs)\n",
    "                \n",
    "                # First pass: collect all decisions\n",
    "                for i, (combo, protein, input_widget) in enumerate(self.decision_inputs):\n",
    "                    try:\n",
    "                        decision = input_widget.value.strip()\n",
    "                        if decision:\n",
    "                            status_display = self.status_displays[(combo, protein)]\n",
    "                            status_display.value = f'<span style=\"color: green\">Decision: {decision}</span>'\n",
    "                            \n",
    "                            if combo not in decisions_by_combo:\n",
    "                                decisions_by_combo[combo] = {}\n",
    "                            decisions_by_combo[combo][protein] = decision\n",
    "                    except Exception as e:\n",
    "                        display(HTML(f\"<b style='color:red;'>Error processing decision for {protein}: {str(e)}</b>\"))\n",
    "                        continue\n",
    "\n",
    "                    self.progress.value = ((i + 1) / total_inputs * 25)\n",
    "                \n",
    "                # Second pass: validate decisions\n",
    "                validation_errors = []\n",
    "                \n",
    "                for combo, protein_decisions in decisions_by_combo.items():\n",
    "                    # Check if decisions are valid for this combination\n",
    "                    has_asis = any(decision.upper() == 'ASIS' for decision in protein_decisions.values())\n",
    "                    has_custom = any(decision.upper().startswith('CUSTOM:') or decision.startswith('Custom:') \n",
    "                                    for decision in protein_decisions.values())\n",
    "                    has_new = any(decision.upper() == 'NEW' for decision in protein_decisions.values())\n",
    "                    has_remove = any(decision.upper() == 'REMOVE' for decision in protein_decisions.values())\n",
    "                    \n",
    "                    # Validation rules\n",
    "                    if has_asis and (has_custom or has_new or has_remove):\n",
    "                        validation_errors.append(f\"Combination '{combo}': ASIS cannot be used with other decision types\")\n",
    "                    \n",
    "                    #if has_new and (has_custom or has_asis):\n",
    "                    #    validation_errors.append(f\"Combination '{combo}': NEW cannot be used with CUSTOM or ASIS\")\n",
    "                    \n",
    "                    # Validate individual decision formats\n",
    "                    for protein, decision in protein_decisions.items():\n",
    "                        decision_upper = decision.upper()\n",
    "                        if (decision_upper not in ['NEW', 'REMOVE', 'ASIS'] and \n",
    "                            not decision_upper.startswith('CUSTOM:') and \n",
    "                            not decision.startswith('Custom:')):\n",
    "                            validation_errors.append(f\"Protein '{protein}': Invalid decision format '{decision}'\")\n",
    "                        \n",
    "                        if (decision_upper.startswith('CUSTOM:') or decision.startswith('Custom:')) and len(decision.split(':', 1)[1].strip()) == 0:\n",
    "                            validation_errors.append(f\"Protein '{protein}': CUSTOM decision requires a protein ID after the colon\")\n",
    "                \n",
    "                # If validation errors, stop processing\n",
    "                if validation_errors:\n",
    "                    error_message = \"Cannot process due to the following errors:<br>\"\n",
    "                    for error in validation_errors:\n",
    "                        error_message += f\"â¢ {error}<br>\"\n",
    "                    error_message += \"<br>Valid combinations:<br>\"\n",
    "                    error_message += \"â¢ All proteins can be ASIS (no changes)<br>\"\n",
    "                    error_message += \"â¢ CUSTOM, NEW and REMOVE can be used together<br>\"\n",
    "                    error_message += \"â¢ ASIS cannot be used with other decision types (CUSTOM, NEW or REMOVE)<br>\"\n",
    "                    \n",
    "                    display(HTML(f\"<b style='color:red;'>{error_message}</b>\"))\n",
    "                    self.progress.value = 0\n",
    "                    self.submit_button.disabled = False\n",
    "                    self.reset_button.disabled = False\n",
    "                    return df\n",
    "                \n",
    "                # Third pass: process the dataframe\n",
    "                if decisions_by_combo:\n",
    "                    processed_df = df.copy()\n",
    "                    processed_count = 0\n",
    "                    total_combinations = len(decisions_by_combo)\n",
    "\n",
    "                    for combo, protein_decisions in decisions_by_combo.items():\n",
    "                        try:\n",
    "                            # Extract protein IDs for pattern matching\n",
    "                            proteins = []\n",
    "                            for part in combo.split('; '):\n",
    "                                if not part.startswith('['):\n",
    "                                    protein_id = part.split()[0]\n",
    "                                    proteins.append(protein_id)\n",
    "                            \n",
    "                            # Create regex pattern\n",
    "                            pattern_parts = []\n",
    "                            for protein in proteins:\n",
    "                                escaped_protein = re.escape(protein)\n",
    "                                pattern_parts.append(f'(?=.*{escaped_protein})')\n",
    "                            pattern = ''.join(pattern_parts)\n",
    "\n",
    "                            try:\n",
    "                                # Find matching rows\n",
    "                                valid_rows = processed_df['Positions in Proteins'].notna()\n",
    "                                mask = processed_df['Positions in Proteins'].fillna('').str.contains(pattern, regex=True)\n",
    "                                mask = valid_rows & mask\n",
    "                                matched_indices = processed_df[mask].index\n",
    "\n",
    "                                for idx in matched_indices:\n",
    "                                    row = processed_df.loc[idx]\n",
    "                                    positions = row['Positions in Proteins'].split('; ')\n",
    "                                    master_accs = row['Master Protein Accessions'].split('; ') if '; ' in row['Master Protein Accessions'] else [row['Master Protein Accessions']]\n",
    "                                    \n",
    "                                    # Extract protein IDs from positions\n",
    "                                    current_proteins = []\n",
    "                                    for pos in positions:\n",
    "                                        parts = pos.split()\n",
    "                                        if parts and not parts[0].startswith('['):\n",
    "                                            current_proteins.append(parts[0])\n",
    "                                    \n",
    "                                    if set(current_proteins) == set(proteins):\n",
    "                                        # Check if all decisions are ASIS\n",
    "                                        all_asis = all(decision.upper() == 'ASIS' for decision in protein_decisions.values())\n",
    "                                        if all_asis:\n",
    "                                            continue\n",
    "                                        \n",
    "                                        # Process decisions\n",
    "                                        proteins_to_remove = []\n",
    "                                        custom_changes = {}\n",
    "                                        new_proteins = []\n",
    "                                        \n",
    "                                        for protein, decision in protein_decisions.items():\n",
    "                                            decision_upper = decision.upper()\n",
    "                                            \n",
    "                                            if decision_upper == 'NEW':\n",
    "                                                new_proteins.append(protein)\n",
    "                                                proteins_to_remove.append(protein)\n",
    "                                                \n",
    "                                                # Create new row for this protein\n",
    "                                                matching_position = next((p for p in positions if p.startswith(protein)), None)\n",
    "                                                if matching_position:\n",
    "                                                    new_row = row.copy()\n",
    "                                                    new_row['Positions in Proteins'] = matching_position\n",
    "                                                    new_row['Master Protein Accessions'] = protein\n",
    "                                                    new_rows.append(new_row)\n",
    "                                                    \n",
    "                                            elif decision_upper == 'REMOVE':\n",
    "                                                proteins_to_remove.append(protein)\n",
    "                                                \n",
    "                                            elif decision_upper.startswith('CUSTOM:') or decision.startswith('Custom:'):\n",
    "                                                parts = decision.split(':', 1)\n",
    "                                                if len(parts) > 1:\n",
    "                                                    new_protein_id = parts[1].strip()\n",
    "                                                    custom_changes[protein] = new_protein_id\n",
    "                                        \n",
    "                                        # First apply custom changes\n",
    "                                        for protein, new_protein_id in custom_changes.items():\n",
    "                                            for i, pos in enumerate(positions):\n",
    "                                                if pos.startswith(protein):\n",
    "                                                    pos_parts = pos.split(' ', 1)\n",
    "                                                    if len(pos_parts) > 1:\n",
    "                                                        pos_range = pos_parts[1]\n",
    "                                                        new_position = f\"{new_protein_id} {pos_range}\"\n",
    "                                                        positions[i] = new_position\n",
    "                                            \n",
    "                                            for i, acc in enumerate(master_accs):\n",
    "                                                if acc == protein:\n",
    "                                                    master_accs[i] = new_protein_id\n",
    "                                        \n",
    "                                        # Then remove proteins\n",
    "                                        if proteins_to_remove:\n",
    "                                            positions = [pos for pos in positions if not any(pos.startswith(p) for p in proteins_to_remove)]\n",
    "                                            master_accs = [acc for acc in master_accs if acc not in proteins_to_remove]\n",
    "                                        \n",
    "                                        # Update the original row if there are positions left\n",
    "                                        if positions:\n",
    "                                            processed_df.at[idx, 'Positions in Proteins'] = '; '.join(positions)\n",
    "                                            processed_df.at[idx, 'Master Protein Accessions'] = '; '.join(master_accs) if master_accs else \"Unknown\"\n",
    "                                        else:\n",
    "                                            rows_to_remove.add(idx)\n",
    "\n",
    "                            except Exception as regex_error:\n",
    "                                display(HTML(f\"<b style='color:orange;'>Warning: Error in pattern matching: {str(regex_error)}</b>\"))\n",
    "                                continue\n",
    "\n",
    "                        except Exception as combo_error:\n",
    "                            display(HTML(f\"<b style='color:orange;'>Warning: Error processing combination {combo}: {str(combo_error)}</b>\"))\n",
    "                            continue\n",
    "\n",
    "                        processed_count += 1\n",
    "                        progress = 30 + (processed_count / total_combinations * 70)\n",
    "                        self.progress.value = progress\n",
    "\n",
    "                    # Final processing\n",
    "                    if rows_to_remove:\n",
    "                        processed_df = processed_df.drop(index=list(rows_to_remove))\n",
    "                    if new_rows:\n",
    "                        processed_df = pd.concat([processed_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "                    self.pd_results_cleaned = processed_df\n",
    "                    self.progress.value = 100\n",
    "                    display(HTML(\"<b style='color:green;'>Processing complete.</b>\"))\n",
    "                else:\n",
    "                    display(HTML(\"<b style='color:orange;'>No decisions to process.</b>\"))\n",
    "\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red;'>Error in submit handler: {str(e)}</b>\"))\n",
    "                self.progress.value = 0\n",
    "\n",
    "            finally:\n",
    "                self.submit_button.disabled = False\n",
    "                self.reset_button.disabled = False\n",
    "\n",
    "        return self.pd_results_cleaned\n",
    "\n",
    "    def create_help_icon(self, tooltip_text):\n",
    "        \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "        return widgets.HTML(\n",
    "            f'<div title=\"{tooltip_text}\" style=\"display: inline-block;\">'\n",
    "            '<i class=\"fa fa-question-circle\" style=\"color: #007bff;\"></i>'\n",
    "            '</div>'\n",
    "        )\n",
    "\n",
    "    def _create_buttons(self):\n",
    "        \"\"\"Create submit and reset buttons\"\"\"\n",
    "        self.submit_button = widgets.Button(\n",
    "            description=\"Submit\",\n",
    "            button_style='success',\n",
    "            disabled=False\n",
    "        )\n",
    "        self.reset_button = widgets.Button(\n",
    "            description=\"Reset\",\n",
    "            button_style='warning',\n",
    "            disabled=False\n",
    "        )\n",
    "        self.progress = widgets.FloatProgress(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=100,\n",
    "            description='Processing:',\n",
    "            bar_style='info',\n",
    "            style={'bar_color': '#0080ff'},\n",
    "            orientation='horizontal',\n",
    "            layout=widgets.Layout(width='50%')\n",
    "        )\n",
    "\n",
    "        button_box = widgets.VBox([\n",
    "            widgets.HBox([self.submit_button, self.reset_button]),\n",
    "            self.progress\n",
    "        ])\n",
    "\n",
    "        self.reset_button.on_click(self._on_reset_button_clicked)\n",
    "        self.submit_button.on_click(lambda b: self._on_submit(b, self.pd_results.copy()))\n",
    "\n",
    "        return button_box\n",
    "\n",
    "    def _on_reset_button_clicked(self, b):\n",
    "        \"\"\"Handle reset button click by resetting options to default values\"\"\"\n",
    "        # Disable buttons during reset\n",
    "        self.submit_button.disabled = True\n",
    "        self.reset_button.disabled = True\n",
    "\n",
    "        # Clear output area\n",
    "        with self.protein_output_area:\n",
    "            self.protein_output_area.clear_output()\n",
    "            display(HTML(\"<b style='color:blue;'>Resetting options to defaults...</b>\"))\n",
    "\n",
    "        # Reset progress bar\n",
    "        self.progress.value = 0\n",
    "\n",
    "        try:\n",
    "            # Reset each input field to its default value based on Master Protein Accessions\n",
    "            df = self.pd_results.copy()\n",
    "            processed = 0\n",
    "            total_inputs = len(self.decision_inputs)\n",
    "\n",
    "            for combo, protein, input_field in self.decision_inputs:\n",
    "                # Find rows with this combination\n",
    "                proteins = combo.split('; ')\n",
    "                combo_rows = []\n",
    "                for _, row in df.iterrows():\n",
    "                    if pd.isna(row['Positions in Proteins']):\n",
    "                        continue\n",
    "                    row_proteins = set(p.split()[0] for p in row['Positions in Proteins'].split('; '))\n",
    "                    if row_proteins == set(proteins):\n",
    "                        combo_rows.append(row)\n",
    "\n",
    "                # Determine default decision\n",
    "                default_decision = 'asis'\n",
    "                if combo_rows:\n",
    "                    first_row = combo_rows[0]\n",
    "                    if not pd.isna(first_row['Master Protein Accessions']):\n",
    "                        master_proteins = first_row['Master Protein Accessions'].split(';')\n",
    "                        master_proteins = [p.strip() for p in master_proteins]\n",
    "                        default_decision = 'new' if protein in master_proteins else 'remove'\n",
    "\n",
    "                # Set input field value\n",
    "                input_field.value = default_decision\n",
    "\n",
    "                # Update status display\n",
    "                status_display = self.status_displays[(combo, protein)]\n",
    "                status_text = {\n",
    "                    'new': \"Will be created as new row\",\n",
    "                    'remove': \"Will be removed\",\n",
    "                    'asis': \"Will keep as is\",\n",
    "                    'Custom: (protein ID)': \"ie. Custom: P02666A1\"\n",
    "                }\n",
    "                status_display.value = f'<span style=\"color: gray\">{status_text[default_decision]}</span>'\n",
    "\n",
    "                # Update progress\n",
    "                processed += 1\n",
    "                self.progress.value = (processed / total_inputs) * 100\n",
    "\n",
    "            # Reset internal state\n",
    "            self.user_decisions = {}\n",
    "            self.pd_results_cleaned = self.pd_results.copy()\n",
    "\n",
    "            with self.protein_output_area:\n",
    "                self.protein_output_area.clear_output()\n",
    "                display(HTML(\"<b style='color:green;'>Reset complete. All options set to defaults.</b>\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            with self.protein_output_area:\n",
    "                self.protein_output_area.clear_output()\n",
    "                display(HTML(f\"<b style='color:red;'>Error during reset: {str(e)}</b>\"))\n",
    "\n",
    "        finally:\n",
    "            # Re-enable buttons\n",
    "            self.submit_button.disabled = False\n",
    "            self.reset_button.disabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "0b182afc-9770-4334-b931-7306109d5a35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ProcessingWorkflow:\n",
    "    def __init__(self):\n",
    "        self.data_transformer = DataTransformation()\n",
    "        self.protein_handler = ProteinCombinationHandler(self.data_transformer)\n",
    "        self.group_processor = GroupProcessing()\n",
    "        \n",
    "        # Set up observers\n",
    "        self.data_transformer.observe(self._handle_data_change, names=['pd_results'])\n",
    "        self.data_transformer.observe(self._handle_fasta_change, names=['protein_dict'])\n",
    "            \n",
    "    def _handle_data_change(self, change):\n",
    "        \"\"\"Handle changes in proteomics data\"\"\"\n",
    "        if change.name == 'pd_results':\n",
    "            with protein_mapping_output:\n",
    "                protein_mapping_output.clear_output()\n",
    "                if change.new is not None:\n",
    "                    self.protein_handler.protein_mapping_widget.disabled = False\n",
    "                    #display(HTML(\"<h3>Multiple Protein Mappings</h3>\"))\n",
    "                    self.protein_handler.pd_results = change.new\n",
    "                    #self.protein_handler.process_protein_mapping()\n",
    "                else:\n",
    "                    display(HTML(\"<h3>Multiple Protein Mappings</h3>\"))\n",
    "                    display(HTML(\"<b style='color:orange;'>Waiting for proteomics data to be uploaded...</b>\"))\n",
    "                    #self.protein_handler.process_protein_mapping()\n",
    "\n",
    "            self.group_processor.update_data(change.new)\n",
    "            \n",
    "    def _handle_fasta_change(self, change):\n",
    "        \"\"\"Handle changes in FASTA data\"\"\"\n",
    "        if change.new != change.old:\n",
    "            # No need to copy dictionary since we're using property access\n",
    "            with protein_mapping_output:\n",
    "                protein_mapping_output.clear_output()\n",
    "                display(HTML(\"<h3>Multiple Protein Mappings</h3>\"))\n",
    "                print(f\"Using updated protein dictionary with {len(self.data_transformer.protein_dict)} proteins\")\n",
    "                #if self.protein_handler.pd_results is not None:\n",
    "                #    self.protein_handler.process_protein_mapping()\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the complete workflow interface\"\"\"\n",
    "        self.data_transformer.setup_data_loading_ui()\n",
    "        display(widgets.HTML(\"<br>\"))\n",
    "        display(widgets.HTML(\"<h3><u>Protein Mapping</u></h3>\"))\n",
    "        display(self.protein_handler.protein_mapping_widget)\n",
    "\n",
    "        display(protein_mapping_output)\n",
    "        display(self.protein_handler.protein_mapping_output_area)\n",
    "        #with protein_mapping_output:\n",
    "            #if self.data_transformer.pd_results is not None:\n",
    "                #self.protein_handler.process_protein_mapping()\n",
    "        \n",
    "        display(widgets.HTML(\"<br>\"))\n",
    "        with group_processing_output:\n",
    "            self.group_processor.display_group_selector()\n",
    "            self.group_processor.display_widgets()\n",
    "        display(group_processing_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "16f50828-45c1-444e-be98-4ed3ff7a947e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CombineAverageDataframes:\n",
    "    def __init__(self, data_transformer, group_processor, protein_handler):\n",
    "        self.data_transformer = data_transformer\n",
    "        self.group_processor = group_processor\n",
    "        self.pd_results = data_transformer.pd_results\n",
    "        self.mbpdb_results = data_transformer.mbpdb_results\n",
    "        self.pd_results_cleaned = protein_handler.pd_results_cleaned if hasattr(protein_handler, 'pd_results_cleaned') and protein_handler.pd_results_cleaned is not None else pd.DataFrame()\n",
    "        self._merged_df = None\n",
    "        # Set up observer for data changes\n",
    "        self.data_transformer.observe(self._handle_data_change, names=['pd_results', 'mbpdb_results'])\n",
    "        \n",
    "    @property  # Make protein_dict a property that always reads from data_transformer\n",
    "    def protein_dict(self):\n",
    "        return self.data_transformer.protein_dict\n",
    "        \n",
    "    def _handle_data_change(self, change):\n",
    "        \"\"\"Handle changes in the input data.\"\"\"\n",
    "        if change.name == 'pd_results':\n",
    "            self.pd_results = change.new\n",
    "        elif change.name == 'mbpdb_results':\n",
    "            self.mbpdb_results = change.new\n",
    "        elif change.name == 'pd_results_cleaned':\n",
    "            self.pd_results_cleaned = change.new        # Re-run interactive display\n",
    "        clear_output()        \n",
    "    @property\n",
    "    def merged_df(self):\n",
    "        \"\"\"Property to access the merged DataFrame.\"\"\"\n",
    "        return self._merged_df\n",
    "        \n",
    "    def add_protein_info(self, df):\n",
    "        \"\"\"\n",
    "        Adds protein species and name information to the dataframe based on Master Protein Accessions,\n",
    "        inserting them after Master Protein Accessions and before Positions in Proteins.\n",
    "        \n",
    "        Args:\n",
    "            df (pandas.DataFrame): Input dataframe containing 'Master Protein Accessions' column\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: DataFrame with added 'protein_species' and 'protein_name' columns\n",
    "        \"\"\"\n",
    "        # First, make a copy to avoid modifying the original\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Create temporary columns\n",
    "        df['protein_species'] = 'Unknown'\n",
    "        df['protein_name'] = 'Unknown Protein'\n",
    "        \n",
    "        # Process each row\n",
    "        for idx, row in df.iterrows():\n",
    "            # Get the protein accessions - handle potential multiple proteins\n",
    "            proteins = str(row['Master Protein Accessions']).split(';')\n",
    "            \n",
    "            # Process first protein in the list (primary protein)\n",
    "            if proteins and proteins[0] != '' and proteins[0] != 'nan':\n",
    "                protein = proteins[0].strip()\n",
    "                df.at[idx, 'protein_species'] = self.protein_dict.get(protein, {}).get('species', \"Unknown\")\n",
    "                df.at[idx, 'protein_name'] = self.protein_dict.get(protein, {}).get('name', \"Unknown Protein\")\n",
    "        \n",
    "        # Get all column names\n",
    "        all_cols = list(df.columns)\n",
    "        \n",
    "        # Remove the new columns from their current position\n",
    "        remaining_cols = [col for col in all_cols if col not in ['protein_species', 'protein_name']]\n",
    "        \n",
    "        # Find the position after 'Master Protein Accessions'\n",
    "        insert_pos = remaining_cols.index('Master Protein Accessions') + 1\n",
    "        \n",
    "        # Create the new column order\n",
    "        new_cols = (\n",
    "            remaining_cols[:insert_pos] +  # Columns before and including Master Protein Accessions\n",
    "            ['protein_species', 'protein_name'] +  # New columns\n",
    "            remaining_cols[insert_pos:]  # Remaining columns\n",
    "        )\n",
    "        \n",
    "        # Reorder the DataFrame with the new column order\n",
    "        result_df = df.reindex(columns=new_cols)\n",
    "        \n",
    "        # Verify column order (optional debug print)\n",
    "        # print(\"Column order:\", new_cols)\n",
    "        # print(\"Position of Master Protein Accessions:\", insert_pos)\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    def extract_bioactive_peptides(self):\n",
    "        \"\"\"\n",
    "        Extracts the list of bioactive peptide matches from the imported MBPDB search.\n",
    "        \"\"\"\n",
    "        if not self.mbpdb_results.empty:\n",
    "            # Drop rows where protein_id is NaN or 'None'\n",
    "            mbpdb_results_cleaned = self.mbpdb_results.copy()\n",
    "            mbpdb_results_cleaned.dropna(subset=['search_peptide'], inplace=True)\n",
    "            mbpdb_results_cleaned = mbpdb_results_cleaned[mbpdb_results_cleaned['protein_id'] != 'None']\n",
    "\n",
    "            # Check if '% Alignment' column exists\n",
    "            if '% Alignment' in mbpdb_results_cleaned.columns:\n",
    "                agg_dict = {\n",
    "                    'peptide': 'first', \n",
    "                    'protein_id': 'first',\n",
    "                    'protein_description': 'first',\n",
    "                    '% Alignment': 'first',\n",
    "                    'species': 'first',\n",
    "                    'intervals': 'first',\n",
    "                    'function': lambda x: list(x.dropna().unique())\n",
    "                }\n",
    "            else:\n",
    "                agg_dict = {\n",
    "                    'peptide': 'first', \n",
    "                    #'search_peptide': 'first',\n",
    "                    'protein_description': 'first',\n",
    "                    'species': 'first',\n",
    "                    'intervals': 'first',\n",
    "                    'function': lambda x: list(x.dropna().unique())\n",
    "                }\n",
    "\n",
    "            # Perform the groupby and aggregation\n",
    "            self.mbpdb_results_grouped = mbpdb_results_cleaned.groupby('search_peptide').agg(agg_dict).reset_index()\n",
    "\n",
    "            # Flatten the 'function' list\n",
    "            self.mbpdb_results_grouped['function'] = self.mbpdb_results_grouped['function'].apply(\n",
    "                lambda x: '; '.join(x) if isinstance(x, list) else x\n",
    "            )\n",
    "            return mbpdb_results_cleaned, self.mbpdb_results_grouped\n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    def create_unique_id(self, row):\n",
    "        \"\"\"Creates a unique ID for each peptide row.\"\"\"\n",
    "        # Handle Sequence - convert list to comma-separated string if needed\n",
    "        sequence = row['Sequence']\n",
    "        if isinstance(sequence, list):\n",
    "            sequence = ','.join(sequence)\n",
    "        else:\n",
    "            sequence = str(sequence).strip()\n",
    "        \n",
    "        # Create unique ID with modifications if present\n",
    "        if pd.notna(row['Modifications']):\n",
    "            unique_id = sequence + \"_\" + row['Modifications'].strip()\n",
    "        else:\n",
    "            unique_id = sequence\n",
    "        \n",
    "        # Ensure unique_id is a string and strip trailing underscores\n",
    "        unique_id = str(unique_id).strip()\n",
    "        return unique_id.rstrip('_')\n",
    "\n",
    "    def process_pd_results(self, mbpdb_results_grouped):\n",
    "        pd_results_cleaned = self.pd_results_cleaned\n",
    "        \n",
    "        # Process positions and accessions\n",
    "        #pd_results_cleaned['Positions in Proteins'] = pd_results_cleaned['Positions in Proteins'].str.split(';', expand=False).str[0]\n",
    "        #pd_results_cleaned['Master Protein Accessions'] = pd_results_cleaned['Master Protein Accessions'].str.split(';', expand=False).str[0]\n",
    "                    \n",
    "        # Handle NaN/Unknown values first\n",
    "        pd_results_cleaned['Master Protein Accessions'] = pd_results_cleaned['Master Protein Accessions'].fillna('Unknown')\n",
    "        pd_results_cleaned['Positions in Proteins'] = pd_results_cleaned['Positions in Proteins'].fillna('Unknown')\n",
    "        \n",
    "        # Create sequence column if needed\n",
    "        # Create sequence column if needed\n",
    "        if 'Sequence' not in pd_results_cleaned.columns:\n",
    "            # First create Sequence column with NaN values\n",
    "            pd_results_cleaned['Sequence'] = pd.NA\n",
    "            \n",
    "            def extract_sequence(annotated_seq):\n",
    "                if pd.isna(annotated_seq):\n",
    "                    return pd.NA\n",
    "                \n",
    "                # Case 1: [X].SEQUENCE.[X] format\n",
    "                if '.' in annotated_seq:\n",
    "                    parts = annotated_seq.split('.')\n",
    "                    if len(parts) > 1:\n",
    "                        return parts[1]\n",
    "                \n",
    "                # Case 2: Plain sequence like \"LLL\" or \"WE\"\n",
    "                return annotated_seq\n",
    "            \n",
    "            # Apply the extraction function to all rows\n",
    "            pd_results_cleaned['Sequence'] = pd_results_cleaned['Annotated Sequence'].apply(extract_sequence)\n",
    "        \n",
    "        # Create unique ID\n",
    "        pd_results_cleaned['unique ID'] = pd_results_cleaned.apply(self.create_unique_id, axis=1)\n",
    "\n",
    "        # Extract start and stop positions\n",
    "        try:\n",
    "            # Initialize start and stop columns with NaN\n",
    "            pd_results_cleaned['start'] = pd.NA\n",
    "            pd_results_cleaned['stop'] = pd.NA\n",
    "            \n",
    "            # Create mask for rows without semicolons (single positions) and not Unknown\n",
    "            valid_position_mask = (~pd_results_cleaned['Positions in Proteins'].str.contains(';', na=False) & \n",
    "                                 (pd_results_cleaned['Positions in Proteins'] != 'Unknown'))\n",
    "            \n",
    "            # Process rows with single positions\n",
    "            single_positions = pd_results_cleaned.loc[valid_position_mask, 'Positions in Proteins']\n",
    "            if not single_positions.empty:\n",
    "                extracted = single_positions.str.extract(r'\\[(\\d+)-(\\d+)\\]')\n",
    "                \n",
    "                # Convert to numeric and handle invalid values\n",
    "                pd_results_cleaned.loc[valid_position_mask, 'start'] = pd.to_numeric(extracted[0], errors='coerce')\n",
    "                pd_results_cleaned.loc[valid_position_mask, 'stop'] = pd.to_numeric(extracted[1], errors='coerce')\n",
    "            \n",
    "            # Convert to Int64 to handle missing values properly\n",
    "            pd_results_cleaned['start'] = pd_results_cleaned['start'].astype('Int64')\n",
    "            pd_results_cleaned['stop'] = pd_results_cleaned['stop'].astype('Int64')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing positions: {str(e)}\")\n",
    "        \n",
    "    \n",
    "        # Reorder columns with unique ID and Sequence first\n",
    "        remaining_cols = [col for col in pd_results_cleaned.columns \n",
    "                         if col not in ['unique ID', 'Sequence', 'Master Protein Accessions', \n",
    "                                      'Positions in Proteins', 'start', 'stop']]\n",
    "        \n",
    "        columns_order = ['unique ID', 'Sequence', 'Master Protein Accessions', \n",
    "                        'Positions in Proteins', 'start', 'stop'] + remaining_cols\n",
    "        \n",
    "        pd_results_cleaned = pd_results_cleaned[columns_order]\n",
    "                \n",
    "        # Merge with MBPDB results if available\n",
    "        if self.mbpdb_results_grouped is not None and not self.mbpdb_results_grouped.empty:\n",
    "            # First do the regular merge\n",
    "            merged_df = pd.merge(pd_results_cleaned, self.mbpdb_results_grouped, \n",
    "                                right_on='search_peptide', left_on='unique ID', how='left')\n",
    "            \n",
    "            # Second pass: handle comma-separated unique IDs\n",
    "            comma_mask = merged_df['unique ID'].str.contains(',', na=False)\n",
    "            comma_rows = merged_df[comma_mask].copy()\n",
    "            \n",
    "            for idx, row in comma_rows.iterrows():\n",
    "                # Split the unique ID\n",
    "                unique_ids = row['unique ID'].split(',')\n",
    "                \n",
    "                # Check if any part matches with search_peptide\n",
    "                matches = self.mbpdb_results_grouped[self.mbpdb_results_grouped['search_peptide'].isin(unique_ids)]\n",
    "\n",
    "                if not matches.empty:\n",
    "                    # Take the first match and update all MBPDB columns\n",
    "                    match = matches.iloc[0]\n",
    "                    for col in self.mbpdb_results_grouped.columns:\n",
    "                        #if col != 'search_peptide':  # Don't overwrite unique ID\n",
    "                        merged_df.loc[idx, col] = match[col]\n",
    "        \n",
    "            display(HTML(\"<b style='color:green;'>The MBPDB was successfully merged with the peptidomic data matching the Search Peptide and Unique ID columns (including comma-separated IDs).</b>\"))\n",
    "        \n",
    "        else:\n",
    "            merged_df = pd_results_cleaned.copy()\n",
    "            merged_df['function'] = np.nan\n",
    "            display(HTML(\"<b style='color:orange;'>No MBPDB was uploaded.</b>\"))\n",
    "            display(HTML(\"<b style='color:orange;'>The merged Dataframe contains only peptidomic data.</b>\"))\n",
    "        \n",
    "        # Ensure columns are in correct order\n",
    "        final_column_order = columns_order + [col for col in merged_df.columns if col not in columns_order]\n",
    "        merged_df = merged_df[final_column_order]\n",
    "        \n",
    "        return merged_df\n",
    "    \n",
    "    def calculate_group_abundance_averages(self, df, group_data):\n",
    "        \"\"\"Calculates group abundance averages and SEMs, organizing them with averages first, then SEMs.\"\"\"\n",
    "        # Check if all average abundance columns already exist\n",
    "        all_columns_exist = True\n",
    "        for group_number, details in group_data.items():\n",
    "            average_column_name = f\"Avg_{details['grouping_variable']}\"\n",
    "            if average_column_name not in df.columns:\n",
    "                all_columns_exist = False\n",
    "                break\n",
    "        \n",
    "        if all_columns_exist:\n",
    "            display(HTML('<b style=\"color:orange;\">All average abundance columns already exist. Returning original DataFrame.</b>'))\n",
    "            return df\n",
    "        \n",
    "        # If not all columns exist, proceed with calculations\n",
    "        average_columns = {}\n",
    "        \n",
    "        # Calculate all averages and SEMs but store them separately\n",
    "        for group_number, details in group_data.items():\n",
    "            grouping_variable = details['grouping_variable']\n",
    "            abundance_columns = details['abundance_columns']\n",
    "            \n",
    "            # Convert abundance columns to numeric\n",
    "            for col in abundance_columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Define column names\n",
    "            average_column_name = f\"Avg_{grouping_variable}\"\n",
    "        \n",
    "            average_columns[average_column_name] = df[abundance_columns].mean(axis=1, skipna=True)\n",
    "        \n",
    "        # Combine the columns in the desired order (all averages, then all SEMs)\n",
    "        new_columns = {**average_columns}\n",
    "        \n",
    "        # Add new columns to DataFrame\n",
    "        df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "        \n",
    "        if not df.empty:\n",
    "            display(HTML('<b style=\"color:green;\">Group average columns have been successfully added to the DataFrame.</b>'))\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def process_data(self, group_data):\n",
    "        \"\"\"Main method to process all data.\"\"\"\n",
    "        if hasattr(self, 'pd_results') and self.pd_results is not None and not self.pd_results.empty:\n",
    "            try:\n",
    "                # Extract and process bioactive peptides\n",
    "                mbpdb_results_cleaned, self.mbpdb_results_grouped = self.extract_bioactive_peptides()\n",
    "                \n",
    "                if not hasattr(self, 'pd_results_cleaned') or self.pd_results_cleaned is None:\n",
    "                    self.pd_results_cleaned = self.pd_results.copy()\n",
    "                \n",
    "                # Process PD results and merge with MBPDB\n",
    "                merged_df_temp = self.process_pd_results(self.mbpdb_results_grouped)\n",
    "                \n",
    "                # Calculate abundance averages if group_data exists\n",
    "                if group_data:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                        final_df_temp = self.calculate_group_abundance_averages(merged_df_temp, group_data)\n",
    "                else:\n",
    "                    final_df_temp = merged_df_temp\n",
    "                    display(HTML(\"<b style='color:orange;'>No group data provided. Skipping abundance calculations.</b>\"))\n",
    "        \n",
    "                \n",
    "                # Store the final DataFrame and add protien name and species \n",
    "                final_df = self.add_protein_info(final_df_temp)\n",
    "                self._merged_df = final_df\n",
    "\n",
    "                return final_df\n",
    "\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red;'>Error processing data: {str(e)}</b>\"))\n",
    "                return None\n",
    "        else:\n",
    "            display(HTML(\"<b style='color:red;'>No PD results data available for processing.</b>\"))\n",
    "            return None\n",
    "            \n",
    "    def update_data(self, pd_results):\n",
    "        \"\"\"Update data and refresh filtered columns\"\"\"\n",
    "        self.pd_results = pd_results\n",
    "        \n",
    "        # Only update if we have valid data\n",
    "        if pd_results is not None and not pd_results.empty:\n",
    "            self.setup_data()\n",
    "            \n",
    "            # Update the dropdown with new filtered columns\n",
    "            self.column_dropdown.options = self.filtered_columns\n",
    "            \n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:green;\">Data updated successfully. Column selection refreshed.</b>'))\n",
    "        else:\n",
    "            # Clear options if no data\n",
    "            self.column_dropdown.options = []\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:orange;\">No data available for column selection.</b>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "20958467-4a02-4b10-afbb-1a3224430ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportManager:\n",
    "    \"\"\"Class to manage all export operations with predefined buttons\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Create output area for status messages\n",
    "        self.status_output = widgets.Output()\n",
    "        \n",
    "        # Create all export buttons\n",
    "        self.mbpdb_button = widgets.Button(\n",
    "            description='Download MBPDB Results',\n",
    "            icon='download',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            tooltip='Download the results from searching your peptides against the MBPDB database',\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        self.group_data_button = widgets.Button(\n",
    "            description='Download Group Definitions',\n",
    "            icon='download',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            tooltip='Download the categorical variable definitions used for data grouping and analysis',\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        self.dataset_button = widgets.Button(\n",
    "            description='Download Merged Dataset',\n",
    "            icon='download',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            tooltip='Download the complete merged dataset containing all processed data',\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        # Add click handlers\n",
    "        self.mbpdb_button.on_click(self._handle_mbpdb_download)\n",
    "        self.group_data_button.on_click(self._handle_group_download)\n",
    "        self.dataset_button.on_click(self._handle_dataset_download)\n",
    "        \"\"\"\n",
    "        # Create labels for descriptions\n",
    "        self.mbpdb_desc = widgets.HTML(\n",
    "            value='<div style=\"color: #666; font-style: italic; margin: 5px 0;\">Download MBPDB search results and bioactivity data</div>'\n",
    "        )\n",
    "        self.group_desc = widgets.HTML(\n",
    "            value='<div style=\"color: #666; font-style: italic; margin: 5px 0;\">Download categorical variable definitions for data grouping</div>'\n",
    "        )\n",
    "        self.dataset_desc = widgets.HTML(\n",
    "            value='<div style=\"color: #666; font-style: italic; margin: 5px 0;\">Download the complete processed dataset</div>'\n",
    "        )\n",
    "        \"\"\"\n",
    "        # Store references to data\n",
    "        self.mbpdb_df = None\n",
    "        self.group_data = None\n",
    "        self.merged_df = None\n",
    "        \n",
    "        # Create button container with spacing\n",
    "        self.button_container = widgets.VBox([\n",
    "            self.mbpdb_button,\n",
    "            self.group_data_button,\n",
    "            self.dataset_button,\n",
    "        ],layout=widgets.Layout(width='310px'),\n",
    "        )\n",
    "\n",
    "    def _trigger_download(self, content, filename, mime_type):\n",
    "        \"\"\"Helper method to trigger file download\"\"\"\n",
    "        if isinstance(content, str):\n",
    "            content = content.encode('utf-8')\n",
    "            \n",
    "        b64_data = base64.b64encode(content).decode('utf-8')\n",
    "        file_data = f\"data:{mime_type};base64,{b64_data}\"\n",
    "        \n",
    "        with self.status_output:\n",
    "            self.status_output.clear_output(wait=True)\n",
    "            display(HTML(f\"\"\"\n",
    "                <div id=\"download_{filename}\">\n",
    "                    <a id=\"download_link_{filename}\" \n",
    "                       href=\"{file_data}\" \n",
    "                       download=\"{filename}\"\n",
    "                       style=\"display: none;\"></a>\n",
    "                    <script>\n",
    "                        document.getElementById('download_link_{filename}').click();\n",
    "                        setTimeout(() => {{\n",
    "                            document.getElementById('download_{filename}').remove();\n",
    "                        }}, 1000);\n",
    "                    </script>\n",
    "                </div>\n",
    "            \"\"\"))\n",
    "            display(HTML(f'<div style=\"color: green\">Successfully downloaded {filename}</div>'))\n",
    "\n",
    "    def _handle_mbpdb_download(self, b):\n",
    "        \"\"\"Handle MBPDB results download\"\"\"\n",
    "        try:\n",
    "            if self.mbpdb_df is not None and 'function' in self.mbpdb_df.columns:\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filename = f\"MBPDB_SEARCH_{timestamp}.tsv\"\n",
    "                content = self.mbpdb_df.to_csv(sep='\\t', index=False)\n",
    "                self._trigger_download(content, filename, 'text/tab-separated-values')\n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                self.status_output.clear_output(wait=True)\n",
    "                display(HTML(f'<div style=\"color: red\">Error downloading MBPDB results: {str(e)}</div>'))\n",
    "\n",
    "    def _handle_group_download(self, b):\n",
    "        \"\"\"Handle group data download\"\"\"\n",
    "        try:\n",
    "            if self.group_data:\n",
    "                # Convert enumerated format to simplified format\n",
    "                simplified_data = {}\n",
    "                for _, group_info in self.group_data.items():\n",
    "                    group_name = group_info['grouping_variable']\n",
    "                    abundance_cols = group_info['abundance_columns']\n",
    "                    simplified_data[group_name] = abundance_cols\n",
    "                \n",
    "                # Create filename with timestamp\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filename = f\"Categorical_variable_definitions_{timestamp}.json\"\n",
    "                \n",
    "                # Convert to JSON with indentation\n",
    "                content = json.dumps(simplified_data, indent=4)\n",
    "                \n",
    "                # Trigger download\n",
    "                self._trigger_download(content, filename, 'application/json')\n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                self.status_output.clear_output(wait=True)\n",
    "                display(HTML(f'<div style=\"color: red\">Error downloading group data: {str(e)}</div>'))\n",
    "\n",
    "    def _handle_dataset_download(self, b):\n",
    "        \"\"\"Handle Merged Dataset download\"\"\"\n",
    "        try:\n",
    "            if self.merged_df is not None:\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filename = f\"Merged_Dataframe_{timestamp}.csv\"\n",
    "                content = self.merged_df.to_csv(index=False)\n",
    "                self._trigger_download(content, filename, 'text/csv')\n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                self.status_output.clear_output(wait=True)\n",
    "                display(HTML(f'<div style=\"color: red\">Error downloading dataset: {str(e)}</div>'))\n",
    "\n",
    "    def update_data(self, mbpdb_df=None, group_data=None, merged_df=None):\n",
    "        \"\"\"Update data and enable/disable buttons accordingly\"\"\"\n",
    "        self.mbpdb_df = mbpdb_df\n",
    "        self.group_data = group_data\n",
    "        self.merged_df = merged_df\n",
    "        \n",
    "        # Enable/disable buttons based on data availability\n",
    "        self.mbpdb_button.disabled = not (mbpdb_df is not None and 'function' in mbpdb_df.columns)\n",
    "        self.group_data_button.disabled = not bool(group_data)\n",
    "        self.dataset_button.disabled = merged_df is None\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display all outputs\"\"\"\n",
    "        display(self.button_container)\n",
    "        display(self.status_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "4b506a35-a853-46ee-b193-3fce79decf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessingController:\n",
    "    def __init__(self, workflow):\n",
    "        self.workflow = workflow  # Store reference to workflow\n",
    "        self.export_manager = ExportManager()\n",
    "        self.data_transformer = self.workflow.data_transformer\n",
    "        self.combiner = None\n",
    "        self.merged_df = None\n",
    "        \n",
    "        # Create processing button\n",
    "        self.process_button = widgets.Button(\n",
    "            description='Process Data',\n",
    "            button_style='success',\n",
    "            icon='refresh',\n",
    "            tooltip='Click to start data processing'\n",
    "        )\n",
    "                \n",
    "        # Create button container\n",
    "        self.button_container = widgets.HBox([self.process_button])\n",
    "        \n",
    "        # Create separate output areas\n",
    "        self.process_output = widgets.Output()\n",
    "        self.export_output = widgets.Output()\n",
    "        self.search_output = widgets.Output()\n",
    "        self.stats_output = widgets.Output()\n",
    "        self.grid_output = widgets.Output()\n",
    "        \n",
    "        # Set up button callbacks\n",
    "        self.process_button.on_click(self._on_process_clicked)\n",
    "        \n",
    "        # Initialize export manager with disabled buttons\n",
    "        self.export_manager.update_data(None, None, None)\n",
    "        \n",
    "    def display_interactive_results(self, df):\n",
    "        \"\"\"Display interactive grid with row search functionality\"\"\"\n",
    "        if df is not None:\n",
    "            # Create search widget\n",
    "            search_widget = widgets.Text(\n",
    "                placeholder='Search for data in rows...',\n",
    "                description='Search:',\n",
    "                layout=widgets.Layout(width='50%'),\n",
    "                style={'description_width': 'initial'}\n",
    "            )\n",
    "            \n",
    "            def get_column_category(col):\n",
    "                \"\"\"Determine category for each column\"\"\"\n",
    "                if col.startswith('Avg_'):\n",
    "                    return 'Average Abundance'\n",
    "                elif col in self.data_transformer.mbpdb_results.columns:\n",
    "                    return 'MBPDB Search Results'\n",
    "                else:\n",
    "                    return 'Peptidomic Data'\n",
    "\n",
    "            # Create multi-level columns while preserving order\n",
    "            column_tuples = [(get_column_category(col), col) for col in df.columns]\n",
    "            \n",
    "            df_display = df.copy()\n",
    "            df_display.columns = pd.MultiIndex.from_tuples(column_tuples)\n",
    "         \n",
    "            def create_grid(df_to_display):\n",
    "                grid = DataGrid(\n",
    "                    df_to_display, \n",
    "                    selection_mode='cell', \n",
    "                    editable=False,\n",
    "                    layout=widgets.Layout(height='600px')\n",
    "                )\n",
    "                grid.auto_fit_columns = True\n",
    "                grid.base_row_size = 25\n",
    "                grid.base_column_size = 150\n",
    "                grid.auto_fit_params = {'area': 'column', 'padding': 10}\n",
    "                return grid\n",
    "            \n",
    "            def on_search_change(change):\n",
    "                with self.grid_output:\n",
    "                    self.grid_output.clear_output()\n",
    "                    \n",
    "                    search_term = change['new'].strip()\n",
    "                    if search_term:\n",
    "                        str_df = df_display.astype(str)\n",
    "                        mask = str_df.apply(\n",
    "                            lambda row: row.str.contains(search_term, case=False, na=False).any(),\n",
    "                            axis=1\n",
    "                        )\n",
    "                        filtered_df = df_display[mask]\n",
    "                        \n",
    "                        with self.stats_output:\n",
    "                            self.stats_output.clear_output()\n",
    "                            print(f\"Found {len(filtered_df)} matching rows out of {len(df_display)} total rows\")\n",
    "                    else:\n",
    "                        filtered_df = df_display\n",
    "                        with self.stats_output:\n",
    "                            self.stats_output.clear_output()\n",
    "                    \n",
    "                    display(create_grid(filtered_df))\n",
    "            \n",
    "            search_widget.observe(on_search_change, names='value')\n",
    "\n",
    "            # Display search interface\n",
    "            with self.search_output:\n",
    "                self.search_output.clear_output()\n",
    "                display(search_widget)\n",
    "            \n",
    "            # Initialize grid display\n",
    "            with self.grid_output:\n",
    "                self.grid_output.clear_output()\n",
    "                display(create_grid(df_display))\n",
    "            \n",
    "        else:\n",
    "            print(\"No data to display\")\n",
    "\n",
    "    def _on_process_clicked(self, b):\n",
    "        # Clear all outputs\n",
    "        self.process_output.clear_output()\n",
    "        self.search_output.clear_output()\n",
    "        self.stats_output.clear_output()\n",
    "        self.grid_output.clear_output()\n",
    "        \n",
    "        with self.process_output:           \n",
    "            # Pass the actual data_transformer, not the workflow\n",
    "            self.combiner = CombineAverageDataframes(\n",
    "                self.workflow.data_transformer,  # Pass the data_transformer directly\n",
    "                self.workflow.group_processor, \n",
    "                self.workflow.protein_handler\n",
    "            )\n",
    "            self.merged_df = self.combiner.process_data(self.workflow.group_processor.group_data)\n",
    "            \n",
    "            if self.merged_df is not None:\n",
    "                display(HTML(\n",
    "                    f'<b style=\"color:green;\">\\nData processing completed successfully!</b>'))\n",
    " \n",
    "                # Enable export buttons after successful processing\n",
    "                self.export_manager.update_data(\n",
    "                    mbpdb_df=self.workflow.data_transformer.mbpdb_results if hasattr(self.workflow.data_transformer, 'mbpdb_results') else None,\n",
    "                    group_data=self.workflow.group_processor.group_data,\n",
    "                    merged_df=self.merged_df\n",
    "                )\n",
    "                \n",
    "                self.display_interactive_results(self.merged_df)\n",
    "            else:\n",
    "                print(\"Error: No data was processed\")\n",
    "                display(HTML(\n",
    "                    f'<b style=\"color:red;\">Error: No data was processed.</b>'))\n",
    "                # Keep export buttons disabled\n",
    "                self.export_manager.update_data(None, None, None)\n",
    "                     \n",
    "    def _on_export_clicked(self, b):\n",
    "            \"\"\"Handle export button click\"\"\"\n",
    "            with self.export_output:\n",
    "                self.export_output.clear_output(wait=True)\n",
    "                \n",
    "                # Update export manager with current data\n",
    "                self.export_manager.update_data(\n",
    "                    mbpdb_df=self.workflow.data_transformer.mbpdb_results if hasattr(self.workflow.data_transformer, 'mbpdb_results') else None,\n",
    "                    group_data=self.workflow.group_processor.group_data,\n",
    "                    merged_df=self.merged_df\n",
    "                )\n",
    "                \n",
    "                # Display the export manager\n",
    "                self.export_manager.display()\n",
    "        \n",
    "    def display(self):\n",
    "        \"\"\"Display the complete interface\"\"\"\n",
    "        display(self.button_container)\n",
    "        display(self.process_output)\n",
    "        \n",
    "        # Always display the export manager buttons (they start disabled)\n",
    "        display(HTML(\"<h3><u>Export Options:</u></h3>\"))\n",
    "        self.export_manager.display()\n",
    "        \n",
    "        display(self.search_output)\n",
    "        display(self.stats_output)\n",
    "        display(self.grid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "1cffc526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078c591ca190453a8929dd4cf70b2f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.1â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ad7ffd50ae483faeee5b27a4f64a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3><u>Upload Peptidomic Data Files:</u></h3>'), HBox(children=(HBox(children=(Fileâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905c19d0681c4a938ca7c3318f1ec343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b47051d6f7f94c09bfaf18e2839c5c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3><u>Protein Mapping</u></h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf083102d204fef9d711bae3388d31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Process peptides mapped to multiple proteins?', disabled=True, options=(('Yes', Trueâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1357ec4554439fa72965ad9f9abc6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2875ce03eab4d2f8f50dcb745e45770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68b2891301542eab4840cf39eb66749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28be5a266e844fb98d3f7859b2f860b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98ceed94c4042d3b8dd555ce9395e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d210c71f694a98beb32569c209b0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Process Data', icon='refresh', style=ButtonStyle(),â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076c2a8c65b8437db2b4bad62217fbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3><u>Export Options:</u></h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7580814cef0643e0bf53e6f5237dd8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Button(button_style='info', description='Download MBPDB Results', disabled=True, icon='downloadâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c5267bf3e84bd687acc7ca7519817c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9ef46e6e094e92bd339f3864b5aaf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d62d6c5ed54d4093770b9eeea9b75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b2897edc2542c3a77f7b9483ed2870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow = ProcessingWorkflow()\n",
    "workflow.display()\n",
    "\n",
    "controller = DataProcessingController(workflow)\n",
    "controller.display()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Vt5dV-pTHRWngML-2nDQAR6_P_KFsIx4",
     "timestamp": 1712158917217
    },
    {
     "file_id": "1l7fpCQepyE1pJq2O5QfOHVv9a4VFpr-B",
     "timestamp": 1712094574841
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python (newenv)",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
