{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0fc630-508d-459d-8cb0-aea424d0d595",
   "metadata": {},
   "source": [
    "# Data Transormation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278kpom78fOP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23025,
     "status": "ok",
     "timestamp": 1712094448807,
     "user": {
      "displayName": "Russell Kuhfeld",
      "userId": "14760569517288879712"
     },
     "user_tz": 420
    },
    "id": "278kpom78fOP",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "95f300e9-4d05-4fe1-a725-f5c3eea6cf80",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Install/Import packages & define key varribles and functions\n",
    "# Run install script\n",
    "# %chmod +x setup_jupyterlab.sh\n",
    "# %./setup_jupyterlab.sh\n",
    "\n",
    "# Import necessary libraries for the script to function.\n",
    "import pandas as pd\n",
    "import csv, json, re, os, shutil, io, base64, time, subprocess, sqlite3\n",
    "from io import StringIO, BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from django.conf import settings\n",
    "from collections import defaultdict\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.formula.api import ols\n",
    "#from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import warnings\n",
    "\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import combinations\n",
    "from ipydatagrid import DataGrid\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from my_functions_datatransformation import (process_protein_combinations, setup_data_loading_ui, display_widgets, setup_widgets,\n",
    "                                     initialize_settings, check_and_add_protein, process_pd_results, extract_bioactive_peptides, select_proteins,\n",
    "                                     calculate_group_abundance_std_averages, export_dataframe, prompt_export_options, setup_widgets_vp, impute_missing_values,\n",
    "                                     adjust_sequence_interval, export_heatmap_data_to_dict, export_group_data, display_grouping_dictionary_selector,\n",
    "                                     replace_protein_accessions,check_sequence_alignment)\n",
    "import traitlets\n",
    "from traitlets import HasTraits, Instance, observe\n",
    "\n",
    "# Global variable declaration\n",
    "settings_dict = initialize_settings()\n",
    "globals().update(settings_dict)\n",
    "import _settings as settings\n",
    "global spec_translate_list\n",
    "spec_translate_list = settings.SPEC_TRANSLATE_LIST\n",
    "# Set the default font to Calibri\n",
    "#matplotlib.rcParams['font.family'] = 'Calibri'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e2ec3-1352-4c86-886d-3e2347b75925",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports Proteome Discover Data and MBPDB Bioactive Peptide Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d06c8cd-9a5f-45cb-bf09-1a96f1eb77bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DataTransformation(HasTraits):\n",
    "    pd_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    mbpdb_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    pd_results_cleaned = Instance(pd.DataFrame, allow_none=True)\n",
    "    search_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pd_results = pd.DataFrame()\n",
    "        self.pd_results_cleaned = pd.DataFrame()\n",
    "        self.mbpdb_results = pd.DataFrame()\n",
    "        self.search_results = pd.DataFrame()\n",
    "        self.proteins_dic = {}\n",
    "        self.output_area = None\n",
    "        self.mbpdb_uploader = None\n",
    "        self.pd_uploader = None\n",
    "        self.fasta_uploader = None\n",
    "        self.reset_button = None\n",
    "        self.search_widget = None\n",
    "        self.search_progress = None\n",
    "             \n",
    "    def setup_search_ui(self, peptides):\n",
    "        \"\"\"Initialize and display the search UI\"\"\"\n",
    "        # Create dropdown for similarity threshold\n",
    "        self.threshold_dropdown = widgets.Dropdown(\n",
    "            options=list(range(0, 101, 10)),\n",
    "            value=80,\n",
    "            description='Similarity Threshold:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        # Create search button\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search Peptides',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        \n",
    "        # Progress indicator\n",
    "        self.search_progress = widgets.HTML(\n",
    "            value=\"\",\n",
    "            layout=widgets.Layout(margin='10px 0px')\n",
    "        )\n",
    "        \n",
    "        # Connect button click to handler\n",
    "        self.search_button.on_click(lambda b: self._on_search_click(b, peptides))\n",
    "        \n",
    "        # Create layout\n",
    "        self.search_widget = widgets.VBox([\n",
    "            widgets.HBox([\n",
    "                self.threshold_dropdown, \n",
    "                self.search_button\n",
    "            ], layout=widgets.Layout(align_items='center')),\n",
    "            self.search_progress\n",
    "        ])\n",
    "        \n",
    "        display(self.search_widget)\n",
    "\n",
    "    def _on_search_click(self, b, peptides):\n",
    "        \"\"\"Handle search button click\"\"\"\n",
    "        self.search_progress.value = \"<b style='color:blue'>Searching...</b>\"\n",
    "        \n",
    "        try:\n",
    "            # Perform search\n",
    "            results = self._search_peptides_comprehensive(\n",
    "                peptides, \n",
    "                similarity_threshold=self.threshold_dropdown.value\n",
    "            )\n",
    "            \n",
    "            # Format results if we have any matches\n",
    "            if not results.empty:\n",
    "                self.search_results = self._format_search_results_with_matches(results)\n",
    "            else:\n",
    "                self.search_results = results\n",
    "            \n",
    "            # Show appropriate completion message\n",
    "            if self.search_results.empty:\n",
    "                self.search_progress.value = \"<b style='color:orange'>No matches found.</b>\"\n",
    "            else:\n",
    "                self.search_progress.value = f\"<b style='color:green'>Search complete! Found {len(self.search_results)} matches</b>\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.search_progress.value = f\"<b style='color:red'>Error: {str(e)}</b>\"\n",
    "            self.search_results = pd.DataFrame()\n",
    "    \n",
    "    def _search_peptides_comprehensive(self, peptides, similarity_threshold=100):\n",
    "        \"\"\"Search for peptides with BLAST-based similarity matching\"\"\"\n",
    "        \n",
    "        WORK_DIRECTORY = '/home/kuhfeldrf/mbpdb/include/peptide/uploads/temp'\n",
    "        conn = sqlite3.connect('/home/kuhfeldrf/mbpdb/include/peptide/db.sqlite3')\n",
    "        work_path = self._create_work_directory(WORK_DIRECTORY)\n",
    "        \n",
    "        fasta_db_path = os.path.join(work_path, \"db.fasta\")\n",
    "        results = []\n",
    "        extra_info = defaultdict(list)\n",
    "        \n",
    "        # Create database with all peptides for BLAST\n",
    "        query = \"SELECT p.id, p.peptide FROM peptide_peptideinfo p\"\n",
    "        db_peptides = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        # Create BLAST database\n",
    "        with open(fasta_db_path, 'w') as f:\n",
    "            for _, row in db_peptides.iterrows():\n",
    "                f.write(f\">{row['id']}\\n{row['peptide']}\\n\")\n",
    "                \n",
    "        self._make_blast_db(fasta_db_path)\n",
    "        \n",
    "        for peptide in peptides:\n",
    "            if similarity_threshold == 100:\n",
    "                query = \"\"\"\n",
    "                SELECT DISTINCT\n",
    "                    ? as search_peptide,\n",
    "                    pi.pid as protein_id,\n",
    "                    p.id as peptide_id,\n",
    "                    p.peptide,\n",
    "                    pi.desc as protein_description,\n",
    "                    pi.species,\n",
    "                    p.intervals,\n",
    "                    f.function,\n",
    "                    r.additional_details,\n",
    "                    r.ic50,\n",
    "                    r.inhibition_type,\n",
    "                    r.inhibited_microorganisms,\n",
    "                    r.ptm,\n",
    "                    r.title,\n",
    "                    r.authors,\n",
    "                    r.abstract,\n",
    "                    r.doi,\n",
    "                    'sequence' as search_type,\n",
    "                    'IDENTITY' as scoring_matrix\n",
    "                FROM peptide_peptideinfo p\n",
    "                JOIN peptide_proteininfo pi ON p.protein_id = pi.id\n",
    "                LEFT JOIN peptide_function f ON f.pep_id = p.id\n",
    "                LEFT JOIN peptide_reference r ON r.func_id = f.id\n",
    "                WHERE p.peptide = ?\n",
    "                \"\"\"\n",
    "                df = pd.read_sql_query(query, conn, params=[peptide, peptide])\n",
    "                results.append(df)\n",
    "            else:\n",
    "                # Run BLASTP search for similarity matching\n",
    "                query_path = os.path.join(work_path, \"query.fasta\")\n",
    "                with open(query_path, \"w\") as query_file:\n",
    "                    query_file.write(f\">pep_query\\n{peptide}\\n\")\n",
    "                    \n",
    "                output_path = os.path.join(work_path, \"blastp_short.out\")\n",
    "                blast_args = [\n",
    "                    \"blastp\",\n",
    "                    \"-query\", query_path,\n",
    "                    \"-db\", fasta_db_path,\n",
    "                    \"-outfmt\", \"6 std ppos qcovs qlen slen positive\",\n",
    "                    \"-evalue\", \"1000\",\n",
    "                    \"-word_size\", \"2\",\n",
    "                    \"-matrix\", \"IDENTITY\",\n",
    "                    \"-threshold\", \"1\",\n",
    "                    \"-task\", \"blastp-short\",\n",
    "                    \"-out\", output_path\n",
    "                ]\n",
    "                \n",
    "                subprocess.check_output(blast_args, stderr=subprocess.STDOUT)\n",
    "                \n",
    "                # Process BLAST results\n",
    "                search_ids = self._process_blast_results(output_path, similarity_threshold, extra_info)\n",
    "                \n",
    "                if search_ids:\n",
    "                    df = self._fetch_peptide_data(conn, peptide, search_ids)\n",
    "                    self._add_blast_details(df, extra_info)\n",
    "                    results.append(df)\n",
    "        \n",
    "        conn.close()\n",
    "        self._cleanup_work_directory(WORK_DIRECTORY)\n",
    "        \n",
    "        return self._combine_results(results)\n",
    "    \n",
    "    def _create_work_directory(self, base_dir):\n",
    "        \"\"\"Create a working directory for BLAST operations\"\"\"\n",
    "        path = os.path.join(base_dir, f'work_{int(round(time.time() * 1000))}')\n",
    "        os.makedirs(path)\n",
    "        return path\n",
    "    \n",
    "    def _make_blast_db(self, library_fasta_path):\n",
    "        \"\"\"Create BLAST database from FASTA file\"\"\"\n",
    "        subprocess.check_output(\n",
    "            ['makeblastdb', '-in', library_fasta_path, '-dbtype', 'prot'],\n",
    "            stderr=subprocess.STDOUT\n",
    "        )\n",
    "    \n",
    "    def _process_blast_results(self, output_path, similarity_threshold, extra_info):\n",
    "        \"\"\"Process BLAST results and collect search IDs\"\"\"\n",
    "        search_ids = []\n",
    "        csv.register_dialect('blast_dialect', delimiter='\\t')\n",
    "        \n",
    "        with open(output_path, \"r\") as output_file:\n",
    "            blast_data = csv.DictReader(\n",
    "                output_file,\n",
    "                fieldnames=['query', 'subject', 'percid', 'align_len', 'mismatches', \n",
    "                           'gaps', 'qstart', 'qend', 'sstart', 'send', 'evalue', \n",
    "                           'bitscore', 'ppos', 'qcov', 'qlen', 'slen', 'numpos'],\n",
    "                dialect='blast_dialect'\n",
    "            )\n",
    "            \n",
    "            for row in blast_data:\n",
    "                tlen = float(row['slen']) if float(row['slen']) > float(row['qlen']) else float(row['qlen'])\n",
    "                simcalc = 100 * ((float(row['numpos']) - float(row['gaps'])) / tlen)\n",
    "                \n",
    "                if simcalc >= similarity_threshold:\n",
    "                    search_ids.append(row['subject'])\n",
    "                    extra_info[row['subject']] = [\n",
    "                        f\"{simcalc:.2f}\", row['qstart'], row['qend'], row['sstart'],\n",
    "                        row['send'], row['evalue'], row['align_len'], row['mismatches'],\n",
    "                        row['gaps']\n",
    "                    ]\n",
    "        \n",
    "        return search_ids\n",
    "    \n",
    "    def _fetch_peptide_data(self, conn, peptide, search_ids):\n",
    "        \"\"\"Fetch peptide data from database\"\"\"\n",
    "        placeholders = ','.join(['?' for _ in search_ids])\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            ? as search_peptide,\n",
    "            pi.pid as protein_id,\n",
    "            p.id as peptide_id,\n",
    "            p.peptide,\n",
    "            pi.desc as protein_description,\n",
    "            pi.species,\n",
    "            p.intervals,\n",
    "            f.function,\n",
    "            r.additional_details,\n",
    "            r.ic50,\n",
    "            r.inhibition_type,\n",
    "            r.inhibited_microorganisms,\n",
    "            r.ptm,\n",
    "            r.title,\n",
    "            r.authors,\n",
    "            r.abstract,\n",
    "            r.doi,\n",
    "            'sequence' as search_type,\n",
    "            'IDENTITY' as scoring_matrix\n",
    "        FROM peptide_peptideinfo p\n",
    "        JOIN peptide_proteininfo pi ON p.protein_id = pi.id\n",
    "        LEFT JOIN peptide_function f ON f.pep_id = p.id\n",
    "        LEFT JOIN peptide_reference r ON r.func_id = f.id\n",
    "        WHERE p.id IN ({placeholders})\n",
    "        \"\"\"\n",
    "        \n",
    "        return pd.read_sql_query(query, conn, params=[peptide] + search_ids)\n",
    "    \n",
    "    def _add_blast_details(self, df, extra_info):\n",
    "        \"\"\"Add BLAST details to DataFrame\"\"\"\n",
    "        for idx, row in df.iterrows():\n",
    "            if str(row['peptide_id']) in extra_info:\n",
    "                blast_details = extra_info[str(row['peptide_id'])]\n",
    "                df.at[idx, '% Alignment'] = blast_details[0]\n",
    "                df.at[idx, 'Query start'] = blast_details[1]\n",
    "                df.at[idx, 'Query end'] = blast_details[2]\n",
    "                df.at[idx, 'Subject start'] = blast_details[3]\n",
    "                df.at[idx, 'Subject end'] = blast_details[4]\n",
    "                df.at[idx, 'e-value'] = blast_details[5]\n",
    "                df.at[idx, 'Alignment length'] = blast_details[6]\n",
    "                df.at[idx, 'Mismatches'] = blast_details[7]\n",
    "                df.at[idx, 'Gap opens'] = blast_details[8]\n",
    "    \n",
    "    def _cleanup_work_directory(self, work_directory):\n",
    "        \"\"\"Clean up old work directories\"\"\"\n",
    "        try:\n",
    "            dirs = [f for f in os.scandir(work_directory) if f.is_dir()]\n",
    "            dirs.sort(key=lambda x: os.path.getmtime(x.path), reverse=True)\n",
    "            \n",
    "            for dir_entry in dirs[25:]:\n",
    "                try:\n",
    "                    shutil.rmtree(dir_entry.path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    def _combine_results(self, results):\n",
    "        \"\"\"Combine and format final results\"\"\"\n",
    "        if not results:\n",
    "            return pd.DataFrame(columns=[\n",
    "                'search_peptide', 'protein_id', 'peptide', 'protein_description',\n",
    "                'species', 'intervals', 'function', 'additional_details', 'ic50',\n",
    "                'inhibition_type', 'inhibited_microorganisms', 'ptm', 'title',\n",
    "                'authors', 'abstract', 'doi', 'search_type', 'scoring_matrix'\n",
    "            ])\n",
    "        \n",
    "        final_results = pd.concat(results, ignore_index=True)\n",
    "        \n",
    "        if 'peptide_id' in final_results.columns:\n",
    "            final_results = final_results.drop('peptide_id', axis=1)\n",
    "            \n",
    "        sort_columns = ['search_peptide']\n",
    "        if '% Alignment' in final_results.columns:\n",
    "            sort_columns.append('% Alignment')\n",
    "            \n",
    "        return final_results.sort_values(\n",
    "            sort_columns,\n",
    "            ascending=[True] + [False] * (len(sort_columns) - 1)\n",
    "        )\n",
    "    \n",
    "    def _format_search_results_with_matches(self, final_results):\n",
    "        \"\"\"Format search results with matches\"\"\"\n",
    "        if '% Alignment' in final_results.columns:\n",
    "            final_results['% Alignment'] = pd.to_numeric(\n",
    "                final_results['% Alignment'], \n",
    "                errors='coerce'\n",
    "            )\n",
    "\n",
    "        grouped = final_results.groupby([\"search_peptide\", \"function\"], as_index=False)\n",
    "        aggregated_results = []\n",
    "        processed_indices = set()\n",
    "\n",
    "        for _, group in grouped:\n",
    "            if len(group) > 1:\n",
    "                aggregated_row = self._aggregate_group_data(group)\n",
    "                aggregated_results.append(aggregated_row)\n",
    "                processed_indices.update(group.index)\n",
    "\n",
    "        remaining_rows = final_results.loc[~final_results.index.isin(processed_indices)]\n",
    "        aggregated_df = pd.DataFrame(aggregated_results)\n",
    "        \n",
    "        return pd.concat([aggregated_df, remaining_rows], ignore_index=True)\n",
    "    \n",
    "    def _aggregate_group_data(self, group):\n",
    "        \"\"\"Aggregate data for a group of results\"\"\"\n",
    "        def enumerate_field(field):\n",
    "            if field in group.columns and not group[field].dropna().empty:\n",
    "                valid_values = set(group[field].dropna().astype(str).str.strip())\n",
    "                valid_values = {val for val in valid_values if val != ''}\n",
    "                if len(valid_values) > 1:\n",
    "                    return \"; \".join([f\"{i+1}) {val}\" for i, val in enumerate(valid_values)])\n",
    "                elif len(valid_values) == 1:\n",
    "                    return next(iter(valid_values))\n",
    "                return ''\n",
    "            return ''\n",
    "\n",
    "        return {col: enumerate_field(col) for col in group.columns}   \n",
    "            \n",
    "    def setup_data_loading_ui(self):\n",
    "        \"\"\"Initialize and display the data loading UI with integrated search\"\"\"\n",
    "        \n",
    "        # Create file upload widgets\n",
    "        self.mbpdb_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload MBPDB File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.pd_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload Peptidomic File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.fasta_uploader = widgets.FileUpload(\n",
    "            accept='.fasta',\n",
    "            multiple=True,\n",
    "            description='Upload FASTA Files',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Create search interface\n",
    "        self.threshold_dropdown = widgets.Dropdown(\n",
    "            options=list(range(0, 101, 10)),\n",
    "            value=80,\n",
    "            description='Similarity Threshold (%):',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='225px')\n",
    "        )\n",
    "        \n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search Database',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        \n",
    "        # Reset button\n",
    "        self.reset_button = widgets.Button(\n",
    "            description='Reset',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='100px')\n",
    "        )\n",
    "        \n",
    "        # Create output areas\n",
    "        self.output_area = widgets.Output()\n",
    "        self.search_output_area = widgets.Output()\n",
    "        \n",
    "        # Create main container\n",
    "        main_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Upload Peptidomic Data Files:</u></h3>\"),\n",
    "            widgets.HTML(\"<p>Upload peptide groups data from Proteomoe Discover export file (required):</p>\"),\n",
    "\n",
    "            self.pd_uploader,\n",
    "            widgets.HTML(\"<h3><u>Upload / Search functional Peptide Data :</u></h3>\"),\n",
    "            widgets.HTML(\"<p>Upload MBPDB file (optional - if not provided, will search database):</p>\"),\n",
    "            self.mbpdb_uploader,\n",
    "            widgets.HTML(\"<p>Search Petides from Peptodmic Data against the  MBPDB (optional - search may take several minutes):</p>\"),\n",
    "            widgets.HBox([\n",
    "                self.threshold_dropdown,\n",
    "                self.search_button\n",
    "            ], layout=widgets.Layout(align_items='center')),\n",
    "            widgets.HTML(\"<h3><u>Upload Protein FASTA Files (Optional):</u></h3>\"),\n",
    "            widgets.HTML(\"<p>Upload Protein FASTA file used in Proteome Discovere Search (optional - This helps label proteins in data transformation):</p>\"),\n",
    "            self.fasta_uploader,\n",
    "            widgets.HTML(\"<br>\"),\n",
    "            widgets.HTML(\"<p>Reset all uploaded files:</p>\"),\n",
    "            self.reset_button,\n",
    "            widgets.HTML(\"<div style='margin-top: 10px;'></div>\"),  # Spacing\n",
    "            self.output_area,\n",
    "            self.search_output_area\n",
    "        ])\n",
    "        \n",
    "        # Register observers\n",
    "        self.pd_uploader.observe(self._on_pd_upload_change, names='value')\n",
    "        self.mbpdb_uploader.observe(self._on_mbpdb_upload_change, names='value')\n",
    "        self.fasta_uploader.observe(self._on_fasta_upload_change, names='value')\n",
    "        self.reset_button.on_click(self._reset_ui)\n",
    "        self.search_button.on_click(self._on_search_click)\n",
    "        \n",
    "        display(main_container)\n",
    "    \n",
    "    def _extract_sequences(self, df):\n",
    "        \"\"\"Extract sequences from peptidomic data\"\"\"\n",
    "        if 'Sequence' not in df.columns:\n",
    "            if 'Annotated Sequence' in df.columns:\n",
    "                sequences = df['Annotated Sequence'].str.split('.', expand=False).str[1]\n",
    "                df = df.assign(Sequence=sequences)\n",
    "            elif 'Positions in Proteins' in df.columns:  # Add any other potential column names\n",
    "                df['Sequence'] = df['Positions in Proteins']\n",
    "        return df['Sequence'].dropna().unique().tolist()\n",
    "    \n",
    "    def _on_search_click(self, b):\n",
    "        \"\"\"Handle search button click\"\"\"\n",
    "        with self.search_output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            if self.pd_results is None or self.pd_results.empty:\n",
    "                display(HTML(\"<b style='color:red'>Please upload peptidomic data first.</b>\"))\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                # Extract sequences from peptidomic data\n",
    "                peptides = self._extract_sequences(self.pd_results)\n",
    "                \n",
    "                if not peptides:\n",
    "                    display(HTML(\"<b style='color:red'>No valid sequences found in peptidomic data.</b>\"))\n",
    "                    return\n",
    "                    \n",
    "                display(HTML(f\"<b style='color:blue'>Found {len(peptides)} sequences. Searching database...</b>\"))\n",
    "                \n",
    "                # Perform search\n",
    "                results = self._search_peptides_comprehensive(\n",
    "                    peptides, \n",
    "                    similarity_threshold=self.threshold_dropdown.value\n",
    "                )\n",
    "                \n",
    "                # Format results if we have any matches\n",
    "                if not results.empty:\n",
    "                    self.mbpdb_results = self._format_search_results_with_matches(results)\n",
    "                    display(HTML(f\"<b style='color:green'>Search complete! Found {len(self.mbpdb_results)} matches</b>\"))\n",
    "                else:\n",
    "                    self.mbpdb_results = results\n",
    "                    display(HTML(\"<b style='color:orange'>No matches found in the database.</b>\"))\n",
    "                \n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red'>Error: {str(e)}</b>\"))\n",
    "                self.mbpdb_results = pd.DataFrame()\n",
    "    \n",
    "    def _reset_ui(self, b):\n",
    "        \"\"\"Reset the UI state\"\"\"\n",
    "        self.mbpdb_uploader._counter = 0\n",
    "        self.pd_uploader._counter = 0\n",
    "        self.fasta_uploader._counter = 0\n",
    "        self.mbpdb_uploader.value = ()\n",
    "        self.pd_uploader.value = ()\n",
    "        self.fasta_uploader.value = ()\n",
    "        self.pd_results = pd.DataFrame()\n",
    "        self.mbpdb_results = pd.DataFrame()\n",
    "        self.search_results = pd.DataFrame()\n",
    "        self.proteins_dic = {}\n",
    "        \n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "            display(HTML('<b style=\"color:blue;\">All uploads cleared.</b>'))\n",
    "        \n",
    "        with self.search_output_area:\n",
    "            clear_output()\n",
    "            display(HTML('<b style=\"color:blue;\">Search results cleared.</b>')) \n",
    "            \n",
    "    def _on_pd_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    self.pd_results, pd_status = self._load_data(\n",
    "                        file_data,\n",
    "                        required_columns=['Positions in Proteins'],\n",
    "                        file_type='Peptidomic'\n",
    "                    )\n",
    "                    if pd_status == 'yes' and self.pd_results is not None:\n",
    "                        display(HTML(f'<b style=\"color:green;\">Peptidomic data imported with {self.pd_results.shape[0]} rows and {self.pd_results.shape[1]} columns.</b>'))\n",
    "\n",
    "    def _on_mbpdb_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    self.mbpdb_results, mbpdb_status = self._load_data(\n",
    "                        file_data,\n",
    "                        required_columns=['Search peptide', 'Protein ID', 'Peptide'],\n",
    "                        file_type='MBPDB'\n",
    "                    )\n",
    "                    if mbpdb_status == 'yes' and self.mbpdb_results is not None:\n",
    "                        self.mbpdb_results.rename(columns={\n",
    "                            'Search peptide': 'search_peptide',\n",
    "                            'Protein ID': 'protein_id',\n",
    "                            'Peptide': 'peptide',\n",
    "                            'Protein description': 'protein_description',\n",
    "                            'Species': 'species',\n",
    "                            'Intervals': 'intervals',\n",
    "                            'Function': 'function',\n",
    "                            'Additional details': 'additional_details',\n",
    "                            'IC50 (Î¼M)': 'ic50',\n",
    "                            'Inhibition type': 'inhibition_type',\n",
    "                            'Inhibited microorganisms': 'inhibited_microorganisms',\n",
    "                            'PTM': 'ptm',\n",
    "                            'Title': 'title',\n",
    "                            'Authors': 'authors',\n",
    "                            'Abstract': 'abstract',\n",
    "                            'DOI': 'doi',\n",
    "                            'Search type': 'search_type',\n",
    "                            'Scoring matrix': 'scoring_matrix',\n",
    "                            }, inplace=True)\n",
    "                        display(HTML(f'<b style=\"color:green;\">MBPDB file imported with {self.mbpdb_results.shape[0]} rows and {self.mbpdb_results.shape[1]} columns</b>'))\n",
    "\n",
    "    def _on_fasta_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    for file_data in change['new']:\n",
    "                        try:\n",
    "                            file_name = getattr(file_data, 'name', None)\n",
    "                            if file_name and file_name.endswith('.fasta'):\n",
    "                                new_proteins = self._parse_uploaded_fasta(file_data)\n",
    "                                self.proteins_dic.update(new_proteins)\n",
    "                                display(HTML(f'<b style=\"color:green;\">Successfully imported FASTA file: {file_name} ({len(new_proteins)} proteins)</b>'))\n",
    "                            else:\n",
    "                                display(HTML(f'<b style=\"color:red;\">Invalid file format. Please upload FASTA files only.</b>'))\n",
    "                        except Exception as e:\n",
    "                            display(HTML(f'<b style=\"color:red;\">Error processing FASTA file: {str(e)}</b>'))\n",
    "    \n",
    "    def _load_data(self, file_obj, required_columns, file_type):\n",
    "        \"\"\"\n",
    "        Load and validate uploaded data files, cleaning empty rows and validating data.\n",
    "        \n",
    "        Args:\n",
    "            file_obj: Uploaded file object\n",
    "            required_columns (list): List of required column names\n",
    "            file_type (str): Type of file being loaded ('MBPDB' or 'Peptidomic')\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (DataFrame or None, status string 'yes'/'no')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            content = file_obj.content\n",
    "            filename = file_obj.name\n",
    "            extension = filename.split('.')[-1].lower()\n",
    "            \n",
    "            file_stream = io.BytesIO(content)\n",
    "            \n",
    "            # Load data based on file extension\n",
    "            if extension == 'csv':\n",
    "                df = pd.read_csv(file_stream)\n",
    "            elif extension in ['txt', 'tsv']:\n",
    "                df = pd.read_csv(file_stream, delimiter='\\t')\n",
    "            elif extension == 'xlsx':\n",
    "                df = pd.read_excel(file_stream)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Please upload .csv, .txt, .tsv, or .xlsx files.\")\n",
    "            \n",
    "            # Clean column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Drop empty rows\n",
    "            df = df.dropna(how='all')  # Drop rows where all values are NaN\n",
    "            df = df[~(df.astype(str).apply(lambda x: x.str.strip().eq('')).all(axis=1))]  # Drop rows where all values are empty strings\n",
    "            \n",
    "            # Replace empty strings with NaN for consistency            \n",
    "            # Validate required columns\n",
    "            if not set(required_columns).issubset(df.columns):\n",
    "                missing = set(required_columns) - set(df.columns)\n",
    "                display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Missing required columns: {\", \".join(missing)}</b>'))\n",
    "                return None, 'no'\n",
    "            \n",
    "            # Validate non-empty required columns\n",
    "            empty_required = []\n",
    "            for col in required_columns:\n",
    "                if df[col].isna().all() or (df[col].astype(str).str.strip() == '').all():\n",
    "                    empty_required.append(col)\n",
    "            \n",
    "            if empty_required:\n",
    "                display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Required columns are empty: {\", \".join(empty_required)}</b>'))\n",
    "                return None, 'no'\n",
    "            \n",
    "            # Show success message with row count\n",
    "            display(HTML(f'<b style=\"color:green;\">{file_type} file loaded successfully with {len(df)} rows after cleaning.</b>'))\n",
    "            \n",
    "            return df, 'yes'\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(HTML(f'<b style=\"color:red;\">{file_type} File Error: {str(e)}</b>'))\n",
    "            return None, 'no'\n",
    "    def _parse_uploaded_fasta(self, file_data):\n",
    "        \"\"\"Parse uploaded FASTA file content\"\"\"\n",
    "        fasta_dict = {}\n",
    "        fasta_text = bytes(file_data.content).decode('utf-8')\n",
    "        lines = fasta_text.split('\\n')\n",
    "        \n",
    "        protein_id = \"\"\n",
    "        protein_name = \"\"\n",
    "        sequence = \"\"\n",
    "        species = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if protein_id:\n",
    "                    fasta_dict[protein_id] = {\n",
    "                        \"name\": protein_name,\n",
    "                        \"sequence\": sequence,\n",
    "                        \"species\": species\n",
    "                    }\n",
    "                sequence = \"\"\n",
    "                header_parts = line[1:].split('|')\n",
    "                if len(header_parts) > 2:\n",
    "                    protein_id = header_parts[1]\n",
    "                    protein_name_full = re.split(r' OS=', header_parts[2])[0]\n",
    "                    if ' ' in protein_name_full:\n",
    "                        protein_name = protein_name_full\n",
    "                    else:\n",
    "                        protein_name = protein_name_full\n",
    "                    species = self._find_species(line)\n",
    "            else:\n",
    "                sequence += line\n",
    "                \n",
    "        if protein_id:\n",
    "            fasta_dict[protein_id] = {\n",
    "                \"name\": protein_name,\n",
    "                \"sequence\": sequence,\n",
    "                \"species\": species\n",
    "            }\n",
    "        \n",
    "        return fasta_dict\n",
    "\n",
    "    def _find_species(self, header):\n",
    "        \"\"\"Find species in FASTA header\"\"\"\n",
    "        header_lower = header.lower()\n",
    "        for spec_group in spec_translate_list:\n",
    "            for term in spec_group[1:]:\n",
    "                if term.lower() in header_lower:\n",
    "                    return spec_group[0]\n",
    "        return \"unknown\"\n",
    "    \n",
    "    def process_protein_combinations(self):\n",
    "        \"\"\"Process protein combinations in pd_results\"\"\"\n",
    "        if not self.pd_results.empty:\n",
    "            df = self.pd_results.copy()\n",
    "            \n",
    "            # Create main grid container\n",
    "            grid = widgets.GridspecLayout(1, 2,  # Number of rows and columns\n",
    "                width='1000px', \n",
    "                grid_gap='5px',  # Adjust spacing between grid elements\n",
    "            )\n",
    "            \n",
    "            # Create input and output areas\n",
    "            input_area = widgets.VBox([\n",
    "                widgets.HTML(\"<h3>Peptides Mapped to Multiple Proteins</h3>\"),\n",
    "                widgets.HTML(\"Peptides that have been identified and <b>mapped to multiple proteins</b> and the '<b>Master Protein Accessions</b>' and '<b>Positions in Proteins</b>' columns have multiple entries for a single peptide require special attention.\")\n",
    "            ], layout=widgets.Layout(width='100%'))\n",
    "            \n",
    "            self.protein_output_area = widgets.Output(\n",
    "                #layout=widgets.Layout(width='90%')\n",
    "            )\n",
    "            \n",
    "            # Create split container for input and output\n",
    "            \"\"\"split_container = widgets.VBox([\n",
    "                input_area,\n",
    "                self.protein_output_area\n",
    "            ])\"\"\"\n",
    "            \n",
    "            # Add to grid\n",
    "            grid[0, 0] = input_area\n",
    "            grid[0, 1] = self.protein_output_area\n",
    "    \n",
    "            # Count peptides with multiple protein accessions\n",
    "            num_multiple_entries = len(self.pd_results[self.pd_results['Master Protein Accessions'].str.contains(';')])\n",
    "            input_area.children += (widgets.HTML(f\"In your dataset, you have <b>{num_multiple_entries}</b> peptides mapped to multiple Master Protein Accessions.\"),)\n",
    "            \n",
    "            unique_proteins = self.pd_results['Master Protein Accessions'].dropna().unique()\n",
    "            self.multi_protein_combinations = [up for up in unique_proteins if ';' in up]\n",
    "            \n",
    "            # Instructions for user actions\n",
    "            html_content = \"\"\"\n",
    "            <h3>Options</h3>\n",
    "            For each protein combination with multiple entries, you have two options:<br>\n",
    "            1. <b>'new'</b> - Create a new row for each protein listed in the 'Master Protein Accessions' column and their corresponding 'Positions in Proteins'.<br>\n",
    "            2. <b>Enter a Protein ID</b> - Replace the current protein combination with a custom Protein ID of your choice, updating 'Positions in Proteins' accordingly.\n",
    "            \"\"\"\n",
    "            input_area.children += (widgets.HTML(html_content),)\n",
    "            \n",
    "            self.user_decisions = {}\n",
    "            self.decision_inputs = []\n",
    "            \n",
    "            # Create input fields\n",
    "            for combo in self.multi_protein_combinations:\n",
    "                named_combo = self.fetch_protein_names(combo)\n",
    "                occurrences = self.pd_results[self.pd_results['Master Protein Accessions'].str.contains(combo, regex=False)].shape[0]\n",
    "                \n",
    "                combo_container = widgets.VBox([\n",
    "                    widgets.HTML(f\"<b>{occurrences}</b> occurrences of<br><b>{named_combo}</b>.\"),\n",
    "                    widgets.Text(\n",
    "                        placeholder=\"Enter 'new', or a custom Protein ID\",\n",
    "                        description='Decision:',\n",
    "                        layout=widgets.Layout(width='300px')\n",
    "                    )\n",
    "                ])\n",
    "                self.decision_inputs.append(combo_container.children[-1])\n",
    "                input_area.children += (combo_container,)\n",
    "            \n",
    "            # Create buttons\n",
    "            submit_button = widgets.Button(description=\"Submit\", button_style='success')\n",
    "            reset_button = widgets.Button(description=\"Reset Selection\", button_style='warning')\n",
    "            button_box_protein = widgets.HBox([submit_button, reset_button])\n",
    "            input_area.children += (button_box_protein,)\n",
    "            \n",
    "            # Register button callbacks\n",
    "            reset_button.on_click(self.on_reset_button_clicked)\n",
    "            submit_button.on_click(lambda b: self.on_submit(b, df))\n",
    "            self.pd_results_cleaned = df\n",
    "            display(grid)\n",
    "            return df\n",
    "        \n",
    "    def on_submit(self, button, df):\n",
    "        \"\"\"Handle submit button click for protein combinations\"\"\"\n",
    "        with self.protein_output_area:\n",
    "            self.protein_output_area.clear_output()\n",
    "            for combo, decision_input in zip(self.multi_protein_combinations, self.decision_inputs):\n",
    "                self.user_decisions[combo] = decision_input.value.strip().upper()\n",
    "            # Iterate over each row in the DataFrame\n",
    "            for index, row in df.iterrows():\n",
    "                proteins_row = row['Master Protein Accessions']\n",
    "                positions_row = row['Positions in Proteins']\n",
    "                if proteins_row in self.user_decisions:\n",
    "                    decision = self.user_decisions[proteins_row]\n",
    "                    # Split accessions and positions\n",
    "                    accessions = proteins_row.split('; ')\n",
    "                    positions = positions_row.split('; ')\n",
    "                    # Create a dictionary to map each accession to its corresponding position\n",
    "                    accession_position_map = {}\n",
    "                    for acc in accessions:\n",
    "                        for pos in positions:\n",
    "                            if acc in pos:\n",
    "                                accession_position_map[acc] = pos\n",
    "                                positions.remove(pos)\n",
    "                                break\n",
    "                    acc_pos_pairs = list(accession_position_map.items())\n",
    "            \n",
    "                    if decision == 'NEW':\n",
    "                        # Update the current row\n",
    "                        df.at[index, 'Master Protein Accessions'] = acc_pos_pairs[0][0]\n",
    "                        df.at[index, 'Positions in Proteins'] = acc_pos_pairs[0][1]\n",
    "                        \n",
    "                        # Create new rows for each additional accession and position\n",
    "                        for acc, pos in acc_pos_pairs[1:]:\n",
    "                            new_row = row.copy()\n",
    "                            new_row['Master Protein Accessions'] = acc\n",
    "                            new_row['Positions in Proteins'] = pos\n",
    "                            df.loc[len(df)] = new_row\n",
    "                     \n",
    "                    else:\n",
    "                        new_accession = decision\n",
    "                        new_positions = []\n",
    "                        for pos in positions_row.split('; '):\n",
    "                            num_range = pos[pos.index('['):] if '[' in pos else ''\n",
    "                            new_positions.append(f\"{new_accession} {num_range}\")\n",
    "                        df.at[index, 'Master Protein Accessions'] = new_accession\n",
    "                        df.at[index, 'Positions in Proteins'] = '; '.join(new_positions)\n",
    "    \n",
    "            # Display output\n",
    "            for combo, decision in self.user_decisions.items():\n",
    "                if decision == 'NEW':\n",
    "                    display(HTML(f'<b>{combo}</b> <b style=\"color:green;\">has been successfully processed.</b>'))\n",
    "                    display(HTML('&nbsp;&nbsp;&nbsp;&nbsp;Shared occurrences of the peptide have been separated, with each now assigned a unique protein ID in a new row.'))\n",
    "                else:\n",
    "                    display(HTML(f'<b>{combo}</b> <b style=\"color:green;\">has been successfully processed.</b>'))\n",
    "                    display(HTML(f'&nbsp;&nbsp;&nbsp;&nbsp;The occurrences of the peptide with the shared combined protein ID \"{combo}\" have been replaced with \"{decision}\".'))\n",
    "        return df\n",
    "    \n",
    "    def on_reset_button_clicked(self, b):\n",
    "        \"\"\"Handle reset button click for protein combinations\"\"\"\n",
    "        with self.protein_output_area:\n",
    "            self.protein_output_area.clear_output()\n",
    "            display(HTML('<span style=\"color:red;\">To reset \"Mapped to Multiple Proteins\" selection after hitting the submit button, <b>rerun the cell</b> and make the correct selections. This button <b>only</b> displays instructions</span>'))\n",
    "        \n",
    "    def fetch_protein_names(self, accession_str):\n",
    "        \"\"\"Fetch protein names from accession string\"\"\"\n",
    "        names = []\n",
    "        for acc in accession_str.split('; '):\n",
    "            if acc in self.proteins_dic:\n",
    "                names.append(f\"{acc}<span style='color:blue'> ({self.proteins_dic[acc]['species']} - {self.proteins_dic[acc]['name']})</span>\")\n",
    "            else:\n",
    "                names.append(acc)\n",
    "        return '<br>'.join(names)\n",
    "    def handle_protein_combinations(self):\n",
    "        \"\"\"\n",
    "        Simple prompt for user to decide whether to process protein combinations.\n",
    "        \"\"\"\n",
    "        display(HTML(\"<h3>Multiple Protein Mappings</h3>\"))\n",
    "                \n",
    "        choice = widgets.RadioButtons(\n",
    "            options=[('Yes', True), ('No', False)],\n",
    "            description='Process peptides mapped to multiple proteins?',\n",
    "            style={'description_width': 'initial'},\n",
    "            value=None  # This makes it start unchecked\n",
    "        )\n",
    "        output = widgets.Output()\n",
    "        \n",
    "        def process_choice(_):\n",
    "            with output:\n",
    "                clear_output()\n",
    "                if choice.value:\n",
    "                    self.pd_results_cleaned = data_transformer.process_protein_combinations()\n",
    "                    display(HTML(\"<b style='color:green;'>Processed peptides mapped to multiple proteins.</b>\"))\n",
    "                else:\n",
    "                    self.pd_results_cleaned = self.pd_results.copy()\n",
    "                    display(HTML(\"<b>Using original protein mappings.</b>\"))\n",
    "        \n",
    "        choice.observe(process_choice, 'value')\n",
    "        display(choice)\n",
    "        display(output)\n",
    "\n",
    "    # Then to use it, we can create an observe function:\n",
    "    def observe_data_changes(change):\n",
    "        if hasattr(change, 'new'):\n",
    "            combiner.update_data(data_transformer.pd_results, data_transformer.mbpdb_results)\n",
    "            setup_data.update_data(data_transformer.pd_results, data_transformer.pd_results_cleaned)\n",
    "    \n",
    "        \n",
    "    \n",
    "    # Add this to DataTransformation class:\n",
    "    def attach_observers(self, group_processor):\n",
    "        \"\"\"\n",
    "        Attach observers to monitor changes in pd_results and pd_results_cleaned\n",
    "        \n",
    "        Args:\n",
    "            group_processor: Instance of GroupProcessing class\n",
    "        \"\"\"\n",
    "        def observe_data_changes(change):\n",
    "            if change.name in ['pd_results', 'pd_results_cleaned']:\n",
    "                group_processor.update_data(self.pd_results, self.pd_results_cleaned)\n",
    "        \n",
    "        self.observe(observe_data_changes, names=['pd_results', 'pd_results_cleaned'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b05c11-89ca-4ea5-8b1f-7a2a62d99f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58fddd1e7364e5ca514cc304c8a2c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3><u>Upload Peptidomic Data Files:</u></h3>'), HTML(value='<p>Upload peptide grouâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 1: Create the instance and setup UI\n",
    "data_transformer = DataTransformation()\n",
    "data_transformer.setup_data_loading_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4399eaa2-2458-4b6c-bba2-a7b98701d93d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Handles Peptides Matched to Multiple Proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b615d4b-664c-4726-9027-1b627f07058d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Multiple Protein Mappings</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45491d05921c4d53ab00c9de181eb156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Process peptides mapped to multiple proteins?', options=(('Yes', True), ('No', Falseâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3211e913777e4816a76bce503a73ed0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Then call the handle_protein_combinations method\n",
    "data_transformer.handle_protein_combinations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb8b293-25d8-415d-a596-67139932cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from ipydatagrid import DataGrid\n",
    "\n",
    "if data_transformer.pd_results is not None:\n",
    "    grid = DataGrid(\n",
    "        data_transformer.pd_results,\n",
    "        selection_mode='cell',\n",
    "        grid_style={'gridStroke': '#ddd'},\n",
    "        base_row_size=25,\n",
    "        base_column_size=100,\n",
    "        auto_fit_columns=True,\n",
    "        layout={'height': '300px', 'width': 'auto'}\n",
    "    )\n",
    "    display(grid)\n",
    "else:\n",
    "    print(\"No peptidomic data loaded yet\")\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16192af0",
   "metadata": {},
   "source": [
    "## Group data by Catagorical Varriables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e0ca14a-1665-43ec-abde-8adb7fc5c835",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GroupProcessing:\n",
    "    def __init__(self):\n",
    "        self.group_data = {}\n",
    "        self.group_number = 1\n",
    "        self.filtered_columns = []\n",
    "        self.group_uploader = widgets.FileUpload(\n",
    "        accept='.json',\n",
    "        multiple=False,\n",
    "        description='Upload Groups File',\n",
    "        layout=widgets.Layout(width='300px'),\n",
    "        style={'description_width': 'initial'}\n",
    "        )\n",
    "        self.group_uploader.observe(self._on_group_upload_change, names='value')\n",
    "        \n",
    "        # Initialize output areas\n",
    "        self.output = widgets.Output()\n",
    "        self.gd_output_area = widgets.Output()\n",
    "        \n",
    "        # Initialize widgets for group selection\n",
    "        self.column_dropdown = widgets.SelectMultiple(\n",
    "            description='Absorbance',\n",
    "            style={'description_width': 'initial'},\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width='90%', height='300px')\n",
    "        )\n",
    "        \n",
    "        self.grouping_variable_text = widgets.Text(\n",
    "            description='Group Name',\n",
    "            layout=widgets.Layout(width='90%'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Initialize buttons\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 0')\n",
    "        )\n",
    "        \n",
    "        self.add_group_button = widgets.Button(\n",
    "            description='Add Group',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 0')\n",
    "        )\n",
    "        \n",
    "        self.reset_file_button = widgets.Button(\n",
    "            description='Reset Selection',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 75px')\n",
    "        )\n",
    "        \n",
    "        # Set up button callbacks\n",
    "        self.search_button.on_click(self._search_columns)\n",
    "        self.add_group_button.on_click(self._add_group)\n",
    "        self.reset_file_button.on_click(self._reset_selection)\n",
    "        \n",
    "\n",
    "    def update_data(self, pd_results, pd_results_cleaned):\n",
    "        \"\"\"Update data and refresh filtered columns\"\"\"\n",
    "        self.pd_results = pd_results\n",
    "        self.pd_results_cleaned = pd_results_cleaned\n",
    "        \n",
    "        # Only update if we have valid data\n",
    "        if pd_results is not None or pd_results_cleaned is not None:\n",
    "            self.setup_data()\n",
    "            \n",
    "            # Update the dropdown with new filtered columns\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:green;\">Data updated successfully. Column selection refreshed.</b>'))\n",
    "    \n",
    "            \n",
    "    def setup_data(self):\n",
    "        \"\"\"Initialize data and filters for the analysis\"\"\"\n",
    "        # Define columns to exclude with more flexible matching\n",
    "        columns_to_exclude = [\n",
    "            'Marked as', 'Number of Missed Cleavages', 'Missed Cleavages',\n",
    "            'Checked', 'Confidence', 'Annotated Sequence', 'Unnamed: 3', \n",
    "            'Modifications', 'Protein Groups', 'Proteins', 'PSMs', \n",
    "            'Master Protepeptidein Accessions', 'Positions in Proteins', \n",
    "            'Modifications in Proteins',\n",
    "            'Theo MHplus in Da', 'Quan Info', \n",
    "            'Confidence by Search Engine', \n",
    "            'q-Value by Search Engine',\n",
    "            'PEP by Search Engine',\n",
    "            'SVM Score by Search Engine',\n",
    "            'XCorr by Search Engine',\n",
    "            'PEP', 'q-Value', 'Top Apex RT', 'RT in min',\n",
    "            'Sequence', 'search_peptide', 'Peptide', 'protein_id', \n",
    "            'protein_description', 'Alignment', 'Species', \n",
    "            'Intervals', 'function', 'unique ID'\n",
    "            ]\n",
    "        \n",
    "        exclude_substrings = [\n",
    "            'Abundances by Bio Rep', \n",
    "            'Count', \n",
    "            'Origin',\n",
    "            'Average_Abundance'  # Added to exclude average abundance columns\n",
    "        ]\n",
    "    \n",
    "        # Use cleaned data if available, otherwise use original\n",
    "        df = self.pd_results_cleaned if (hasattr(self, 'pd_results_cleaned') and \n",
    "                                       not self.pd_results_cleaned.empty) else self.pd_results\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            # More flexible column filtering\n",
    "            self.filtered_columns = []\n",
    "            for col in df.columns:\n",
    "                # Check if any exclusion pattern matches the column name\n",
    "                should_exclude = any(excl.lower() in col.lower() for excl in columns_to_exclude)\n",
    "                # Check if any substring pattern matches\n",
    "                has_excluded_substring = any(sub.lower() in col.lower() for sub in exclude_substrings)\n",
    "                \n",
    "                if not should_exclude and not has_excluded_substring:\n",
    "                    self.filtered_columns.append(col)\n",
    "              \n",
    "            # Update dropdown options\n",
    "            self.column_dropdown.options = self.filtered_columns\n",
    "            self._reset_inputs()\n",
    "        else:\n",
    "            self.filtered_columns = []\n",
    "            self.column_dropdown.options = []\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:red;\">No valid data available for processing.</b>'))\n",
    "\n",
    "    def display_group_selector(self):\n",
    "        \"\"\"Display the JSON file selector for group dictionaries\"\"\"\n",
    "        display(widgets.HTML(\"<h3><u>Upload Existing Group Dictionary:</u></h3>\"))\n",
    "        display(self.group_uploader, self.gd_output_area)\n",
    "        \n",
    "\n",
    "    def display_widgets(self):\n",
    "        \"\"\"Display the main UI for group selection\"\"\"\n",
    "        # Create main grid container\n",
    "        grid = widgets.GridspecLayout(1, 2,  # Number of rows and columns\n",
    "            width='1000px', \n",
    "            grid_gap='5px',  # Adjust spacing between grid elements\n",
    "        )\n",
    "        \n",
    "        # Create input container with vertical scroll\n",
    "        input_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Select New Grouping of Data:</u></h3>\"),\n",
    "            widgets.HTML('Now select the <b>absorbance columns</b> and assign the name of the <b>grouping variable</b>:'),\n",
    "            self.column_dropdown,\n",
    "            self.grouping_variable_text,\n",
    "            # Create button layouts\n",
    "            widgets.HBox([self.search_button, self.add_group_button]),\n",
    "            widgets.HBox([self.reset_file_button])\n",
    "        ], layout=widgets.Layout(\n",
    "            width='95%',\n",
    "            height='600px',\n",
    "            overflow_y='auto'  # Add vertical scroll\n",
    "        ))\n",
    "        \n",
    "        # Create output container with vertical scroll\n",
    "        output_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Group Selection Results:</u></h3>\"),\n",
    "            self.output\n",
    "        ], layout=widgets.Layout(\n",
    "            width='95%',\n",
    "            height='600px',\n",
    "            overflow_y='auto',  # Add vertical scroll\n",
    "            padding='10px'\n",
    "        ))\n",
    "        \n",
    "        # Add to grid\n",
    "        grid[0, 0] = input_container  # Left column\n",
    "        grid[0, 1] = output_container  # Right column\n",
    "        \n",
    "        display(grid)\n",
    "    def _on_gd_submit(self, b, dropdown):\n",
    "        \"\"\"Handle JSON file submission\"\"\"\n",
    "        selected_file = dropdown.value\n",
    "        with self.gd_output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            if selected_file == 'Select an existing grouping dictionary file':\n",
    "                print(\"Please select a valid file.\")\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                # Load and process JSON file\n",
    "                with open(selected_file, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                self.group_data = {}\n",
    "                \n",
    "                # Process groups\n",
    "                with self.output:\n",
    "                    clear_output()\n",
    "                    for group_number, group_info in data.items():\n",
    "                        group_name = group_info.get('grouping_variable')\n",
    "                        selected_columns = group_info.get('abundance_columns')\n",
    "                        \n",
    "                        self.group_data[group_number] = {\n",
    "                            'grouping_variable': group_name,\n",
    "                            'abundance_columns': selected_columns\n",
    "                        }\n",
    "                        \n",
    "                        display(widgets.HTML(\n",
    "                            f\"<b>Group {group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"\n",
    "                        ))\n",
    "                        display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "                        display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "                        display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "                        \n",
    "                display(widgets.HTML(f'<b style=\"color:green;\">Successfully uploaded: {selected_file}</b>'))\n",
    "                \n",
    "            except Exception as e:\n",
    "                display(widgets.HTML(f\"<b style='color:red;'>An error occurred while processing the file: {str(e)}</b>\"))\n",
    "    \n",
    "    def _search_columns(self, b):\n",
    "        \"\"\"Search for columns based on group name\"\"\"\n",
    "        group_name = self.grouping_variable_text.value\n",
    "        if group_name:\n",
    "            matching_columns = [col for col in self.filtered_columns if group_name in col]\n",
    "            self.column_dropdown.value = matching_columns\n",
    "        else:\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:red;\">Please enter a group name to search.</b>'))\n",
    "    \n",
    "    def _add_group(self, b):\n",
    "        \"\"\"Add a new group to the data\"\"\"\n",
    "        group_name = self.grouping_variable_text.value\n",
    "        selected_columns = list(self.column_dropdown.value)\n",
    "        \n",
    "        if not (group_name and selected_columns):\n",
    "            with self.output:\n",
    "                display(widgets.HTML('<b style=\"color:red;\">Please enter a group name and select at least one column.</b>'))\n",
    "            return\n",
    "        \n",
    "        # If group_data exists, use next number, otherwise start at 1\n",
    "        if self.group_data:\n",
    "            # Convert existing keys to integers and find max\n",
    "            existing_numbers = [int(k) for k in self.group_data.keys()]\n",
    "            next_number = max(existing_numbers) + 1\n",
    "            self.group_number = str(next_number)\n",
    "        else:\n",
    "            self.group_data = {}\n",
    "            self.group_number = \"1\"\n",
    "        \n",
    "        # Add new group data to the dictionary\n",
    "        self.group_data[self.group_number] = {\n",
    "            'grouping_variable': group_name,\n",
    "            'abundance_columns': selected_columns\n",
    "        }\n",
    "        \n",
    "        # Display output\n",
    "        with self.output:\n",
    "            display(widgets.HTML(f\"<b>Group {self.group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"))\n",
    "            display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "            display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "            display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "        \n",
    "        self._reset_inputs()\n",
    "        \n",
    "    def _reset_selection(self, b):\n",
    "        \"\"\"Reset all selections and data\"\"\"\n",
    "        self.group_data = {}\n",
    "        self.group_number = 1\n",
    "        with self.gd_output_area:\n",
    "            clear_output()\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "        self._reset_inputs()\n",
    "    \n",
    "    def _reset_inputs(self):\n",
    "        \"\"\"Reset input fields\"\"\"\n",
    "        self.grouping_variable_text.value = ''\n",
    "        self.column_dropdown.value = ()\n",
    "\n",
    "    def _on_group_upload_change(self, change):\n",
    "        \"\"\"Handle JSON file upload\"\"\"\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.gd_output_area:\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    try:\n",
    "                        content = bytes(file_data.content).decode('utf-8')\n",
    "                        data = json.loads(content)\n",
    "                        \n",
    "                        # Process groups\n",
    "                        with self.output:\n",
    "                            for group_number, group_info in data.items():\n",
    "                                group_name = group_info.get('grouping_variable')\n",
    "                                selected_columns = group_info.get('abundance_columns')\n",
    "                                \n",
    "                                # Update group_data without clearing previous entries\n",
    "                                self.group_data[group_number] = {\n",
    "                                    'grouping_variable': group_name,\n",
    "                                    'abundance_columns': selected_columns\n",
    "                                }\n",
    "                                \n",
    "                                display(widgets.HTML(\n",
    "                                    f\"<b>Group {group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"\n",
    "                                ))\n",
    "                                display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "                                display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "                                display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "                                \n",
    "                        display(widgets.HTML(f'<b style=\"color:green;\">Successfully uploaded: {file_data.name}</b>'))\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        display(widgets.HTML(f\"<b style='color:red;'>An error occurred while processing the file: {str(e)}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "562e0b03-4474-4a37-b9d6-f73fc45d47c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8ef00096324b42a0a1286122fcad9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3><u>Upload Existing Group Dictionary:</u></h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d3e3643cb44c048546f521a07bd3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.json', description='Upload Groups File', layout=Layout(width='300px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2825cc55cc984cdaa068a81edc418d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001bc1b3d0be441e97021c7244d7c080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridspecLayout(children=(VBox(children=(HTML(value='<h3><u>Select New Grouping of Data:</u></h3>'), HTML(valueâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "group_processor = GroupProcessing()\n",
    "group_processor.display_group_selector()\n",
    "#roup_processor.setup_data()\n",
    "group_processor.display_widgets()\n",
    "data_transformer.attach_observers(group_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c47e5-227d-4fb1-8147-a187f6086bba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transforms & Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16f50828-45c1-444e-be98-4ed3ff7a947e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CombineAverageDataframes:\n",
    "    def __init__(self, data_transformer, group_processor):\n",
    "        self.data_transformer = data_transformer\n",
    "        self.group_processor = group_processor\n",
    "        self.pd_results = data_transformer.pd_results\n",
    "        self.mbpdb_results = data_transformer.mbpdb_results\n",
    "        self.pd_results_cleaned = self.pd_results.copy() if self.pd_results is not None else None\n",
    "        self._merged_df = None  # Add this line\n",
    "        # Set up observer for data changes\n",
    "        self.data_transformer.observe(self._handle_data_change, names=['pd_results', 'mbpdb_results'])\n",
    "     \n",
    "    def _handle_data_change(self, change):\n",
    "        \"\"\"Handle changes in the input data.\"\"\"\n",
    "        if change.name == 'pd_results':\n",
    "            self.pd_results = change.new\n",
    "        elif change.name == 'mbpdb_results':\n",
    "            self.mbpdb_results = change.new\n",
    "            \n",
    "        self.pd_results_cleaned = self.pd_results.copy() if self.pd_results is not None else None\n",
    "        \n",
    "        # Re-run interactive display\n",
    "        clear_output()        \n",
    "    @property\n",
    "    def merged_df(self):\n",
    "        \"\"\"Property to access the merged DataFrame.\"\"\"\n",
    "        return self._merged_df\n",
    "\n",
    "    def extract_bioactive_peptides(self):\n",
    "        \"\"\"\n",
    "        Extracts the list of bioactive peptide matches from the imported MBPDB search.\n",
    "        \"\"\"\n",
    "        if not self.mbpdb_results.empty:\n",
    "            # Drop rows where protein_id is NaN or 'None'\n",
    "            mbpdb_results_cleaned = self.mbpdb_results.copy()\n",
    "            mbpdb_results_cleaned.dropna(subset=['search_peptide'], inplace=True)\n",
    "            mbpdb_results_cleaned = mbpdb_results_cleaned[mbpdb_results_cleaned['protein_id'] != 'None']\n",
    "\n",
    "            # Check if '% Alignment' column exists\n",
    "            if '% Alignment' in mbpdb_results_cleaned.columns:\n",
    "                agg_dict = {\n",
    "                    'peptide': 'first', \n",
    "                    'protein_id': 'first',\n",
    "                    'protein_description': 'first',\n",
    "                    '% Alignment': 'first',\n",
    "                    'species': 'first',\n",
    "                    'intervals': 'first',\n",
    "                    'function': lambda x: list(x.dropna().unique())\n",
    "                }\n",
    "            else:\n",
    "                agg_dict = {\n",
    "                    'peptide': 'first', \n",
    "                    'search_peptide': 'first',\n",
    "                    'protein_description': 'first',\n",
    "                    'species': 'first',\n",
    "                    'intervals': 'first',\n",
    "                    'function': lambda x: list(x.dropna().unique())\n",
    "                }\n",
    "\n",
    "            # Perform the groupby and aggregation\n",
    "            mbpdb_results_grouped = mbpdb_results_cleaned.groupby('search_peptide').agg(agg_dict).reset_index()\n",
    "\n",
    "            # Flatten the 'function' list\n",
    "            mbpdb_results_grouped['function'] = mbpdb_results_grouped['function'].apply(\n",
    "                lambda x: '; '.join(x) if isinstance(x, list) else x\n",
    "            )\n",
    "            return mbpdb_results_cleaned, mbpdb_results_grouped\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def create_unique_id(self, row):\n",
    "        \"\"\"Creates a unique ID for each peptide row.\"\"\"\n",
    "        if pd.notna(row['Modifications']):\n",
    "            unique_id = row['Sequence'] + \"_\" + row['Modifications'].strip()\n",
    "        else:\n",
    "            unique_id = row['Sequence']\n",
    "        return unique_id.rstrip('_')\n",
    "\n",
    "    def process_pd_results(self, mbpdb_results_grouped):\n",
    "        \"\"\"Processes the PD results and merges with MBPDB results.\"\"\"\n",
    "        pd_results_cleaned = self.pd_results_cleaned\n",
    "\n",
    "        # Process positions and accessions\n",
    "        pd_results_cleaned['Positions in Proteins'] = pd_results_cleaned['Positions in Proteins'].str.split(';', expand=False).str[0]\n",
    "        pd_results_cleaned['Master Protein Accessions'] = pd_results_cleaned['Master Protein Accessions'].str.split(';', expand=False).str[0]\n",
    "\n",
    "        # Create sequence column if needed\n",
    "        if 'Sequence' not in pd_results_cleaned.columns:\n",
    "            pd_results_cleaned['Sequence'] = pd_results_cleaned['Annotated Sequence'].str.split('.', expand=False).str[1]\n",
    "\n",
    "        # Create unique ID\n",
    "        pd_results_cleaned['unique ID'] = pd_results_cleaned.apply(self.create_unique_id, axis=1)\n",
    "\n",
    "        # Extract start and stop positions\n",
    "        try:\n",
    "            extracted = pd_results_cleaned['Positions in Proteins'].str.extract(r'\\[(\\d+)-(\\d+)\\]')\n",
    "            pd_results_cleaned[['start', 'stop']] = extracted.astype(float).astype('Int64')\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "        # Reorder columns\n",
    "        columns_order = ['Master Protein Accessions', 'Positions in Proteins', 'start', 'stop'] + \\\n",
    "                        [col for col in pd_results_cleaned.columns if col not in ['Master Protein Accessions', 'Positions in Proteins', 'start', 'stop']]\n",
    "        pd_results_cleaned = pd_results_cleaned[columns_order]\n",
    "\n",
    "        # Merge with MBPDB results if available\n",
    "        if mbpdb_results_grouped is not None and not mbpdb_results_grouped.empty:\n",
    "            merged_df = pd.merge(pd_results_cleaned, mbpdb_results_grouped, \n",
    "                               right_on='search_peptide', left_on='unique ID', how='left')\n",
    "            display(HTML(\"<b style='color:green;'>The MBPDB was successfully merged with the peptidomic data matching the Search Peptide and Unique ID columns.</b>\"))\n",
    "        else:\n",
    "            merged_df = pd_results_cleaned.copy()\n",
    "            merged_df['function'] = np.nan\n",
    "            display(HTML(\"<b style='color:orange;'>No MBPDB was uploaded.</b>\"))\n",
    "            display(HTML(\"<b style='color:orange;'>The merged Dataframe contains only peptidomic data.</b>\"))\n",
    "\n",
    "        return merged_df\n",
    "    \n",
    "    def calculate_group_abundance_averages(self, df, group_data):\n",
    "        \"\"\"Calculates group abundance averages.\"\"\"\n",
    "        # Check if all average abundance columns already exist\n",
    "        all_columns_exist = True\n",
    "        for group_number, details in group_data.items():\n",
    "            average_column_name = f\"Average_Abundance_{details['grouping_variable']}\"\n",
    "            if average_column_name not in df.columns:\n",
    "                all_columns_exist = False\n",
    "                break\n",
    "        \n",
    "        if all_columns_exist:\n",
    "            display(HTML('<b style=\"color:orange;\">All average abundance columns already exist. Returning original DataFrame.</b>'))\n",
    "            return df\n",
    "        \n",
    "        # If not all columns exist, proceed with calculations\n",
    "        new_columns = {}\n",
    "        for group_number, details in group_data.items():\n",
    "            grouping_variable = details['grouping_variable']\n",
    "            abundance_columns = details['abundance_columns']\n",
    "            \n",
    "            # Convert abundance columns to numeric\n",
    "            for col in abundance_columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Calculate averages\n",
    "            average_column_name = f\"Average_Abundance_{grouping_variable}\"\n",
    "            new_columns[average_column_name] = df[abundance_columns].mean(axis=1, skipna=True)\n",
    "        \n",
    "        # Add new columns to DataFrame\n",
    "        df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "        if not df.empty:\n",
    "            display(HTML('<b style=\"color:green;\">Group average abundance columns have been successfully added to the DataFrame.</b>'))\n",
    "        return df\n",
    "    \n",
    "    def process_data(self, group_data):\n",
    "        \"\"\"Main method to process all data.\"\"\"\n",
    "        if hasattr(self, 'pd_results') and self.pd_results is not None and not self.pd_results.empty:\n",
    "            try:\n",
    "                # Extract and process bioactive peptides\n",
    "                mbpdb_results_cleaned, mbpdb_results_grouped = self.extract_bioactive_peptides()\n",
    "                \n",
    "                if not hasattr(self, 'pd_results_cleaned') or self.pd_results_cleaned is None:\n",
    "                    self.pd_results_cleaned = self.pd_results.copy()\n",
    "                \n",
    "                # Process PD results and merge with MBPDB\n",
    "                merged_df_temp = self.process_pd_results(mbpdb_results_grouped)\n",
    "                \n",
    "                # Calculate abundance averages if group_data exists\n",
    "                if group_data:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                        final_df = self.calculate_group_abundance_averages(merged_df_temp, group_data)\n",
    "                else:\n",
    "                    final_df = merged_df_temp\n",
    "                    display(HTML(\"<b style='color:orange;'>No group data provided. Skipping abundance calculations.</b>\"))\n",
    "                \n",
    "                # Store the final DataFrame\n",
    "                self._merged_df = final_df\n",
    "                return final_df\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red;'>Error processing data: {str(e)}</b>\"))\n",
    "                return None\n",
    "        else:\n",
    "            display(HTML(\"<b style='color:red;'>No PD results data available for processing.</b>\"))\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def display_interactive_results(self, df):\n",
    "        if df is not None:\n",
    "            # Create DataGrid\n",
    "            grid = DataGrid(df, selection_mode='cell', editable=False)\n",
    "            grid.auto_fit_columns = True\n",
    "            grid.base_row_size = 25\n",
    "            grid.base_column_size = 150\n",
    "            grid.auto_fit_params = {'area': 'column', 'padding': 10}\n",
    "            \n",
    "            # Display the grid\n",
    "            display(grid)\n",
    "        else:\n",
    "            print(\"No data to display\")\n",
    "\n",
    "        \n",
    "    def update_data(self, pd_results, mbpdb_results):\n",
    "        \"\"\"Update the input data and refresh the displa'.\"\"\"\n",
    "        self.pd_results = pd_results\n",
    "        self.mbpdb_results = mbpdb_results\n",
    "        self.pd_results_cleaned = pd_results.copy() if pd_results is not None else None\n",
    "        \n",
    "        # Clear previous outputs and rerun interactive display\n",
    "        clear_output()\n",
    "    \n",
    "\n",
    "        # Display button and output\n",
    "        display(confirm_button)\n",
    "        display(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20958467-4a02-4b10-afbb-1a3224430ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExportManager:\n",
    "    \"\"\"Class to manage all export operations with download link generation\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output_style = \"\"\"\n",
    "            <style>\n",
    "            .download-link {\n",
    "                background-color: #4CAF50;\n",
    "                border: none;\n",
    "                color: white;\n",
    "                padding: 10px 20px;\n",
    "                text-align: center;\n",
    "                text-decoration: none;\n",
    "                display: inline-block;\n",
    "                font-size: 14px;\n",
    "                margin: 4px 2px;\n",
    "                cursor: pointer;\n",
    "                border-radius: 4px;\n",
    "            }\n",
    "            </style>\n",
    "        \"\"\"\n",
    "    \n",
    "    def _generate_download_link(self, content, filename, filetype='text/csv'):\n",
    "        \"\"\"Generate a download link for any content\"\"\"\n",
    "        if isinstance(content, pd.DataFrame):\n",
    "            content = content.to_csv(index=False)\n",
    "            \n",
    "        if isinstance(content, dict):\n",
    "            content = json.dumps(content, indent=4)\n",
    "            \n",
    "        if isinstance(content, str):\n",
    "            content = content.encode()\n",
    "            \n",
    "        b64 = base64.b64encode(content).decode()\n",
    "        return f\"\"\"\n",
    "            {self.output_style}\n",
    "            <a download=\"{filename}\" href=\"data:{filetype};base64,{b64}\" class=\"download-link\">\n",
    "                Download {filename}\n",
    "            </a>\n",
    "        \"\"\"\n",
    "\n",
    "    def export_group_data(self, group_data):\n",
    "        \"\"\"Export group data as JSON with download link\"\"\"\n",
    "        display(HTML(\"<h3><u>Export Group Data</u></h3>\"))\n",
    "        display(HTML(\"Enter a name for your group data JSON file:\"))\n",
    "\n",
    "        name_widget = widgets.Text(\n",
    "            value='',\n",
    "            placeholder='Enter file name',\n",
    "            description='File name:',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        save_button = widgets.Button(\n",
    "            description='Generate Download',\n",
    "            button_style='success'\n",
    "        )\n",
    "        \n",
    "        output = widgets.Output()\n",
    "        \n",
    "        def on_save_clicked(b):\n",
    "            with output:\n",
    "                output.clear_output()\n",
    "                filename = name_widget.value\n",
    "                if not filename:\n",
    "                    display(HTML(\"<b style='color:red;'>Please enter a file name</b>\"))\n",
    "                    return\n",
    "                    \n",
    "                if not filename.endswith('.json'):\n",
    "                    filename += '.json'\n",
    "                    \n",
    "                download_link = self._generate_download_link(\n",
    "                    group_data, \n",
    "                    filename,\n",
    "                    'application/json'\n",
    "                )\n",
    "                display(HTML(f\"<h4>Group Data Export:</h4><hr style='border:1px solid grey;'>{download_link}\"))\n",
    "        \n",
    "        save_button.on_click(on_save_clicked)\n",
    "        display(name_widget, save_button, output)\n",
    "\n",
    "    def export_dataframe(self, df):\n",
    "        \"\"\"Export DataFrame as CSV with download link\"\"\"\n",
    "        display(HTML(\"<h3><u>Export Full Dataset</u></h3>\"))\n",
    "        display(HTML(\"Enter a name for your CSV file:\"))\n",
    "\n",
    "        name_widget = widgets.Text(\n",
    "            value='',\n",
    "            placeholder='Enter file name',\n",
    "            description='File name:',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        save_button = widgets.Button(\n",
    "            description='Generate Download',\n",
    "            button_style='success'\n",
    "        )\n",
    "        \n",
    "        output = widgets.Output()\n",
    "        \n",
    "        def on_save_clicked(b):\n",
    "            with output:\n",
    "                output.clear_output()\n",
    "                filename = name_widget.value\n",
    "                if not filename:\n",
    "                    display(HTML(\"<b style='color:red;'>Please enter a file name</b>\"))\n",
    "                    return\n",
    "                    \n",
    "                if not filename.endswith('.csv'):\n",
    "                    filename += '.csv'\n",
    "                    \n",
    "                download_link = self._generate_download_link(df, filename)\n",
    "                display(HTML(f\"<h4>DataFrame Export:</h4><hr style='border:1px solid grey;'>{download_link}\"))\n",
    "        \n",
    "        save_button.on_click(on_save_clicked)\n",
    "        display(name_widget, save_button, output)\n",
    "\n",
    "    def setup_volcano_plot_export(self, merged_df, group_data):\n",
    "        \"\"\"Setup and handle volcano plot data export\"\"\"\n",
    "        display(HTML(\"<h3><u>Volcano Plot Exporting</u></h3>\"))\n",
    "        \n",
    "        save_button = widgets.Button(\n",
    "            description='Generate Downloads',\n",
    "            button_style='success'\n",
    "        )\n",
    "        output = widgets.Output()\n",
    "        \n",
    "        def create_pivoted_df(df, abundance_columns):\n",
    "            melted_df = df.melt(\n",
    "                id_vars=['unique ID'],\n",
    "                value_vars=abundance_columns,\n",
    "                var_name='Sample',\n",
    "                value_name='Abundance'\n",
    "            )\n",
    "            return melted_df.pivot_table(\n",
    "                index='Sample',\n",
    "                columns='unique ID',\n",
    "                values='Abundance'\n",
    "            )\n",
    "\n",
    "        def on_save_clicked(b):\n",
    "            with output:\n",
    "                output.clear_output()\n",
    "                download_links = []\n",
    "                \n",
    "                for group_key, group_info in group_data.items():\n",
    "                    pivoted_df = create_pivoted_df(\n",
    "                        merged_df, \n",
    "                        group_info['abundance_columns']\n",
    "                    )\n",
    "                    \n",
    "                    if not pivoted_df.empty:\n",
    "                        filename = f\"{group_info['grouping_variable']}___all-peptides___volcano_plot.csv\"\n",
    "                        download_link = self._generate_download_link(pivoted_df, filename)\n",
    "                        download_links.append((filename, download_link))\n",
    "                \n",
    "                if download_links:\n",
    "                    output_html = \"<h4>Volcano Plot Exports:</h4><hr style='border:1px solid grey;'>\"\n",
    "                    for filename, link in download_links:\n",
    "                        output_html += f\"<p><b>{filename}</b>:<br>{link}</p>\"\n",
    "                    display(HTML(output_html))\n",
    "                else:\n",
    "                    display(HTML(\"<b style='color:red;'>No data available for export</b>\"))\n",
    "        \n",
    "        save_button.on_click(on_save_clicked)\n",
    "        display(save_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b506a35-a853-46ee-b193-3fce79decf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3cec7c42d1c436d8f5c137bc942d149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Process Data', style=ButtonStyle(), tooltip='Click to start data pâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7981a6450de348ea9d7e133fd146edfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cecb01b9a1b049eda6e4ae2038a8d75b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DataProcessingController:\n",
    "    def __init__(self):\n",
    "        self.export_manager = ExportManager()\n",
    "        self.combiner = None\n",
    "        self.merged_df = None\n",
    "        \n",
    "        # Create processing button\n",
    "        self.process_button = widgets.Button(\n",
    "            description='Process Data',\n",
    "            button_style='success',\n",
    "            tooltip='Click to start data processing'\n",
    "        )\n",
    "        self.process_output = widgets.Output()\n",
    "        self.export_output = widgets.Output()\n",
    "        \n",
    "        # Set up button callback\n",
    "        self.process_button.on_click(self._on_process_clicked)\n",
    "    \n",
    "    def _on_process_clicked(self, b):\n",
    "        with self.process_output:\n",
    "            clear_output()\n",
    "            print(\"Processing data...\")\n",
    "            \n",
    "            # Create the combiner\n",
    "            self.combiner = CombineAverageDataframes(data_transformer, group_processor)\n",
    "            \n",
    "            # Process the data\n",
    "            self.merged_df = self.combiner.process_data(group_processor.group_data)\n",
    "            \n",
    "            if self.merged_df is not None:\n",
    "                print(\"\\nData processing completed successfully!\")\n",
    "                print(f\"Final results shape: {self.merged_df.shape}\")\n",
    "                \n",
    "                # Display results\n",
    "                grid = DataGrid(self.merged_df)\n",
    "                grid.auto_fit_columns = True\n",
    "                display(grid)\n",
    "                \n",
    "                # Show export options\n",
    "                self._show_export_options()\n",
    "            else:\n",
    "                print(\"Error: No data was processed\")\n",
    "    \n",
    "    def _show_export_options(self):\n",
    "        with self.export_output:\n",
    "            clear_output()\n",
    "            display(HTML(\"<h2>Export:</h2>\"))\n",
    "            \n",
    "            # Export group data\n",
    "            self.export_manager.export_group_data(group_processor.group_data)\n",
    "            \n",
    "            # Only show DataFrame and volcano plot exports if data is processed\n",
    "            if self.merged_df is not None:\n",
    "                self.export_manager.export_dataframe(self.merged_df)\n",
    "                self.export_manager.setup_volcano_plot_export(\n",
    "                    self.merged_df,\n",
    "                    group_processor.group_data\n",
    "                )\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the complete interface\"\"\"\n",
    "        display(self.process_button)\n",
    "        display(self.process_output)\n",
    "        display(self.export_output)\n",
    "\n",
    "# Initialize the controller\n",
    "controller = DataProcessingController()\n",
    "\n",
    "# Display the interface\n",
    "controller.display()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Vt5dV-pTHRWngML-2nDQAR6_P_KFsIx4",
     "timestamp": 1712158917217
    },
    {
     "file_id": "1l7fpCQepyE1pJq2O5QfOHVv9a4VFpr-B",
     "timestamp": 1712094574841
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
