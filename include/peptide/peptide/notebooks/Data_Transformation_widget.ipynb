{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278kpom78fOP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23025,
     "status": "ok",
     "timestamp": 1712094448807,
     "user": {
      "displayName": "Russell Kuhfeld",
      "userId": "14760569517288879712"
     },
     "user_tz": 420
    },
    "id": "278kpom78fOP",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "95f300e9-4d05-4fe1-a725-f5c3eea6cf80",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Install/Import packages & define key varribles and functions\n",
    "# Run install script\n",
    "# %chmod +x setup_jupyterlab.sh\n",
    "# %./setup_jupyterlab.sh\n",
    "\n",
    "# Import necessary libraries for the script to function.\n",
    "import pandas as pd\n",
    "import tempfile, csv, json, re, os, shutil, io, base64, time, subprocess, sqlite3, zipfile, base64\n",
    "from io import StringIO, BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from django.conf import settings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.formula.api import ols\n",
    "#from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import warnings\n",
    "\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import combinations\n",
    "from ipydatagrid import DataGrid\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "import traitlets\n",
    "from traitlets import HasTraits, Instance, observe\n",
    "\n",
    "# Global variable declaration\n",
    "\n",
    "import _settings as settings\n",
    "global spec_translate_list\n",
    "spec_translate_list = settings.SPEC_TRANSLATE_LIST\n",
    "# Set the default font to Calibri\n",
    "#matplotlib.rcParams['font.family'] = 'Calibri'\n",
    "\n",
    "def find_species(header, spec_translate_list):\n",
    "    \"\"\"Search for a species in the header and return the first element (species name) from the list.\"\"\"\n",
    "    header_lower = header.lower()\n",
    "    for spec_group in spec_translate_list:\n",
    "        for term in spec_group[1:]:  # Iterate over possible species names/terms except the first element\n",
    "            if term.lower() in header_lower:\n",
    "                return spec_group[0]  # Return the first element of the list (main species name)\n",
    "    return \"unknown\"  # Return unknown if no species match is found\n",
    "\n",
    "def parse_headers():\n",
    "    fasta_dict = {}\n",
    "    with open(\"protein_headers.txt\", 'r') as file:\n",
    "        protein_id = \"\"\n",
    "        protein_name = \"\"\n",
    "        species = \"\"\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if protein_id:\n",
    "                    # Save the previous protein entry in the dictionary\n",
    "                    fasta_dict[protein_id] = {\n",
    "                        \"name\": protein_name,\n",
    "                        \"species\": species\n",
    "                    }\n",
    "                sequence = \"\"\n",
    "                header_parts = line[1:].split('|')\n",
    "                if len(header_parts) > 2:\n",
    "                    protein_id = header_parts[1]\n",
    "                    protein_name_full = re.split(r' OS=', header_parts[2])[0]\n",
    "                    if ' ' in protein_name_full:\n",
    "\n",
    "                        protein_name = protein_name_full#.split()[1]\n",
    "                    else:\n",
    "                        protein_name = protein_name_full\n",
    "                    # Find species in the header\n",
    "                    species = find_species(line, spec_translate_list)\n",
    "\n",
    "        if protein_id:\n",
    "            # Save the last protein entry in the dictionary\n",
    "            fasta_dict[protein_id] = {\n",
    "                \"name\": protein_name,\n",
    "                \"species\": species\n",
    "            }\n",
    "    return fasta_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d06c8cd-9a5f-45cb-bf09-1a96f1eb77bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataTransformation(HasTraits):\n",
    "    pd_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    mbpdb_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    # pd_results_cleaned = Instance(pd.DataFrame, allow_none=True)\n",
    "    search_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    proteins_dic = {}  # Add explicit trait for proteins_dic\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pd_results = pd.DataFrame()\n",
    "        # self.pd_results_cleaned = pd.DataFrame()\n",
    "        self.mbpdb_results = pd.DataFrame()\n",
    "        self.search_results = pd.DataFrame()\n",
    "        self.proteins_dic = parse_headers()\n",
    "        self.output_area = None\n",
    "        self.mbpdb_uploader = None\n",
    "        self.pd_uploader = None\n",
    "        self.fasta_uploader = None\n",
    "        self.reset_button = None\n",
    "        self.search_widget = None\n",
    "        self.search_progress = None\n",
    "\n",
    "    def create_download_link(self, file_path, label):\n",
    "        \"\"\"Create a download link for a file.\"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            # Read file content and encode it as base64\n",
    "            with open(file_path, 'rb') as f:\n",
    "                content = f.read()\n",
    "            b64_content = base64.b64encode(content).decode('utf-8')\n",
    "\n",
    "            # Generate the download link HTML\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <a download=\"{os.path.basename(file_path)}\" \n",
    "                   href=\"data:application/octet-stream;base64,{b64_content}\" \n",
    "                   style=\"color: #0366d6; text-decoration: none; margin-left: 20px; font-size: 14px;\">\n",
    "                    {label}\n",
    "                </a>\n",
    "            \"\"\")\n",
    "        else:\n",
    "            # Show an error message if the file does not exist\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <span style=\"color: red; margin-left: 20px; font-size: 14px;\">\n",
    "                    File \"{file_path}\" not found!\n",
    "                </span>\n",
    "            \"\"\")\n",
    "\n",
    "    def setup_search_ui(self, peptides):\n",
    "        \"\"\"Initialize and display the search UI\"\"\"\n",
    "        # Create dropdown for similarity threshold\n",
    "        self.threshold_dropdown = widgets.Dropdown(\n",
    "            options=list(range(0, 101, 10)),\n",
    "            value=80,\n",
    "            description='Similarity Threshold:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "\n",
    "        # Create search button\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search Peptides',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "\n",
    "        # Progress indicator\n",
    "        self.search_progress = widgets.HTML(\n",
    "            value=\"\",\n",
    "            layout=widgets.Layout(margin='10px 0px')\n",
    "        )\n",
    "\n",
    "        # Connect button click to handler\n",
    "        self.search_button.on_click(lambda b: self._on_search_click(b, ))\n",
    "\n",
    "        # Create layout\n",
    "        self.search_widget = widgets.VBox([\n",
    "            widgets.HBox([\n",
    "                self.threshold_dropdown,\n",
    "                self.search_button\n",
    "            ], layout=widgets.Layout(align_items='center')),\n",
    "            self.search_progress\n",
    "        ])\n",
    "\n",
    "        display(self.search_widget)\n",
    "\n",
    "    def _on_search_click(self, b):\n",
    "        \"\"\"Handle search button click\"\"\"\n",
    "        with self.search_output_area:\n",
    "            clear_output()\n",
    "\n",
    "            if self.pd_results is None or self.pd_results.empty:\n",
    "                display(HTML(\"<b style='color:red'>Please upload peptidomic data first.</b>\"))\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                # Extract sequences from peptidomic data\n",
    "                self.peptides = self._extract_sequences(self.pd_results)\n",
    "\n",
    "                if not self.peptides:\n",
    "                    display(HTML(\"<b style='color:red'>No valid sequences found in peptidomic data.</b>\"))\n",
    "                    return\n",
    "\n",
    "                display(HTML(f\"<b style='color:blue'>Found {len(self.peptides)} sequences. Searching database...</b>\"))\n",
    "\n",
    "                # Perform search\n",
    "                results = self._search_peptides_comprehensive(\n",
    "                    self.peptides,\n",
    "                    similarity_threshold=self.threshold_dropdown.value\n",
    "                )\n",
    "                # Format results if we have any matches\n",
    "                if not results.empty:\n",
    "                    self.mbpdb_results = self._format_search_results_with_matches(results)\n",
    "                    display(\n",
    "                        HTML(f\"<b style='color:green'>Search complete! Found {len(self.mbpdb_results)} matches</b>\"))\n",
    "                else:\n",
    "                    self.mbpdb_results = results\n",
    "                    display(HTML(\"<b style='color:orange'>No matches found in the database.</b>\"))\n",
    "\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red'>Error: {str(e)}</b>\"))\n",
    "                self.mbpdb_results = pd.DataFrame()\n",
    "\n",
    "    def _search_peptides_comprehensive(self, peptides, similarity_threshold=100):\n",
    "        \"\"\"Search for peptides with BLAST-based similarity matching\"\"\"\n",
    "\n",
    "        # WORK_DIRECTORY = '/home/kuhfeldrf/mbpdb/include/peptide/uploads/temp'\n",
    "        # conn = sqlite3.connect('/home/kuhfeldrf/mbpdb/include/peptide/db.sqlite3')\n",
    "\n",
    "        WORK_DIRECTORY = '../../uploads/temp'\n",
    "        conn = sqlite3.connect('../../db.sqlite3')\n",
    "        work_path = self._create_work_directory(WORK_DIRECTORY)\n",
    "\n",
    "        fasta_db_path = os.path.join(work_path, \"db.fasta\")\n",
    "        results = []\n",
    "        extra_info = defaultdict(list)\n",
    "\n",
    "        # Create database with all peptides for BLAST\n",
    "        query = \"SELECT p.id, p.peptide FROM peptide_peptideinfo p\"\n",
    "        db_peptides = pd.read_sql_query(query, conn)\n",
    "\n",
    "        # Create BLAST database\n",
    "        with open(fasta_db_path, 'w') as f:\n",
    "            for _, row in db_peptides.iterrows():\n",
    "                f.write(f\">{row['id']}\\n{row['peptide']}\\n\")\n",
    "\n",
    "        self._make_blast_db(fasta_db_path)\n",
    "\n",
    "        for peptide in self.peptides:\n",
    "            if similarity_threshold == 100 or len(peptide) <4:\n",
    "                query = \"\"\"\n",
    "                SELECT DISTINCT\n",
    "                    ? as search_peptide,\n",
    "                    pi.pid as protein_id,\n",
    "                    p.id as peptide_id,\n",
    "                    p.peptide,\n",
    "                    pi.desc as protein_description,\n",
    "                    pi.species,\n",
    "                    p.intervals,\n",
    "                    f.function,\n",
    "                    r.additional_details,\n",
    "                    r.ic50,\n",
    "                    r.inhibition_type,\n",
    "                    r.inhibited_microorganisms,\n",
    "                    r.ptm,\n",
    "                    r.title,\n",
    "                    r.authors,\n",
    "                    r.abstract,\n",
    "                    r.doi,\n",
    "                    'sequence' as search_type,\n",
    "                    'IDENTITY' as scoring_matrix\n",
    "                FROM peptide_peptideinfo p\n",
    "                JOIN peptide_proteininfo pi ON p.protein_id = pi.id\n",
    "                LEFT JOIN peptide_function f ON f.pep_id = p.id\n",
    "                LEFT JOIN peptide_reference r ON r.func_id = f.id\n",
    "                WHERE p.peptide = ?\n",
    "                \"\"\"\n",
    "                df = pd.read_sql_query(query, conn, params=[peptide, peptide])\n",
    "                results.append(df)\n",
    "            else:\n",
    "                # Run BLASTP search for similarity matching\n",
    "                query_path = os.path.join(work_path, \"query.fasta\")\n",
    "                with open(query_path, \"w\") as query_file:\n",
    "                    query_file.write(f\">pep_query\\n{peptide}\\n\")\n",
    "\n",
    "                output_path = os.path.join(work_path, \"blastp_short.out\")\n",
    "                blast_args = [\n",
    "                    \"blastp\",\n",
    "                    \"-query\", query_path,\n",
    "                    \"-db\", fasta_db_path,\n",
    "                    \"-outfmt\", \"6 std ppos qcovs qlen slen positive\",\n",
    "                    \"-evalue\", \"1000\",\n",
    "                    \"-word_size\", \"2\",\n",
    "                    \"-matrix\", \"IDENTITY\",\n",
    "                    \"-threshold\", \"1\",\n",
    "                    \"-task\", \"blastp-short\",\n",
    "                    \"-out\", output_path\n",
    "                ]\n",
    "\n",
    "                subprocess.check_output(blast_args, stderr=subprocess.STDOUT)\n",
    "\n",
    "                # Process BLAST results\n",
    "                search_ids = self._process_blast_results(output_path, similarity_threshold, extra_info)\n",
    "\n",
    "                if search_ids:\n",
    "                    df = self._fetch_peptide_data(conn, peptide, search_ids)\n",
    "                    self._add_blast_details(df, extra_info)\n",
    "                    results.append(df)\n",
    "\n",
    "        conn.close()\n",
    "        self._cleanup_work_directory(WORK_DIRECTORY)\n",
    "\n",
    "        return self._combine_results(results)\n",
    "\n",
    "    def _create_work_directory(self, base_dir):\n",
    "        \"\"\"Create a working directory for BLAST operations\"\"\"\n",
    "        path = os.path.join(base_dir, f'work_{int(round(time.time() * 1000))}')\n",
    "        os.makedirs(path)\n",
    "        return path\n",
    "\n",
    "    def _make_blast_db(self, library_fasta_path):\n",
    "        \"\"\"Create BLAST database from FASTA file\"\"\"\n",
    "        subprocess.check_output(\n",
    "            ['makeblastdb', '-in', library_fasta_path, '-dbtype', 'prot'],\n",
    "            stderr=subprocess.STDOUT\n",
    "        )\n",
    "\n",
    "    def _process_blast_results(self, output_path, similarity_threshold, extra_info):\n",
    "        \"\"\"Process BLAST results and collect search IDs\"\"\"\n",
    "        search_ids = []\n",
    "        csv.register_dialect('blast_dialect', delimiter='\\t')\n",
    "\n",
    "        with open(output_path, \"r\") as output_file:\n",
    "            blast_data = csv.DictReader(\n",
    "                output_file,\n",
    "                fieldnames=['query', 'subject', 'percid', 'align_len', 'mismatches',\n",
    "                            'gaps', 'qstart', 'qend', 'sstart', 'send', 'evalue',\n",
    "                            'bitscore', 'ppos', 'qcov', 'qlen', 'slen', 'numpos'],\n",
    "                dialect='blast_dialect'\n",
    "            )\n",
    "\n",
    "            for row in blast_data:\n",
    "                tlen = float(row['slen']) if float(row['slen']) > float(row['qlen']) else float(row['qlen'])\n",
    "                simcalc = 100 * ((float(row['numpos']) - float(row['gaps'])) / tlen)\n",
    "\n",
    "                if simcalc >= similarity_threshold:\n",
    "                    search_ids.append(row['subject'])\n",
    "                    extra_info[row['subject']] = [\n",
    "                        f\"{simcalc:.2f}\", row['qstart'], row['qend'], row['sstart'],\n",
    "                        row['send'], row['evalue'], row['align_len'], row['mismatches'],\n",
    "                        row['gaps']\n",
    "                    ]\n",
    "\n",
    "        return search_ids\n",
    "\n",
    "    def _fetch_peptide_data(self, conn, peptide, search_ids):\n",
    "        \"\"\"Fetch peptide data from database\"\"\"\n",
    "        placeholders = ','.join(['?' for _ in search_ids])\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            ? as search_peptide,\n",
    "            pi.pid as protein_id,\n",
    "            p.id as peptide_id,\n",
    "            p.peptide,\n",
    "            pi.desc as protein_description,\n",
    "            pi.species,\n",
    "            p.intervals,\n",
    "            f.function,\n",
    "            r.additional_details,\n",
    "            r.ic50,\n",
    "            r.inhibition_type,\n",
    "            r.inhibited_microorganisms,\n",
    "            r.ptm,\n",
    "            r.title,\n",
    "            r.authors,\n",
    "            r.abstract,\n",
    "            r.doi,\n",
    "            'sequence' as search_type,\n",
    "            'IDENTITY' as scoring_matrix\n",
    "        FROM peptide_peptideinfo p\n",
    "        JOIN peptide_proteininfo pi ON p.protein_id = pi.id\n",
    "        LEFT JOIN peptide_function f ON f.pep_id = p.id\n",
    "        LEFT JOIN peptide_reference r ON r.func_id = f.id\n",
    "        WHERE p.id IN ({placeholders})\n",
    "        \"\"\"\n",
    "\n",
    "        return pd.read_sql_query(query, conn, params=[peptide] + search_ids)\n",
    "\n",
    "    def _add_blast_details(self, df, extra_info):\n",
    "        \"\"\"Add BLAST details to DataFrame\"\"\"\n",
    "        for idx, row in df.iterrows():\n",
    "            if str(row['peptide_id']) in extra_info:\n",
    "                blast_details = extra_info[str(row['peptide_id'])]\n",
    "                df.at[idx, '% Alignment'] = blast_details[0]\n",
    "                df.at[idx, 'Query start'] = blast_details[1]\n",
    "                df.at[idx, 'Query end'] = blast_details[2]\n",
    "                df.at[idx, 'Subject start'] = blast_details[3]\n",
    "                df.at[idx, 'Subject end'] = blast_details[4]\n",
    "                df.at[idx, 'e-value'] = blast_details[5]\n",
    "                df.at[idx, 'Alignment length'] = blast_details[6]\n",
    "                df.at[idx, 'Mismatches'] = blast_details[7]\n",
    "                df.at[idx, 'Gap opens'] = blast_details[8]\n",
    "\n",
    "    def _cleanup_work_directory(self, work_directory):\n",
    "        \"\"\"Clean up old work directories\"\"\"\n",
    "        try:\n",
    "            dirs = [f for f in os.scandir(work_directory) if f.is_dir()]\n",
    "            dirs.sort(key=lambda x: os.path.getmtime(x.path), reverse=True)\n",
    "\n",
    "            for dir_entry in dirs[25:]:\n",
    "                try:\n",
    "                    shutil.rmtree(dir_entry.path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _combine_results(self, results):\n",
    "        \"\"\"Combine and format final results\"\"\"\n",
    "        if not results:\n",
    "            mbpdb_columns = [\n",
    "                'search_peptide', 'protein_id', 'peptide', 'protein_description',\n",
    "                'species', 'intervals', 'function', 'additional_details', 'ic50',\n",
    "                'inhibition_type', 'inhibited_microorganisms', 'ptm', 'title',\n",
    "                'authors', 'abstract', 'doi', 'search_type', 'scoring_matrix'\n",
    "            ]\n",
    "            return pd.DataFrame(columns=mbpdb_columns)\n",
    "\n",
    "        final_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "        if 'peptide_id' in final_results.columns:\n",
    "            final_results = final_results.drop('peptide_id', axis=1)\n",
    "\n",
    "        sort_columns = ['search_peptide']\n",
    "        if '% Alignment' in final_results.columns:\n",
    "            sort_columns.append('% Alignment')\n",
    "\n",
    "        return final_results.sort_values(\n",
    "            sort_columns,\n",
    "            ascending=[True] + [False] * (len(sort_columns) - 1)\n",
    "        )\n",
    "\n",
    "    def _format_search_results_with_matches(self, final_results):\n",
    "        \"\"\"Format search results with matches\"\"\"\n",
    "        if '% Alignment' in final_results.columns:\n",
    "            final_results['% Alignment'] = pd.to_numeric(\n",
    "                final_results['% Alignment'],\n",
    "                errors='coerce'\n",
    "            )\n",
    "\n",
    "        grouped = final_results.groupby([\"search_peptide\", \"function\"], as_index=False)\n",
    "        aggregated_results = []\n",
    "        processed_indices = set()\n",
    "\n",
    "        for _, group in grouped:\n",
    "            if len(group) > 1:\n",
    "                aggregated_row = self._aggregate_group_data(group)\n",
    "                aggregated_results.append(aggregated_row)\n",
    "                processed_indices.update(group.index)\n",
    "\n",
    "        remaining_rows = final_results.loc[~final_results.index.isin(processed_indices)]\n",
    "        aggregated_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "        return pd.concat([aggregated_df, remaining_rows], ignore_index=True)\n",
    "\n",
    "    def _aggregate_group_data(self, group):\n",
    "        \"\"\"Aggregate data for a group of results\"\"\"\n",
    "\n",
    "        def enumerate_field(field):\n",
    "            if field in group.columns and not group[field].dropna().empty:\n",
    "                valid_values = set(group[field].dropna().astype(str).str.strip())\n",
    "                valid_values = {val for val in valid_values if val != ''}\n",
    "                if len(valid_values) > 1:\n",
    "                    return \"; \".join([f\"{i + 1}) {val}\" for i, val in enumerate(valid_values)])\n",
    "                elif len(valid_values) == 1:\n",
    "                    return next(iter(valid_values))\n",
    "                return ''\n",
    "            return ''\n",
    "\n",
    "        return {col: enumerate_field(col) for col in group.columns}\n",
    "\n",
    "    def setup_data_loading_ui(self):\n",
    "        \"\"\"Initialize and display the data loading UI with integrated search and help tooltips\"\"\"\n",
    "\n",
    "        def create_help_icon(tooltip_text):\n",
    "            \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "            help_icon = widgets.HTML(\n",
    "                value='<i class=\"fa fa-question-circle\" style=\"color: #007bff;\"></i>',\n",
    "                layout=widgets.Layout(width='25px', margin='2px 5px')\n",
    "            )\n",
    "            help_icon.add_class('jupyter-widgets')\n",
    "            help_icon.add_class('widget-html')\n",
    "            return widgets.HTML(\n",
    "                f'<div title=\"{tooltip_text}\" style=\"display: inline-block;\">{help_icon.value}</div>'\n",
    "            )\n",
    "\n",
    "        def create_labeled_uploader(widget, label, tooltip):\n",
    "            \"\"\"Create an uploader with label and help icon\"\"\"\n",
    "            return widgets.HBox([\n",
    "                widget,\n",
    "                create_help_icon(tooltip)\n",
    "            ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        # Create file upload widgets with the same configurations\n",
    "        self.mbpdb_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload MBPDB File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        self.pd_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload Peptidomic File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        self.fasta_uploader = widgets.FileUpload(\n",
    "            accept='.fasta',\n",
    "            multiple=True,\n",
    "            description='Upload FASTA Files',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        # Create search interface\n",
    "        self.threshold_dropdown = widgets.Dropdown(\n",
    "            options=list(range(0, 101, 10)),\n",
    "            value=80,\n",
    "            description='Similarity Threshold (%):',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='225px')\n",
    "        )\n",
    "\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search Database',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "\n",
    "        # Reset button\n",
    "        self.reset_button = widgets.Button(\n",
    "            description='Reset',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='100px')\n",
    "        )\n",
    "\n",
    "        # Create output areas\n",
    "        self.output_area = widgets.Output()\n",
    "        self.search_output_area = widgets.Output()\n",
    "\n",
    "        mbpdb_box = widgets.HBox([\n",
    "            widgets.HTML(\"\"\"\n",
    "                    <div margin-bottom: 5px;'>\n",
    "                        <b>Option 1: Upload File</b>\n",
    "                    </div>\n",
    "                \"\"\"),\n",
    "            self.create_download_link(\n",
    "                \"example_MBPDB_search.tsv\",\n",
    "                \"Example\"\n",
    "            )\n",
    "        ])\n",
    "        # Create MBPDB options section\n",
    "        mbpdb_options = widgets.HBox([widgets.VBox([\n",
    "            mbpdb_box,\n",
    "            create_labeled_uploader(\n",
    "                self.mbpdb_uploader,\n",
    "                \"MBPDB File\",\n",
    "                \"Upload your own MBPDB file (optional)\"\n",
    "            )\n",
    "        ]),\n",
    "            widgets.HTML(\"<div style='margin: 0 20px; line-height: 100px;'><b>OR</b></div>\"),\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(\"<div style='font-weight: bold; margin-bottom: 5px;'>Option 2: Search Database</div>\"),\n",
    "                widgets.HBox([\n",
    "                    self.threshold_dropdown,\n",
    "                    self.search_button,\n",
    "                    create_help_icon(\"Search peptides against the MBPDB (optional)\")\n",
    "                ], layout=widgets.Layout(align_items='center'))\n",
    "            ])\n",
    "        ], layout=widgets.Layout(align_items='center', margin='0'))\n",
    "\n",
    "        # Create peptide file uploader box with example link\n",
    "        peptide_box = widgets.HBox([\n",
    "            create_labeled_uploader(\n",
    "                self.pd_uploader,\n",
    "                \"Peptidomic File\",\n",
    "                \"Upload peptide groups data from Proteome Discover export file (required)\"\n",
    "            ),\n",
    "            self.create_download_link(\n",
    "                \"example_peptide_data.csv\",\n",
    "                \"Example\"\n",
    "            )\n",
    "        ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        # Create FASTA uploader box with example link\n",
    "        fasta_box = widgets.HBox([\n",
    "            create_labeled_uploader(\n",
    "                self.fasta_uploader,\n",
    "                \"FASTA Files\",\n",
    "                \"Upload Protein FASTA file used in Proteome Discoverer Search (optional)\"\n",
    "            ),\n",
    "            self.create_download_link(\n",
    "                \"example_fasta.fasta\",\n",
    "                \"Example\"\n",
    "            )\n",
    "        ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        # Create main container\n",
    "        main_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Upload Peptidomic Data Files:</u></h3>\"),\n",
    "            peptide_box,\n",
    "            widgets.HTML(\"<h3 style='margin-bottom: 0;'><u>MBPDB Data (Optional):</u></h3>\"),\n",
    "            mbpdb_options,\n",
    "            widgets.HTML(\"<h3><u>Upload Protein FASTA Files (Optional):</u></h3>\"),\n",
    "            fasta_box,\n",
    "            widgets.HTML(\"<br>\"),\n",
    "            widgets.HBox([\n",
    "                self.reset_button,\n",
    "                create_help_icon(\"Reset all uploaded files\")\n",
    "            ], layout=widgets.Layout(align_items='center')),\n",
    "            widgets.HTML(\"<div style='margin-top: 10px;'></div>\"),\n",
    "            self.output_area,\n",
    "            self.search_output_area\n",
    "        ])\n",
    "\n",
    "        # Register observers\n",
    "        self.pd_uploader.observe(self._on_pd_upload_change, names='value')\n",
    "        self.mbpdb_uploader.observe(self._on_mbpdb_upload_change, names='value')\n",
    "        self.fasta_uploader.observe(self._on_fasta_upload_change, names='value')\n",
    "        self.reset_button.on_click(self._reset_ui)\n",
    "        self.search_button.on_click(self._on_search_click)\n",
    "\n",
    "        # Add Font Awesome CSS for help icons\n",
    "        display(widgets.HTML(\"\"\"\n",
    "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\">\n",
    "        \"\"\"))\n",
    "\n",
    "        display(main_container)\n",
    "    \n",
    "    def _extract_sequences(self, df):\n",
    "        \"\"\"Extract sequences from peptidomic data\"\"\"\n",
    "        if 'Sequence' not in df.columns:\n",
    "            # First create Sequence column with NaN values\n",
    "            df['Sequence'] = pd.NA\n",
    "            \n",
    "            def extract_sequence(annotated_seq):\n",
    "                if pd.isna(annotated_seq):\n",
    "                    return pd.NA\n",
    "                \n",
    "                # Split by comma if present to handle multiple sequences\n",
    "                if ',' in annotated_seq:\n",
    "                    sequences = []\n",
    "                    for seq in annotated_seq.split(','):\n",
    "                        seq = seq.strip()\n",
    "                        # Handle [X].SEQUENCE.[X] format\n",
    "                        if '.' in seq:\n",
    "                            parts = seq.split('.')\n",
    "                            if len(parts) > 1:\n",
    "                                sequences.append(parts[1])\n",
    "                        # Handle plain sequence\n",
    "                        else:\n",
    "                            sequences.append(seq)\n",
    "                    return sequences\n",
    "                \n",
    "                # Single sequence case\n",
    "                # Handle [X].SEQUENCE.[X] format\n",
    "                if '.' in annotated_seq:\n",
    "                    parts = annotated_seq.split('.')\n",
    "                    if len(parts) > 1:\n",
    "                        return parts[1]\n",
    "                \n",
    "                # Handle plain sequence\n",
    "                return annotated_seq\n",
    "            \n",
    "            # Apply the extraction function and explode the results\n",
    "            df['Sequence'] = df['Annotated Sequence'].apply(extract_sequence)\n",
    "            # Explode sequences if they're in a list (from comma separation)\n",
    "            df = df.explode('Sequence')\n",
    "            \n",
    "        return df['Sequence'].dropna().unique().tolist()\n",
    "        \n",
    "    def _reset_ui(self, b):\n",
    "        \"\"\"Reset the UI state\"\"\"\n",
    "        self.mbpdb_uploader._counter = 0\n",
    "        self.pd_uploader._counter = 0\n",
    "        self.fasta_uploader._counter = 0\n",
    "        self.mbpdb_uploader.value = ()\n",
    "        self.pd_uploader.value = ()\n",
    "        self.fasta_uploader.value = ()\n",
    "        self.pd_results = pd.DataFrame()\n",
    "        self.mbpdb_results = pd.DataFrame()\n",
    "        self.search_results = pd.DataFrame()\n",
    "        self.proteins_dic = parse_headers()\n",
    "\n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "            display(HTML('<b style=\"color:blue;\">All uploads cleared.</b>'))\n",
    "\n",
    "        with self.search_output_area:\n",
    "            clear_output()\n",
    "            display(HTML('<b style=\"color:blue;\">Search results cleared.</b>'))\n",
    "\n",
    "    def _on_pd_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    self.pd_results, pd_status = self._load_data(\n",
    "                        file_data,\n",
    "                        required_columns=['Positions in Proteins'],\n",
    "                        file_type='Peptidomic'\n",
    "                    )\n",
    "                    if pd_status == 'yes' and self.pd_results is not None:\n",
    "                        display(HTML(\n",
    "                            f'<b style=\"color:green;\">Peptidomic data imported with {self.pd_results.shape[0]} rows and {self.pd_results.shape[1]} columns.</b>'))\n",
    "\n",
    "    def _on_mbpdb_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    self.mbpdb_results, mbpdb_status = self._load_data(\n",
    "                        file_data,\n",
    "                        required_columns=['Search peptide', 'Protein ID', 'Peptide'],\n",
    "                        file_type='MBPDB'\n",
    "                    )\n",
    "                    if mbpdb_status == 'yes' and self.mbpdb_results is not None:\n",
    "                        self.mbpdb_results.rename(columns={\n",
    "                            'Search peptide': 'search_peptide',\n",
    "                            'Protein ID': 'protein_id',\n",
    "                            'Peptide': 'peptide',\n",
    "                            'Protein description': 'protein_description',\n",
    "                            'Species': 'species',\n",
    "                            'Intervals': 'intervals',\n",
    "                            'Function': 'function',\n",
    "                            'Additional details': 'additional_details',\n",
    "                            'IC50 (Î¼M)': 'ic50',\n",
    "                            'Inhibition type': 'inhibition_type',\n",
    "                            'Inhibited microorganisms': 'inhibited_microorganisms',\n",
    "                            'PTM': 'ptm',\n",
    "                            'Title': 'title',\n",
    "                            'Authors': 'authors',\n",
    "                            'Abstract': 'abstract',\n",
    "                            'DOI': 'doi',\n",
    "                            'Search type': 'search_type',\n",
    "                            'Scoring matrix': 'scoring_matrix',\n",
    "                        }, inplace=True)\n",
    "                        display(HTML(\n",
    "                            f'<b style=\"color:green;\">MBPDB file imported with {self.mbpdb_results.shape[0]} rows and {self.mbpdb_results.shape[1]} columns</b>'))\n",
    "\n",
    "    def _on_fasta_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            new_proteins = {}\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    for file_data in change['new']:\n",
    "                        try:\n",
    "                            if file_data.name.endswith('.fasta'):\n",
    "                                parsed = self._parse_uploaded_fasta(file_data)\n",
    "                                new_proteins.update(parsed)\n",
    "                                print(f\"Parsed {len(parsed)} proteins from {file_data.name}\")\n",
    "                                display(HTML(f'<b style=\"color:green;\">Successfully imported {file_data.name}</b>'))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error: {str(e)}\")\n",
    "\n",
    "                    # Update proteins_dic with new data\n",
    "                    self.proteins_dic = new_proteins\n",
    "                    print(f\"Updated proteins_dic with {len(new_proteins)} entries\")\n",
    "\n",
    "    def _load_data(self, file_obj, required_columns, file_type):\n",
    "        \"\"\"\n",
    "        Load and validate uploaded data files, cleaning empty rows and validating data.\n",
    "        \n",
    "        Args:\n",
    "            file_obj: Uploaded file object\n",
    "            required_columns (list): List of required column names (either single names or pairs)\n",
    "            file_type (str): Type of file being loaded ('MBPDB' or 'Peptidomic')\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (DataFrame or None, status string 'yes'/'no')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            content = file_obj.content\n",
    "            filename = file_obj.name\n",
    "            extension = filename.split('.')[-1].lower()\n",
    "            \n",
    "            file_stream = io.BytesIO(content)\n",
    "            \n",
    "            # Load data based on file extension\n",
    "            if extension == 'csv':\n",
    "                df = pd.read_csv(file_stream)\n",
    "            elif extension in ['txt', 'tsv']:\n",
    "                df = pd.read_csv(file_stream, delimiter='\\t')\n",
    "            elif extension == 'xlsx':\n",
    "                df = pd.read_excel(file_stream)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Please upload .csv, .txt, .tsv, or .xlsx files.\")\n",
    "            \n",
    "            # Clean column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Drop empty rows\n",
    "            df = df.dropna(how='all')\n",
    "            df = df[~(df.astype(str).apply(lambda x: x.str.strip().eq('')).all(axis=1))]\n",
    "            \n",
    "            # Handle validation differently based on file type\n",
    "            if file_type == 'MBPDB':\n",
    "                # Use column pairs for MBPDB validation\n",
    "                column_pairs = {\n",
    "                    'Search peptide': 'search_peptide',\n",
    "                    'Protein ID': 'protein_id',\n",
    "                    'Peptide': 'peptide'\n",
    "                }\n",
    "                \n",
    "                # Check for required columns in either format\n",
    "                missing_pairs = []\n",
    "                for orig_col, std_col in column_pairs.items():\n",
    "                    if not (orig_col in df.columns or std_col in df.columns):\n",
    "                        missing_pairs.append(f\"'{orig_col}' or '{std_col}'\")\n",
    "                \n",
    "                if missing_pairs:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Missing required columns: {\", \".join(missing_pairs)}</b>'))\n",
    "                    return None, 'no'\n",
    "                \n",
    "                # Validate non-empty required columns\n",
    "                empty_pairs = []\n",
    "                for orig_col, std_col in column_pairs.items():\n",
    "                    col_to_check = orig_col if orig_col in df.columns else std_col\n",
    "                    if df[col_to_check].isna().all() or (df[col_to_check].astype(str).str.strip() == '').all():\n",
    "                        empty_pairs.append(f\"'{orig_col}' or '{std_col}'\")\n",
    "                \n",
    "                if empty_pairs:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Required columns are empty: {\", \".join(empty_pairs)}</b>'))\n",
    "                    return None, 'no'\n",
    "                    \n",
    "            else:\n",
    "                # Standard validation for other file types\n",
    "                if not set(required_columns).issubset(df.columns):\n",
    "                    missing = set(required_columns) - set(df.columns)\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Missing required columns: {\", \".join(missing)}</b>'))\n",
    "                    return None, 'no'\n",
    "                \n",
    "                # Validate non-empty required columns\n",
    "                empty_required = []\n",
    "                for col in required_columns:\n",
    "                    if df[col].isna().all() or (df[col].astype(str).str.strip() == '').all():\n",
    "                        empty_required.append(col)\n",
    "                \n",
    "                if empty_required:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Required columns are empty: {\", \".join(empty_required)}</b>'))\n",
    "                    return None, 'no'\n",
    "            \n",
    "            # Show success message\n",
    "            display(HTML(f'<b style=\"color:green;\">{file_type} file loaded successfully with {len(df)} rows after cleaning.</b>'))\n",
    "            \n",
    "            return df, 'yes'\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(HTML(f'<b style=\"color:red;\">{file_type} File Error: {str(e)}</b>'))\n",
    "            return None, 'no'\n",
    "\n",
    "    def _parse_uploaded_fasta(self, file_data):\n",
    "        \"\"\"Parse uploaded FASTA file content\"\"\"\n",
    "        fasta_dict = {}\n",
    "        fasta_text = bytes(file_data.content).decode('utf-8')\n",
    "        lines = fasta_text.split('\\n')\n",
    "\n",
    "        protein_id = \"\"\n",
    "        protein_name = \"\"\n",
    "        sequence = \"\"\n",
    "        species = \"\"\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if protein_id:\n",
    "                    fasta_dict[protein_id] = {\n",
    "                        \"name\": protein_name,\n",
    "                        \"sequence\": sequence,\n",
    "                        \"species\": species\n",
    "                    }\n",
    "                sequence = \"\"\n",
    "                header_parts = line[1:].split('|')\n",
    "                if len(header_parts) > 2:\n",
    "                    protein_id = header_parts[1]\n",
    "                    protein_name_full = re.split(r' OS=', header_parts[2])[0]\n",
    "                    if ' ' in protein_name_full:\n",
    "                        protein_name = protein_name_full\n",
    "                    else:\n",
    "                        protein_name = protein_name_full\n",
    "                    species = self._find_species(line)\n",
    "            else:\n",
    "                sequence += line\n",
    "\n",
    "        if protein_id:\n",
    "            fasta_dict[protein_id] = {\n",
    "                \"name\": protein_name,\n",
    "                \"sequence\": sequence,\n",
    "                \"species\": species\n",
    "            }\n",
    "\n",
    "        return fasta_dict\n",
    "\n",
    "    def _find_species(self, header):\n",
    "        \"\"\"Find species in FASTA header\"\"\"\n",
    "        header_lower = header.lower()\n",
    "        for spec_group in spec_translate_list:\n",
    "            for term in spec_group[1:]:\n",
    "                if term.lower() in header_lower:\n",
    "                    return spec_group[0]\n",
    "        return \"unknown\"\n",
    "\n",
    "    # Then to use it, we can create an observe function:\n",
    "    def observe_data_changes(change):\n",
    "        if hasattr(change, 'new'):\n",
    "            combiner.update_data(data_transformer.pd_results, data_transformer.mbpdb_results)\n",
    "            setup_data.update_data(data_transformer.pd_results)  # , data_transformer.pd_results_cleaned)\n",
    "\n",
    "    # Add this to DataTransformation class:\n",
    "    def attach_observers(self, group_processor):\n",
    "        \"\"\"\n",
    "        Attach observers to monitor changes in pd_results #and pd_results_cleaned\n",
    "\n",
    "        Args:\n",
    "            group_processor: Instance of GroupProcessing class\n",
    "        \"\"\"\n",
    "\n",
    "        def observe_data_changes(change):\n",
    "            if change.name in ['pd_results']:  # , 'pd_results_cleaned']:\n",
    "                group_processor.update_data(self.pd_results)  # , self.pd_results_cleaned)\n",
    "\n",
    "        self.observe(observe_data_changes, names=['pd_results'])  # , 'pd_results_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0ca14a-1665-43ec-abde-8adb7fc5c835",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GroupProcessing:\n",
    "    def __init__(self):\n",
    "        self.group_data = {}\n",
    "        self.group_number = 1\n",
    "        self.filtered_columns = []\n",
    "        self.group_uploader = widgets.FileUpload(\n",
    "        accept='.json',\n",
    "        multiple=False,\n",
    "        description='Upload Groups File',\n",
    "        layout=widgets.Layout(width='300px'),\n",
    "        style={'description_width': 'initial'}\n",
    "        )\n",
    "        self.group_uploader.observe(self._on_group_upload_change, names='value')\n",
    "        \n",
    "        # Initialize output areas\n",
    "        self.output = widgets.Output()\n",
    "        self.gd_output_area = widgets.Output()\n",
    "        \n",
    "        # Initialize widgets for group selection\n",
    "        self.column_dropdown = widgets.SelectMultiple(\n",
    "            description='Absorbance',\n",
    "            style={'description_width': 'initial'},\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width='90%', height='300px')\n",
    "        )\n",
    "        \n",
    "        self.grouping_variable_text = widgets.Text(\n",
    "            description='Group Name',\n",
    "            layout=widgets.Layout(width='90%'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Initialize buttons\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 0')\n",
    "        )\n",
    "        \n",
    "        self.add_group_button = widgets.Button(\n",
    "            description='Add Group',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 0')\n",
    "        )\n",
    "        \n",
    "        self.reset_file_button = widgets.Button(\n",
    "            description='Reset Selection',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(margin='10px 10px 0 75px')\n",
    "        )\n",
    "        \n",
    "        # Set up button callbacks\n",
    "        self.search_button.on_click(self._search_columns)\n",
    "        self.add_group_button.on_click(self._add_group)\n",
    "        self.reset_file_button.on_click(self._reset_selection)\n",
    "        \n",
    "\n",
    "    def update_data(self, pd_results):\n",
    "        \"\"\"Update data and refresh filtered columns\"\"\"\n",
    "        self.pd_results = pd_results\n",
    "        \n",
    "        # Only update if we have valid data\n",
    "        if pd_results is not None:\n",
    "            self.setup_data()\n",
    "            \n",
    "            # Update the dropdown with new filtered columns\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:green;\">Data updated successfully. Column selection refreshed.</b>'))\n",
    "    \n",
    "            \n",
    "    def setup_data(self):\n",
    "        \"\"\"Initialize data and filters for the analysis\"\"\"\n",
    "        # Define columns to exclude with more flexible matching\n",
    "        columns_to_exclude = [\n",
    "            'Marked as', 'Number of Missed Cleavages', 'Missed Cleavages',\n",
    "            'Checked', 'Confidence', 'Annotated Sequence', 'Unnamed: 3', \n",
    "            'Modifications', 'Protein Groups', 'Proteins', 'PSMs', \n",
    "            'Master Protein Accessions', 'Positions in Proteins', \n",
    "            'Modifications in Proteins',\n",
    "            'Theo MHplus in Da', 'Quan Info', \n",
    "            'Confidence by Search Engine', \n",
    "            'q-Value by Search Engine',\n",
    "            'PEP by Search Engine',\n",
    "            'SVM Score by Search Engine',\n",
    "            'XCorr by Search Engine',\n",
    "            'PEP', 'q-Value', 'Top Apex RT', 'RT in min',\n",
    "            'Sequence', 'search_peptide', 'Peptide', 'protein_id', \n",
    "            'protein_description', 'Alignment', 'Species', \n",
    "            'Intervals', 'function', 'unique ID'\n",
    "            ]\n",
    "        \n",
    "        exclude_substrings = [\n",
    "            'Abundances by Bio Rep', \n",
    "            'Count', \n",
    "            'Origin',\n",
    "            'Average_Abundance',\n",
    "            'Avg_',\n",
    "            'SEM_'\n",
    "        ]\n",
    "    \n",
    "        # Use cleaned data if available, otherwise use original\n",
    "        df = self.pd_results_cleaned if (hasattr(self, 'pd_results_cleaned') and \n",
    "                                       not self.pd_results_cleaned.empty) else self.pd_results\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            # More flexible column filtering\n",
    "            self.filtered_columns = []\n",
    "            for col in df.columns:\n",
    "                # Check if any exclusion pattern matches the column name\n",
    "                should_exclude = any(excl.lower() in col.lower() for excl in columns_to_exclude)\n",
    "                # Check if any substring pattern matches\n",
    "                has_excluded_substring = any(sub.lower() in col.lower() for sub in exclude_substrings)\n",
    "                \n",
    "                if not should_exclude and not has_excluded_substring:\n",
    "                    self.filtered_columns.append(col)\n",
    "              \n",
    "            # Update dropdown options\n",
    "            self.column_dropdown.options = self.filtered_columns\n",
    "            self._reset_inputs()\n",
    "        else:\n",
    "            self.filtered_columns = []\n",
    "            self.column_dropdown.options = []\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:red;\">No valid data available for processing.</b>'))\n",
    "   \n",
    "    def create_download_link(self, file_path, label):\n",
    "        \"\"\"Create a download link for a file.\"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            # Read file content and encode it as base64\n",
    "            with open(file_path, 'rb') as f:\n",
    "                content = f.read()\n",
    "            b64_content = base64.b64encode(content).decode('utf-8')\n",
    "    \n",
    "            # Generate the download link HTML\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <a download=\"{os.path.basename(file_path)}\" \n",
    "                   href=\"data:application/octet-stream;base64,{b64_content}\" \n",
    "                   style=\"color: #0366d6; text-decoration: none; margin-left: 20px; font-size: 14px;\">\n",
    "                    {label}\n",
    "                </a>\n",
    "            \"\"\")\n",
    "        else:\n",
    "            # Show an error message if the file does not exist\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <span style=\"color: red; margin-left: 20px; font-size: 14px;\">\n",
    "                    File \"{file_path}\" not found!\n",
    "                </span>\n",
    "            \"\"\")\n",
    "            \n",
    "    def display_group_selector(self):\n",
    "        \"\"\"Display the JSON file selector for group dictionaries\"\"\"\n",
    "        group_box = widgets.HBox([\n",
    "            self.group_uploader,\n",
    "            self.create_download_link(\"example_group_definition.json\", \"Example\")\n",
    "        ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        display(widgets.HTML(\"<h3><u>Upload Existing Group Dictionary:</u></h3>\"))\n",
    "        display(group_box, self.gd_output_area)\n",
    "        \n",
    "\n",
    "    def display_widgets(self):\n",
    "        \"\"\"Display the main UI for group selection\"\"\"\n",
    "        # Create main grid container\n",
    "        grid = widgets.GridspecLayout(1, 2,  # Number of rows and columns\n",
    "            width='1000px', \n",
    "            grid_gap='5px',  # Adjust spacing between grid elements\n",
    "        )\n",
    "        \n",
    "        # Create input container with vertical scroll\n",
    "        input_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Select New Grouping of Data:</u></h3>\"),\n",
    "            widgets.HTML('Now select the <b>absorbance columns</b> and assign the name of the <b>grouping variable</b>:'),\n",
    "            self.column_dropdown,\n",
    "            self.grouping_variable_text,\n",
    "            # Create button layouts\n",
    "            widgets.HBox([self.search_button, self.add_group_button]),\n",
    "            widgets.HBox([self.reset_file_button])\n",
    "        ], layout=widgets.Layout(\n",
    "            width='95%',\n",
    "            height='600px',\n",
    "            overflow_y='auto'  # Add vertical scroll\n",
    "        ))\n",
    "        \n",
    "        # Create output container with vertical scroll\n",
    "        output_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Group Selection Results:</u></h3>\"),\n",
    "            self.output\n",
    "        ], layout=widgets.Layout(\n",
    "            width='95%',\n",
    "            height='600px',\n",
    "            overflow_y='auto',  # Add vertical scroll\n",
    "            padding='10px'\n",
    "        ))\n",
    "        \n",
    "        # Add to grid\n",
    "        grid[0, 0] = input_container  # Left column\n",
    "        grid[0, 1] = output_container  # Right column\n",
    "        \n",
    "        display(grid)\n",
    "    def _on_gd_submit(self, b, dropdown):\n",
    "        \"\"\"Handle JSON file submission\"\"\"\n",
    "        selected_file = dropdown.value\n",
    "        with self.gd_output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            if selected_file == 'Select an existing grouping dictionary file':\n",
    "                print(\"Please select a valid file.\")\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                # Load and process JSON file\n",
    "                with open(selected_file, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                self.group_data = {}\n",
    "                \n",
    "                # Process groups\n",
    "                with self.output:\n",
    "                    clear_output()\n",
    "                    for group_number, group_info in data.items():\n",
    "                        group_name = group_info.get('grouping_variable')\n",
    "                        selected_columns = group_info.get('abundance_columns')\n",
    "                        \n",
    "                        self.group_data[group_number] = {\n",
    "                            'grouping_variable': group_name,\n",
    "                            'abundance_columns': selected_columns\n",
    "                        }\n",
    "                        \n",
    "                        display(widgets.HTML(\n",
    "                            f\"<b>Group {group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"\n",
    "                        ))\n",
    "                        display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "                        display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "                        display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "                        \n",
    "                display(widgets.HTML(f'<b style=\"color:green;\">Successfully uploaded: {selected_file}</b>'))\n",
    "                \n",
    "            except Exception as e:\n",
    "                display(widgets.HTML(f\"<b style='color:red;'>An error occurred while processing the file: {str(e)}</b>\"))\n",
    "    \n",
    "    def _search_columns(self, b):\n",
    "        \"\"\"Search for columns based on group name\"\"\"\n",
    "        group_name = self.grouping_variable_text.value\n",
    "        if group_name:\n",
    "            matching_columns = [col for col in self.filtered_columns if group_name in col]\n",
    "            self.column_dropdown.value = matching_columns\n",
    "        else:\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:red;\">Please enter a group name to search.</b>'))\n",
    "    \n",
    "    def _add_group(self, b):\n",
    "        \"\"\"Add a new group to the data\"\"\"\n",
    "        group_name = self.grouping_variable_text.value\n",
    "        selected_columns = list(self.column_dropdown.value)\n",
    "        \n",
    "        if not (group_name and selected_columns):\n",
    "            with self.output:\n",
    "                display(widgets.HTML('<b style=\"color:red;\">Please enter a group name and select at least one column.</b>'))\n",
    "            return\n",
    "        \n",
    "        # If group_data exists, use next number, otherwise start at 1\n",
    "        if self.group_data:\n",
    "            # Convert existing keys to integers and find max\n",
    "            existing_numbers = [int(k) for k in self.group_data.keys()]\n",
    "            next_number = max(existing_numbers) + 1\n",
    "            self.group_number = str(next_number)\n",
    "        else:\n",
    "            self.group_data = {}\n",
    "            self.group_number = \"1\"\n",
    "        \n",
    "        # Add new group data to the dictionary\n",
    "        self.group_data[self.group_number] = {\n",
    "            'grouping_variable': group_name,\n",
    "            'abundance_columns': selected_columns\n",
    "        }\n",
    "        \n",
    "        # Display output\n",
    "        with self.output:\n",
    "            display(widgets.HTML(f\"<b>Group {self.group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"))\n",
    "            display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "            display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "            display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "        \n",
    "        self._reset_inputs()\n",
    "        \n",
    "    def _reset_selection(self, b):\n",
    "        \"\"\"Reset all selections and data\"\"\"\n",
    "        self.group_data = {}\n",
    "        self.group_number = 1\n",
    "        with self.gd_output_area:\n",
    "            clear_output()\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "        self._reset_inputs()\n",
    "    \n",
    "    def _reset_inputs(self):\n",
    "        \"\"\"Reset input fields\"\"\"\n",
    "        self.grouping_variable_text.value = ''\n",
    "        self.column_dropdown.value = ()\n",
    "\n",
    "    def _on_group_upload_change(self, change):\n",
    "        \"\"\"Handle JSON file upload\"\"\"\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.gd_output_area:\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    try:\n",
    "                        content = bytes(file_data.content).decode('utf-8')\n",
    "                        data = json.loads(content)\n",
    "                        \n",
    "                        # Process groups\n",
    "                        with self.output:\n",
    "                            for group_number, group_info in data.items():\n",
    "                                group_name = group_info.get('grouping_variable')\n",
    "                                selected_columns = group_info.get('abundance_columns')\n",
    "                                \n",
    "                                # Update group_data without clearing previous entries\n",
    "                                self.group_data[group_number] = {\n",
    "                                    'grouping_variable': group_name,\n",
    "                                    'abundance_columns': selected_columns\n",
    "                                }\n",
    "                                \n",
    "                                display(widgets.HTML(\n",
    "                                    f\"<b>Group {group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"\n",
    "                                ))\n",
    "                                display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "                                display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "                                display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "                                \n",
    "                        display(widgets.HTML(f'<b style=\"color:green;\">Successfully uploaded: {file_data.name}</b>'))\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        display(widgets.HTML(f\"<b style='color:red;'>An error occurred while processing the file: {str(e)}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7fd494-b273-4018-8b3a-9e0e31867530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinCombinationHandler(HasTraits):\n",
    "    def __init__(self, data_transformer):\n",
    "        super().__init__()\n",
    "        self.data_transformer = data_transformer  # Store reference to data_transformer\n",
    "        self.pd_results = data_transformer.pd_results\n",
    "        self.pd_results_cleaned = None\n",
    "        self.protein_output_area = None\n",
    "        self.user_decisions = {}\n",
    "        self.decision_inputs = []\n",
    "        self.multi_position_combinations = []\n",
    "        self.submit_button = None\n",
    "        self.reset_button = None\n",
    "        self.progress = None\n",
    "\n",
    "    @property  # Make proteins_dic a property that always reads from data_transformer\n",
    "    def proteins_dic(self):\n",
    "        return self.data_transformer.proteins_dic\n",
    "\n",
    "    def _get_protein_combinations(self):\n",
    "        \"\"\"Extract unique protein combinations from the dataset with NaN handling\"\"\"\n",
    "        if self.pd_results is None or self.pd_results.empty:\n",
    "            return []\n",
    "\n",
    "        protein_combinations = set()\n",
    "        nan_warnings = {\n",
    "            'positions': 0,\n",
    "            'master_acc': 0,\n",
    "            'unknown_added': 0\n",
    "        }\n",
    "\n",
    "        # Create a working copy of the dataframe\n",
    "        working_df = self.pd_results.copy()\n",
    "\n",
    "        # Track NaN counts before modification\n",
    "        nan_warnings['positions'] = working_df['Positions in Proteins'].isna().sum()\n",
    "        nan_warnings['master_acc'] = working_df['Master Protein Accessions'].isna().sum()\n",
    "\n",
    "        # Replace NaN values with \"Unknown\" instead of dropping\n",
    "        working_df['Positions in Proteins'] = working_df['Positions in Proteins'].fillna('Unknown')\n",
    "        working_df['Master Protein Accessions'] = working_df['Master Protein Accessions'].fillna('Unknown')\n",
    "\n",
    "        for _, row in working_df.iterrows():\n",
    "            try:\n",
    "                # Handle \"Unknown\" case specially\n",
    "                if row['Positions in Proteins'] == 'Unknown':\n",
    "                    position_proteins = ['Unknown']\n",
    "                else:\n",
    "                    position_proteins = [p.split()[0] for p in row['Positions in Proteins'].split('; ')]\n",
    "\n",
    "                master_acc = row['Master Protein Accessions']\n",
    "\n",
    "                # Check species of proteins in Positions in Proteins\n",
    "                species_set = set()\n",
    "                for protein in position_proteins:\n",
    "                    if protein in self.proteins_dic:\n",
    "                        species_set.add(self.proteins_dic[protein]['species'])\n",
    "                    elif protein == 'Unknown':\n",
    "                        species_set.add('Unknown')\n",
    "\n",
    "                if (';' in master_acc or\n",
    "                        ';' in row['Positions in Proteins'] or\n",
    "                        len(species_set) > 1 or\n",
    "                        'Unknown' in species_set):  # Include Unknown combinations\n",
    "                    protein_combinations.add('; '.join(sorted(position_proteins)))\n",
    "                    if 'Unknown' in position_proteins:\n",
    "                        nan_warnings['unknown_added'] += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing row {_}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Store warning message for display\n",
    "        warning_message = []\n",
    "\n",
    "        if nan_warnings['positions'] > 0:\n",
    "            warning_message.append(\n",
    "                f\"{nan_warnings['positions']} rows with missing 'Positions in Proteins' were marked as Unknown\")\n",
    "        if nan_warnings['master_acc'] > 0:\n",
    "            warning_message.append(\n",
    "                f\"{nan_warnings['master_acc']} rows with missing 'Master Protein Accessions' were marked as Unknown\")\n",
    "        if nan_warnings['unknown_added'] > 0:\n",
    "            warning_message.append(f\"{nan_warnings['unknown_added']} combinations now include Unknown proteins\")\n",
    "\n",
    "        # if warning_message:\n",
    "        #   print(\"Warning: \" + \"; \".join(warning_message))\n",
    "\n",
    "        self.multi_position_combinations = list(protein_combinations)\n",
    "        return self.multi_position_combinations\n",
    "\n",
    "    def handle_combinations(self):\n",
    "        \"\"\"Main method to handle protein combinations\"\"\"\n",
    "        if self.pd_results is None or self.pd_results.empty:\n",
    "            return None\n",
    "\n",
    "        if self.protein_output_area is None:\n",
    "            self.protein_output_area = widgets.Output()\n",
    "\n",
    "        choice = widgets.RadioButtons(\n",
    "            options=[('Yes', True), ('No', False)],\n",
    "            description='Process peptides mapped to multiple proteins?',\n",
    "            style={'description_width': 'initial'},\n",
    "            value=None\n",
    "        )\n",
    "\n",
    "        output = widgets.Output()\n",
    "\n",
    "        def process_choice(_):\n",
    "            with output:\n",
    "                clear_output()\n",
    "                if choice.value:\n",
    "                    self.pd_results_cleaned = self.process_protein_combinations()\n",
    "                    display(HTML(\"<b style='color:green;'>Processed peptides mapped to multiple proteins.</b>\"))\n",
    "                else:\n",
    "                    self.pd_results_cleaned = self.pd_results.copy()\n",
    "                    display(HTML(\"<b>Using original protein mappings.</b>\"))\n",
    "\n",
    "        choice.observe(process_choice, 'value')\n",
    "        display(choice)\n",
    "        display(output)\n",
    "\n",
    "        return self.pd_results_cleaned\n",
    "\n",
    "    def process_protein_combinations(self):\n",
    "        \"\"\"Process protein combinations in pd_results with Unknown handling\"\"\"\n",
    "        if not self.pd_results.empty:\n",
    "            df = self.pd_results.copy()\n",
    "\n",
    "            # Fill NaN values with \"Unknown\"\n",
    "            df['Positions in Proteins'] = df['Positions in Proteins'].fillna('Unknown')\n",
    "            df['Master Protein Accessions'] = df['Master Protein Accessions'].fillna('Unknown')\n",
    "\n",
    "            # Create warning display area\n",
    "            warning_area = widgets.HTML(\n",
    "                layout=widgets.Layout(\n",
    "                    margin='10px 0',\n",
    "                    padding='10px',\n",
    "                    border='1px solid #ffeeba',\n",
    "                    background_color='#fff3cd',\n",
    "                    border_radius='4px'\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Get combinations and track Unknown statistics\n",
    "            combinations = self._get_protein_combinations()\n",
    "\n",
    "            # Update warning area with statistics\n",
    "            unknown_positions = (df['Positions in Proteins'] == 'Unknown').sum()\n",
    "            unknown_master_acc = (df['Master Protein Accessions'] == 'Unknown').sum()\n",
    "\n",
    "            if unknown_positions > 0 or unknown_master_acc > 0:\n",
    "                warning_html = \"<div><b>â¹ï¸ Notice:</b><ul style='margin: 5px 0'>\"\n",
    "                if unknown_positions > 0:\n",
    "                    warning_html += f\"<li>{unknown_positions} rows with missing 'Positions in Proteins' are marked as Unknown</li>\"\n",
    "                if unknown_master_acc > 0:\n",
    "                    warning_html += f\"<li>{unknown_master_acc} rows with missing 'Master Protein Accessions' are marked as Unknown</li>\"\n",
    "                warning_html += \"</ul>These peptides will be preserved in the output.</div>\"\n",
    "                warning_area.value = warning_html\n",
    "            else:\n",
    "                warning_area = widgets.HTML(f\"<br>\")\n",
    "\n",
    "            # Main container with warning area\n",
    "            main_container = widgets.VBox([\n",
    "                warning_area,\n",
    "                widgets.HTML(\"\"\"\n",
    "                    <h3>Peptides Mapped to Multiple Proteins</h3>\n",
    "                    <div style='margin-bottom: 15px;'>\n",
    "                        Select how to handle each protein mapping combination in your dataset.\n",
    "                        These combinations come from either:\n",
    "                        <ul>\n",
    "                            <li>Multiple proteins in Master Protein Accessions</li>\n",
    "                            <li>Multiple proteins in Positions in Proteins</li>\n",
    "                            <li>Proteins from different species</li>\n",
    "                            <li>Unknown protein mappings (from missing values)</li>\n",
    "                        </ul>\n",
    "                    </div>\n",
    "                \"\"\")\n",
    "            ], layout=widgets.Layout(width='100%', padding='20px'))\n",
    "\n",
    "            # Get combinations\n",
    "            combinations = self._get_protein_combinations()\n",
    "\n",
    "            def create_help_icon(self, tooltip_text):\n",
    "                \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "                return f'<div title=\"{tooltip_text}\" style=\"display: inline-block; margin-left: 4px;\">' \\\n",
    "                       '<i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>' \\\n",
    "                       '</div>'\n",
    "\n",
    "            table_header = widgets.HTML(\"\"\"\n",
    "                            <div style=\"display: grid; grid-template-columns: 100px 100px 420px 200px auto; gap: 2px; margin-bottom: 10px; font-weight: bold; align-items: center;\">\n",
    "                                <div>\n",
    "                                    Protein ID\n",
    "                                    <span title=\"Unique identifier for the protein\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Species\n",
    "                                    <span title=\"Source organism of the protein\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Description\n",
    "                                    <span title=\"Full protein name or description\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Decision\n",
    "                                    <span title=\"Available options:\\n\n",
    "            - 'new' - Create a separate row for this protein\\n\n",
    "            - 'remove' - Remove this protein from combination\\n\n",
    "            - 'asis' - Keep as part of current combination\\n\n",
    "            - 'Custom: (protein ID)': ie. Custom: P02666A1\"\n",
    "            style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Status\n",
    "                                    <span title=\"Color indicators:\\n\n",
    "            - Grey - Default option (not yet submitted)\\n\n",
    "            - Green - Option has been submitted\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                            </div>\n",
    "                            <hr style=\"margin: 0 0 10px 0;\">\n",
    "                        \"\"\")\n",
    "\n",
    "            # Create input area\n",
    "            input_area = widgets.VBox([table_header],\n",
    "                                      layout=widgets.Layout(width='100%', margin='10px 0'))\n",
    "\n",
    "            # Add rows for each combination\n",
    "            self.decision_inputs = []\n",
    "            self.status_displays = {}\n",
    "\n",
    "            for combo_idx, combo in enumerate(combinations, 1):\n",
    "                proteins = combo.split('; ')\n",
    "\n",
    "                # Find rows with this combination\n",
    "                combo_rows = []\n",
    "                for _, row in df.iterrows():\n",
    "                    if pd.isna(row['Positions in Proteins']):\n",
    "                        continue\n",
    "                    row_proteins = set(p.split()[0] for p in row['Positions in Proteins'].split('; '))\n",
    "                    if row_proteins == set(proteins):\n",
    "                        combo_rows.append(row)\n",
    "\n",
    "                occurrences = len(combo_rows)\n",
    "\n",
    "                # Add combination header\n",
    "                input_area.children += (widgets.HTML(f\"\"\"\n",
    "                    <div style=\"background-color: #f8f9fa; padding: 2px; margin: 5px 0; border-radius: 5px;\">\n",
    "                        <b>Combination {combo_idx}</b> ({occurrences} occurrences)\n",
    "                    </div>\n",
    "                \"\"\"),)\n",
    "\n",
    "                # Process each protein in the combination\n",
    "                for protein in proteins:\n",
    "                    species = \"Unknown\" if protein == 'Unknown' else self.proteins_dic.get(protein, {}).get('species',\n",
    "                                                                                                            \"Unknown\")\n",
    "                    name = \"Unknown Protein\" if protein == 'Unknown' else self.proteins_dic.get(protein, {}).get('name',\n",
    "                                                                                                                 \"Unknown\")\n",
    "\n",
    "                    # Set default decision based on Master Protein Accessions\n",
    "                    default_decision = 'asis'  # Always keep Unknown proteins as-is\n",
    "                    if protein != 'Unknown' and combo_rows:\n",
    "                        first_row = combo_rows[0]\n",
    "                        if not pd.isna(first_row['Master Protein Accessions']):\n",
    "                            master_proteins = first_row['Master Protein Accessions'].split(';')\n",
    "                            master_proteins = [p.strip() for p in master_proteins]\n",
    "                            default_decision = 'new' if protein in master_proteins else 'remove'\n",
    "\n",
    "                    # Create decision input\n",
    "                    decision_input = widgets.Text(\n",
    "                        layout=widgets.Layout(width='125px'),\n",
    "                        value=default_decision\n",
    "                    )\n",
    "                    self.decision_inputs.append((combo, protein, decision_input))\n",
    "\n",
    "                    # Create status display with initial status\n",
    "                    status_text = {\n",
    "                        'new': \"Will be created as new row\",\n",
    "                        'remove': \"Will be removed\",\n",
    "                        'asis': \"Will keep as is\",\n",
    "                        'Custom: (protein ID)': \"ie. Custom: P02666A1\"\n",
    "                    }\n",
    "                    initial_status = status_text.get(default_decision, '')\n",
    "                    status_display = widgets.HTML(f'<span style=\"color: gray\">{initial_status}</span>')\n",
    "                    self.status_displays[(combo, protein)] = status_display\n",
    "\n",
    "                    # Create the row content\n",
    "                    row_content = widgets.HTML(f\"\"\"\n",
    "                    <div style=\"display: grid; grid-template-columns: 100px 100px 420px; gap: 2px; align-items: center;\">\n",
    "                            <div>{protein}</div>\n",
    "                            <div>{species}</div>\n",
    "                            <div>{name}</div>\n",
    "                        </div>\n",
    "                    \"\"\")\n",
    "\n",
    "                    # Create container with all elements\n",
    "                    container = widgets.HBox([\n",
    "                        row_content,\n",
    "                        widgets.HBox([decision_input], layout=widgets.Layout(width='150px', padding='0')),\n",
    "                        widgets.HBox([status_display], layout=widgets.Layout(width='200px', padding='0'))\n",
    "                    ], layout=widgets.Layout(\n",
    "                        margin='2px 0',\n",
    "                        display='flex',\n",
    "                        align_items='center',\n",
    "                        overflow='hidden',\n",
    "                        width='100%'\n",
    "                    ))\n",
    "\n",
    "                    input_area.children += (container,)\n",
    "\n",
    "            # Create buttons\n",
    "            button_box = self._create_buttons()\n",
    "\n",
    "            # Add output area\n",
    "            self.protein_output_area = widgets.Output(\n",
    "                layout=widgets.Layout(width='100%', margin='5px 0')\n",
    "            )\n",
    "\n",
    "            # Add all components\n",
    "            main_container.children += (input_area, button_box, self.protein_output_area)\n",
    "\n",
    "            self.pd_results_cleaned = df\n",
    "            display(main_container)\n",
    "            return df\n",
    "\n",
    "    \"\"\"\n",
    "    def _handle_remove_decision(self, df, index, row, positions, protein_to_remove):\n",
    "        # Simply drop the row\n",
    "        df = df.drop(index)\n",
    "        return df\n",
    "\n",
    "    def _handle_new_decision(self, df, index, row, positions):\n",
    "        # Create new rows for each protein's position\n",
    "        new_rows = []\n",
    "        for pos in positions:\n",
    "            new_row = row.copy()\n",
    "            new_row['Positions in Proteins'] = pos\n",
    "            protein_id = pos.split()[0]\n",
    "            new_row['Master Protein Accessions'] = protein_id\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "        # Remove the original row\n",
    "        df = df.drop(index)\n",
    "\n",
    "        # Add all new rows\n",
    "        if new_rows:\n",
    "            df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "        return df\n",
    "    \"\"\"\n",
    "\n",
    "    def _on_submit(self, button, df):\n",
    "        \"\"\"Handle submit button click\"\"\"\n",
    "        self.submit_button.disabled = True\n",
    "        self.reset_button.disabled = True\n",
    "        self.progress.value = 0\n",
    "\n",
    "        with self.protein_output_area:\n",
    "            try:\n",
    "                self.protein_output_area.clear_output()\n",
    "\n",
    "                # Process decisions by combination\n",
    "                decisions_by_combo = {}\n",
    "                rows_to_remove = set()\n",
    "                new_rows = []\n",
    "                total_inputs = len(self.decision_inputs)\n",
    "\n",
    "                # First pass: collect all decisions\n",
    "                for i, (combo, protein, input_widget) in enumerate(self.decision_inputs):\n",
    "                    decision = input_widget.value.strip().upper()\n",
    "                    if decision:\n",
    "                        # Update status\n",
    "                        status_display = self.status_displays[(combo, protein)]\n",
    "                        status_display.value = f'<span style=\"color: green\">Decision: {decision}</span>'\n",
    "\n",
    "                        # Store decision\n",
    "                        if combo not in decisions_by_combo:\n",
    "                            decisions_by_combo[combo] = {}\n",
    "                        decisions_by_combo[combo][protein] = decision\n",
    "\n",
    "                    # Update progress\n",
    "                    self.progress.value = ((i + 1) / total_inputs * 50)\n",
    "\n",
    "                # Second pass: process the dataframe based on decisions\n",
    "                if decisions_by_combo:\n",
    "                    processed_df = df.copy()\n",
    "                    processed_count = 0\n",
    "                    total_combinations = len(decisions_by_combo)\n",
    "\n",
    "                    for combo, protein_decisions in decisions_by_combo.items():\n",
    "                        proteins = combo.split('; ')\n",
    "\n",
    "                        # Create mask only on non-NaN rows\n",
    "                        valid_rows = processed_df['Positions in Proteins'].notna()\n",
    "                        pattern = ''.join(f'(?=.*{p})' for p in proteins)\n",
    "                        mask = processed_df['Positions in Proteins'].fillna('').str.contains(pattern, regex=True)\n",
    "                        mask = valid_rows & mask\n",
    "\n",
    "                        matched_indices = processed_df[mask].index\n",
    "                        for idx in matched_indices:\n",
    "                            row = processed_df.loc[idx]\n",
    "                            positions = row['Positions in Proteins'].split('; ')\n",
    "                            current_proteins = [p.split()[0] for p in positions]\n",
    "                            current_combo = '; '.join(sorted(current_proteins))\n",
    "\n",
    "                            if current_combo == combo:\n",
    "                                any_new_or_remove = False\n",
    "                                asis_updates = []\n",
    "\n",
    "                                for protein, decision in protein_decisions.items():\n",
    "                                    if decision in ['NEW', 'REMOVE']:\n",
    "                                        any_new_or_remove = True\n",
    "                                        break\n",
    "\n",
    "                                    elif decision == 'ASIS':\n",
    "                                        # For ASIS, keep the original protein accession and positions unmodified\n",
    "                                        # No need to make any changes or append to asis_updates\n",
    "                                        pass  # Skip any modifications\n",
    "\n",
    "                                if any_new_or_remove:\n",
    "                                    rows_to_remove.add(idx)\n",
    "\n",
    "                                    for protein, decision in protein_decisions.items():\n",
    "                                        if decision == 'NEW':\n",
    "                                            position = next(p for p in positions if protein in p)\n",
    "                                            new_row = row.copy()\n",
    "                                            new_row['Positions in Proteins'] = position\n",
    "                                            new_row['Master Protein Accessions'] = protein\n",
    "                                            new_rows.append(new_row)\n",
    "\n",
    "                                elif decision.startswith('CUSTOM:'):\n",
    "                                    # Handle custom protein ID\n",
    "                                    new_protein_id = decision.split(':')[1].strip()\n",
    "                                    positions = row['Positions in Proteins'].split('; ')\n",
    "                                    original_protein = protein  # Use the protein from the loop iteration\n",
    "    \n",
    "                                    # Find the position for the target protein\n",
    "                                    for pos in positions:\n",
    "                                        if pos.startswith(original_protein):\n",
    "                                            num_range = pos[pos.find('['):]  # Extract everything from '[' onwards\n",
    "                                            new_position = pos.replace(original_protein,\n",
    "                                                                       new_protein_id)  # Replace only the matching protein ID\n",
    "    \n",
    "                                            # Update the DataFrame\n",
    "                                            processed_df.at[idx, 'Master Protein Accessions'] = new_protein_id\n",
    "                                            processed_df.at[idx, 'Positions in Proteins'] = new_position\n",
    "                                            break\n",
    "\n",
    "                        processed_count += 1\n",
    "                        self.progress.value = 50 + (processed_count / total_combinations * 50)\n",
    "\n",
    "                    # Remove marked rows\n",
    "                    processed_df = processed_df.drop(index=list(rows_to_remove))\n",
    "\n",
    "                    # Add all new rows\n",
    "                    if new_rows:\n",
    "                        processed_df = pd.concat([processed_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "                    self.pd_results_cleaned = processed_df\n",
    "\n",
    "                display(HTML(\"<b style='color:green;'>Processing complete.</b>\"))\n",
    "\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red;'>Error: {str(e)}</b>\"))\n",
    "\n",
    "            finally:\n",
    "                self.submit_button.disabled = False\n",
    "                self.reset_button.disabled = False\n",
    "\n",
    "        return self.pd_results_cleaned\n",
    "\n",
    "    def create_help_icon(self, tooltip_text):\n",
    "        \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "        return widgets.HTML(\n",
    "            f'<div title=\"{tooltip_text}\" style=\"display: inline-block;\">'\n",
    "            '<i class=\"fa fa-question-circle\" style=\"color: #007bff;\"></i>'\n",
    "            '</div>'\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def _create_buttons(self):\n",
    "        \"\"\"Create submit and reset buttons\"\"\"\n",
    "        self.submit_button = widgets.Button(\n",
    "            description=\"Submit\",\n",
    "            button_style='success',\n",
    "            disabled=False\n",
    "        )\n",
    "        self.reset_button = widgets.Button(\n",
    "            description=\"Reset\",\n",
    "            button_style='warning',\n",
    "            disabled=False\n",
    "        )\n",
    "        self.progress = widgets.FloatProgress(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=100,\n",
    "            description='Processing:',\n",
    "            bar_style='info',\n",
    "            style={'bar_color': '#0080ff'},\n",
    "            orientation='horizontal',\n",
    "            layout=widgets.Layout(width='50%')\n",
    "        )\n",
    "\n",
    "        button_box = widgets.VBox([\n",
    "            widgets.HBox([self.submit_button, self.reset_button]),\n",
    "            self.progress\n",
    "        ])\n",
    "\n",
    "        self.reset_button.on_click(self._on_reset_button_clicked)\n",
    "        self.submit_button.on_click(lambda b: self._on_submit(b, self.pd_results.copy()))\n",
    "\n",
    "        return button_box\n",
    "\n",
    "    def _on_reset_button_clicked(self, b):\n",
    "        \"\"\"Handle reset button click by resetting options to default values\"\"\"\n",
    "        # Disable buttons during reset\n",
    "        self.submit_button.disabled = True\n",
    "        self.reset_button.disabled = True\n",
    "\n",
    "        # Clear output area\n",
    "        with self.protein_output_area:\n",
    "            self.protein_output_area.clear_output()\n",
    "            display(HTML(\"<b style='color:blue;'>Resetting options to defaults...</b>\"))\n",
    "\n",
    "        # Reset progress bar\n",
    "        self.progress.value = 0\n",
    "\n",
    "        try:\n",
    "            # Reset each input field to its default value based on Master Protein Accessions\n",
    "            df = self.pd_results.copy()\n",
    "            processed = 0\n",
    "            total_inputs = len(self.decision_inputs)\n",
    "\n",
    "            for combo, protein, input_field in self.decision_inputs:\n",
    "                # Find rows with this combination\n",
    "                proteins = combo.split('; ')\n",
    "                combo_rows = []\n",
    "                for _, row in df.iterrows():\n",
    "                    if pd.isna(row['Positions in Proteins']):\n",
    "                        continue\n",
    "                    row_proteins = set(p.split()[0] for p in row['Positions in Proteins'].split('; '))\n",
    "                    if row_proteins == set(proteins):\n",
    "                        combo_rows.append(row)\n",
    "\n",
    "                # Determine default decision\n",
    "                default_decision = 'asis'\n",
    "                if combo_rows:\n",
    "                    first_row = combo_rows[0]\n",
    "                    if not pd.isna(first_row['Master Protein Accessions']):\n",
    "                        master_proteins = first_row['Master Protein Accessions'].split(';')\n",
    "                        master_proteins = [p.strip() for p in master_proteins]\n",
    "                        default_decision = 'new' if protein in master_proteins else 'remove'\n",
    "\n",
    "                # Set input field value\n",
    "                input_field.value = default_decision\n",
    "\n",
    "                # Update status display\n",
    "                status_display = self.status_displays[(combo, protein)]\n",
    "                status_text = {\n",
    "                    'new': \"Will be created as new row\",\n",
    "                    'remove': \"Will be removed\",\n",
    "                    'asis': \"Will keep as is\",\n",
    "                    'Custom: (protein ID)': \"ie. Custom: P02666A1\"\n",
    "                }\n",
    "                status_display.value = f'<span style=\"color: gray\">{status_text[default_decision]}</span>'\n",
    "\n",
    "                # Update progress\n",
    "                processed += 1\n",
    "                self.progress.value = (processed / total_inputs) * 100\n",
    "\n",
    "            # Reset internal state\n",
    "            self.user_decisions = {}\n",
    "            self.pd_results_cleaned = self.pd_results.copy()\n",
    "\n",
    "            with self.protein_output_area:\n",
    "                self.protein_output_area.clear_output()\n",
    "                display(HTML(\"<b style='color:green;'>Reset complete. All options set to defaults.</b>\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            with self.protein_output_area:\n",
    "                self.protein_output_area.clear_output()\n",
    "                display(HTML(f\"<b style='color:red;'>Error during reset: {str(e)}</b>\"))\n",
    "\n",
    "        finally:\n",
    "            # Re-enable buttons\n",
    "            self.submit_button.disabled = False\n",
    "            self.reset_button.disabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b182afc-9770-4334-b931-7306109d5a35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd0dc24c8b24d029f727d918ed3050e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='\\n            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.1â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c554146b664bc6b4f4374e1fd11cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3><u>Upload Peptidomic Data Files:</u></h3>'), HBox(children=(HBox(children=(Fileâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e5476ac860443e8141a37048f274d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a25baba4d894cf0bfc9aa7387540c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3><u>Protein Mapping</u></h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54182ca3f0994063bdc5c46fcadb7e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67e97ace4fb4e33ad472a25ea4dbcd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<br>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8b4c657b6b4522b3d3ec31769868da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create global output areas\n",
    "protein_mapping_output = widgets.Output()\n",
    "group_processing_output = widgets.Output()\n",
    "\n",
    "class ProcessingWorkflow:\n",
    "    def __init__(self):\n",
    "        self.data_transformer = DataTransformation()\n",
    "        self.protein_handler = ProteinCombinationHandler(self.data_transformer)\n",
    "        self.group_processor = GroupProcessing()\n",
    "        \n",
    "        # Set up observers\n",
    "        self.data_transformer.observe(self._handle_data_change, names=['pd_results'])\n",
    "        self.data_transformer.observe(self._handle_fasta_change, names=['proteins_dic'])\n",
    "            \n",
    "    def _handle_data_change(self, change):\n",
    "        \"\"\"Handle changes in proteomics data\"\"\"\n",
    "        if change.name == 'pd_results':\n",
    "            with protein_mapping_output:\n",
    "                protein_mapping_output.clear_output()\n",
    "                if change.new is not None:\n",
    "                    display(HTML(\"<h3>Multiple Protein Mappings</h3>\"))\n",
    "                    self.protein_handler.pd_results = change.new\n",
    "                    self.protein_handler.handle_combinations()\n",
    "                else:\n",
    "                    display(HTML(\"<h3>Multiple Protein Mappings</h3>\"))\n",
    "                    display(HTML(\"<b style='color:orange;'>Waiting for proteomics data to be uploaded...</b>\"))\n",
    "            \n",
    "            self.group_processor.update_data(change.new)\n",
    "            \n",
    "    def _handle_fasta_change(self, change):\n",
    "        \"\"\"Handle changes in FASTA data\"\"\"\n",
    "        if change.new != change.old:\n",
    "            # No need to copy dictionary since we're using property access\n",
    "            with protein_mapping_output:\n",
    "                protein_mapping_output.clear_output()\n",
    "                display(HTML(\"<h3>Multiple Protein Mappings</h3>\"))\n",
    "                print(f\"Using updated protein dictionary with {len(self.data_transformer.proteins_dic)} proteins\")\n",
    "                if self.protein_handler.pd_results is not None:\n",
    "                    self.protein_handler.handle_combinations()\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the complete workflow interface\"\"\"\n",
    "        self.data_transformer.setup_data_loading_ui()\n",
    "        display(widgets.HTML(\"<br>\"))\n",
    "        display(widgets.HTML(\"<h3><u>Protein Mapping</u></h3>\"))\n",
    "        display(protein_mapping_output)\n",
    "        \n",
    "        with protein_mapping_output:\n",
    "            if self.data_transformer.pd_results is not None:\n",
    "                self.protein_handler.handle_combinations()\n",
    "        \n",
    "        display(widgets.HTML(\"<br>\"))\n",
    "        with group_processing_output:\n",
    "            self.group_processor.display_group_selector()\n",
    "            self.group_processor.display_widgets()\n",
    "        display(group_processing_output)\n",
    "\n",
    "# Initialize workflow\n",
    "workflow = ProcessingWorkflow()\n",
    "workflow.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f50828-45c1-444e-be98-4ed3ff7a947e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CombineAverageDataframes:\n",
    "    def __init__(self, data_transformer, group_processor, protein_handler):\n",
    "        self.data_transformer = data_transformer\n",
    "        self.group_processor = group_processor\n",
    "        self.pd_results = data_transformer.pd_results\n",
    "        self.mbpdb_results = data_transformer.mbpdb_results\n",
    "        self.pd_results_cleaned = protein_handler.pd_results_cleaned if hasattr(protein_handler, 'pd_results_cleaned') and protein_handler.pd_results_cleaned is not None else pd.DataFrame()\n",
    "        self._merged_df = None\n",
    "        # Set up observer for data changes\n",
    "        self.data_transformer.observe(self._handle_data_change, names=['pd_results', 'mbpdb_results'])\n",
    "        \n",
    "    @property  # Make proteins_dic a property that always reads from data_transformer\n",
    "    def proteins_dic(self):\n",
    "        return self.data_transformer.proteins_dic\n",
    "        \n",
    "    def _handle_data_change(self, change):\n",
    "        \"\"\"Handle changes in the input data.\"\"\"\n",
    "        if change.name == 'pd_results':\n",
    "            self.pd_results = change.new\n",
    "        elif change.name == 'mbpdb_results':\n",
    "            self.mbpdb_results = change.new\n",
    "        elif change.name == 'pd_results_cleaned':\n",
    "            self.pd_results_cleaned = change.new        # Re-run interactive display\n",
    "        clear_output()        \n",
    "    @property\n",
    "    def merged_df(self):\n",
    "        \"\"\"Property to access the merged DataFrame.\"\"\"\n",
    "        return self._merged_df\n",
    "        \n",
    "    def add_protein_info(self, df):\n",
    "        \"\"\"\n",
    "        Adds protein species and name information to the dataframe based on Master Protein Accessions,\n",
    "        inserting them after Master Protein Accessions and before Positions in Proteins.\n",
    "        \n",
    "        Args:\n",
    "            df (pandas.DataFrame): Input dataframe containing 'Master Protein Accessions' column\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: DataFrame with added 'protein_species' and 'protein_name' columns\n",
    "        \"\"\"\n",
    "        # First, make a copy to avoid modifying the original\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Create temporary columns\n",
    "        df['protein_species'] = 'Unknown'\n",
    "        df['protein_name'] = 'Unknown Protein'\n",
    "        \n",
    "        # Process each row\n",
    "        for idx, row in df.iterrows():\n",
    "            # Get the protein accessions - handle potential multiple proteins\n",
    "            proteins = str(row['Master Protein Accessions']).split(';')\n",
    "            \n",
    "            # Process first protein in the list (primary protein)\n",
    "            if proteins and proteins[0] != '' and proteins[0] != 'nan':\n",
    "                protein = proteins[0].strip()\n",
    "                df.at[idx, 'protein_species'] = self.proteins_dic.get(protein, {}).get('species', \"Unknown\")\n",
    "                df.at[idx, 'protein_name'] = self.proteins_dic.get(protein, {}).get('name', \"Unknown Protein\")\n",
    "        \n",
    "        # Get all column names\n",
    "        all_cols = list(df.columns)\n",
    "        \n",
    "        # Remove the new columns from their current position\n",
    "        remaining_cols = [col for col in all_cols if col not in ['protein_species', 'protein_name']]\n",
    "        \n",
    "        # Find the position after 'Master Protein Accessions'\n",
    "        insert_pos = remaining_cols.index('Master Protein Accessions') + 1\n",
    "        \n",
    "        # Create the new column order\n",
    "        new_cols = (\n",
    "            remaining_cols[:insert_pos] +  # Columns before and including Master Protein Accessions\n",
    "            ['protein_species', 'protein_name'] +  # New columns\n",
    "            remaining_cols[insert_pos:]  # Remaining columns\n",
    "        )\n",
    "        \n",
    "        # Reorder the DataFrame with the new column order\n",
    "        result_df = df.reindex(columns=new_cols)\n",
    "        \n",
    "        # Verify column order (optional debug print)\n",
    "        # print(\"Column order:\", new_cols)\n",
    "        # print(\"Position of Master Protein Accessions:\", insert_pos)\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    def extract_bioactive_peptides(self):\n",
    "        \"\"\"\n",
    "        Extracts the list of bioactive peptide matches from the imported MBPDB search.\n",
    "        \"\"\"\n",
    "        if not self.mbpdb_results.empty:\n",
    "            # Drop rows where protein_id is NaN or 'None'\n",
    "            mbpdb_results_cleaned = self.mbpdb_results.copy()\n",
    "            mbpdb_results_cleaned.dropna(subset=['search_peptide'], inplace=True)\n",
    "            mbpdb_results_cleaned = mbpdb_results_cleaned[mbpdb_results_cleaned['protein_id'] != 'None']\n",
    "\n",
    "            # Check if '% Alignment' column exists\n",
    "            if '% Alignment' in mbpdb_results_cleaned.columns:\n",
    "                agg_dict = {\n",
    "                    'peptide': 'first', \n",
    "                    'protein_id': 'first',\n",
    "                    'protein_description': 'first',\n",
    "                    '% Alignment': 'first',\n",
    "                    'species': 'first',\n",
    "                    'intervals': 'first',\n",
    "                    'function': lambda x: list(x.dropna().unique())\n",
    "                }\n",
    "            else:\n",
    "                agg_dict = {\n",
    "                    'peptide': 'first', \n",
    "                    #'search_peptide': 'first',\n",
    "                    'protein_description': 'first',\n",
    "                    'species': 'first',\n",
    "                    'intervals': 'first',\n",
    "                    'function': lambda x: list(x.dropna().unique())\n",
    "                }\n",
    "\n",
    "            # Perform the groupby and aggregation\n",
    "            self.mbpdb_results_grouped = mbpdb_results_cleaned.groupby('search_peptide').agg(agg_dict).reset_index()\n",
    "\n",
    "            # Flatten the 'function' list\n",
    "            self.mbpdb_results_grouped['function'] = self.mbpdb_results_grouped['function'].apply(\n",
    "                lambda x: '; '.join(x) if isinstance(x, list) else x\n",
    "            )\n",
    "            return mbpdb_results_cleaned, self.mbpdb_results_grouped\n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    def create_unique_id(self, row):\n",
    "        \"\"\"Creates a unique ID for each peptide row.\"\"\"\n",
    "        # Handle Sequence - convert list to comma-separated string if needed\n",
    "        sequence = row['Sequence']\n",
    "        if isinstance(sequence, list):\n",
    "            sequence = ','.join(sequence)\n",
    "        else:\n",
    "            sequence = str(sequence).strip()\n",
    "        \n",
    "        # Create unique ID with modifications if present\n",
    "        if pd.notna(row['Modifications']):\n",
    "            unique_id = sequence + \"_\" + row['Modifications'].strip()\n",
    "        else:\n",
    "            unique_id = sequence\n",
    "        \n",
    "        # Ensure unique_id is a string and strip trailing underscores\n",
    "        unique_id = str(unique_id).strip()\n",
    "        return unique_id.rstrip('_')\n",
    "    \n",
    "    def process_pd_results(self, mbpdb_results_grouped):\n",
    "        \"\"\"Processes the PD results and merges with MBPDB results.\"\"\"\n",
    "        pd_results_cleaned = self.pd_results_cleaned\n",
    "        \n",
    "        # Process positions and accessions\n",
    "        #pd_results_cleaned['Positions in Proteins'] = pd_results_cleaned['Positions in Proteins'].str.split(';', expand=False).str[0]\n",
    "        #pd_results_cleaned['Master Protein Accessions'] = pd_results_cleaned['Master Protein Accessions'].str.split(';', expand=False).str[0]\n",
    "                    \n",
    "        # Handle NaN/Unknown values first\n",
    "        pd_results_cleaned['Master Protein Accessions'] = pd_results_cleaned['Master Protein Accessions'].fillna('Unknown')\n",
    "        pd_results_cleaned['Positions in Proteins'] = pd_results_cleaned['Positions in Proteins'].fillna('Unknown')\n",
    "        \n",
    "        # Create sequence column if needed\n",
    "        # Create sequence column if needed\n",
    "        if 'Sequence' not in pd_results_cleaned.columns:\n",
    "            # First create Sequence column with NaN values\n",
    "            pd_results_cleaned['Sequence'] = pd.NA\n",
    "            \n",
    "            def extract_sequence(annotated_seq):\n",
    "                if pd.isna(annotated_seq):\n",
    "                    return pd.NA\n",
    "                \n",
    "                # Case 1: [X].SEQUENCE.[X] format\n",
    "                if '.' in annotated_seq:\n",
    "                    parts = annotated_seq.split('.')\n",
    "                    if len(parts) > 1:\n",
    "                        return parts[1]\n",
    "                \n",
    "                # Case 2: Plain sequence like \"LLL\" or \"WE\"\n",
    "                return annotated_seq\n",
    "            \n",
    "            # Apply the extraction function to all rows\n",
    "            pd_results_cleaned['Sequence'] = pd_results_cleaned['Annotated Sequence'].apply(extract_sequence)\n",
    "        \n",
    "        # Create unique ID\n",
    "        pd_results_cleaned['unique ID'] = pd_results_cleaned.apply(self.create_unique_id, axis=1)\n",
    "        \n",
    "        # Extract start and stop positions\n",
    "        try:\n",
    "            # Initialize start and stop columns with NaN\n",
    "            pd_results_cleaned['start'] = pd.NA\n",
    "            pd_results_cleaned['stop'] = pd.NA\n",
    "            \n",
    "            # Create mask for rows without semicolons (single positions) and not Unknown\n",
    "            valid_position_mask = (~pd_results_cleaned['Positions in Proteins'].str.contains(';', na=False) & \n",
    "                                 (pd_results_cleaned['Positions in Proteins'] != 'Unknown'))\n",
    "            \n",
    "            # Process rows with single positions\n",
    "            single_positions = pd_results_cleaned.loc[valid_position_mask, 'Positions in Proteins']\n",
    "            if not single_positions.empty:\n",
    "                extracted = single_positions.str.extract(r'\\[(\\d+)-(\\d+)\\]')\n",
    "                \n",
    "                # Convert to numeric and handle invalid values\n",
    "                pd_results_cleaned.loc[valid_position_mask, 'start'] = pd.to_numeric(extracted[0], errors='coerce')\n",
    "                pd_results_cleaned.loc[valid_position_mask, 'stop'] = pd.to_numeric(extracted[1], errors='coerce')\n",
    "            \n",
    "            # Convert to Int64 to handle missing values properly\n",
    "            pd_results_cleaned['start'] = pd_results_cleaned['start'].astype('Int64')\n",
    "            pd_results_cleaned['stop'] = pd_results_cleaned['stop'].astype('Int64')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing positions: {str(e)}\")\n",
    "            \n",
    "        # Reorder columns with unique ID and Sequence first\n",
    "        remaining_cols = [col for col in pd_results_cleaned.columns \n",
    "                         if col not in ['unique ID', 'Sequence', 'Master Protein Accessions', \n",
    "                                      'Positions in Proteins', 'start', 'stop']]\n",
    "        \n",
    "        columns_order = ['unique ID', 'Sequence', 'Master Protein Accessions', \n",
    "                        'Positions in Proteins', 'start', 'stop'] + remaining_cols\n",
    "        \n",
    "        pd_results_cleaned = pd_results_cleaned[columns_order]\n",
    "                \n",
    "        # Merge with MBPDB results if available\n",
    "        if self.mbpdb_results_grouped is not None and not self.mbpdb_results_grouped.empty:\n",
    "            # First do the regular merge\n",
    "            merged_df = pd.merge(pd_results_cleaned, self.mbpdb_results_grouped, \n",
    "                                right_on='search_peptide', left_on='unique ID', how='left')\n",
    "            \n",
    "            # Second pass: handle comma-separated unique IDs\n",
    "            comma_mask = merged_df['unique ID'].str.contains(',', na=False)\n",
    "            comma_rows = merged_df[comma_mask].copy()\n",
    "            \n",
    "            for idx, row in comma_rows.iterrows():\n",
    "                # Split the unique ID\n",
    "                unique_ids = row['unique ID'].split(',')\n",
    "                \n",
    "                # Check if any part matches with search_peptide\n",
    "                matches = self.mbpdb_results_grouped[self.mbpdb_results_grouped['search_peptide'].isin(unique_ids)]\n",
    "\n",
    "                if not matches.empty:\n",
    "                    # Take the first match and update all MBPDB columns\n",
    "                    match = matches.iloc[0]\n",
    "                    for col in self.mbpdb_results_grouped.columns:\n",
    "                        #if col != 'search_peptide':  # Don't overwrite unique ID\n",
    "                        merged_df.loc[idx, col] = match[col]\n",
    "        \n",
    "            display(HTML(\"<b style='color:green;'>The MBPDB was successfully merged with the peptidomic data matching the Search Peptide and Unique ID columns (including comma-separated IDs).</b>\"))\n",
    "        \n",
    "        else:\n",
    "            merged_df = pd_results_cleaned.copy()\n",
    "            merged_df['function'] = np.nan\n",
    "            display(HTML(\"<b style='color:orange;'>No MBPDB was uploaded.</b>\"))\n",
    "            display(HTML(\"<b style='color:orange;'>The merged Dataframe contains only peptidomic data.</b>\"))\n",
    "        \n",
    "        # Ensure columns are in correct order\n",
    "        final_column_order = columns_order + [col for col in merged_df.columns if col not in columns_order]\n",
    "        merged_df = merged_df[final_column_order]\n",
    "        \n",
    "        return merged_df\n",
    "    \n",
    "    def calculate_group_abundance_sem_averages(self, df, group_data):\n",
    "        \"\"\"Calculates group abundance averages and SEMs, organizing them with averages first, then SEMs.\"\"\"\n",
    "        # Check if all average abundance columns already exist\n",
    "        all_columns_exist = True\n",
    "        for group_number, details in group_data.items():\n",
    "            average_column_name = f\"Avg_{details['grouping_variable']}\"\n",
    "            if average_column_name not in df.columns:\n",
    "                all_columns_exist = False\n",
    "                break\n",
    "        \n",
    "        if all_columns_exist:\n",
    "            display(HTML('<b style=\"color:orange;\">All average abundance columns already exist. Returning original DataFrame.</b>'))\n",
    "            return df\n",
    "        \n",
    "        # If not all columns exist, proceed with calculations\n",
    "        average_columns = {}\n",
    "        sem_columns = {}\n",
    "        \n",
    "        # Calculate all averages and SEMs but store them separately\n",
    "        for group_number, details in group_data.items():\n",
    "            grouping_variable = details['grouping_variable']\n",
    "            abundance_columns = details['abundance_columns']\n",
    "            \n",
    "            # Convert abundance columns to numeric\n",
    "            for col in abundance_columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Define column names\n",
    "            average_column_name = f\"Avg_{grouping_variable}\"\n",
    "            sem_column_name = f\"SEM_{grouping_variable}\"\n",
    "        \n",
    "            # Calculate standard deviation\n",
    "            std = df[abundance_columns].std(axis=1, skipna=True)\n",
    "            \n",
    "            # Calculate number of non-NaN values for each row\n",
    "            n_samples = df[abundance_columns].notna().sum(axis=1)\n",
    "            \n",
    "            # Calculate SEM (standard deviation divided by square root of n)\n",
    "            sem = std / np.sqrt(n_samples)\n",
    "            \n",
    "            # Store results in separate dictionaries\n",
    "            average_columns[average_column_name] = df[abundance_columns].mean(axis=1, skipna=True)\n",
    "            sem_columns[sem_column_name] = sem\n",
    "        \n",
    "        # Combine the columns in the desired order (all averages, then all SEMs)\n",
    "        new_columns = {**average_columns, **sem_columns}\n",
    "        \n",
    "        # Add new columns to DataFrame\n",
    "        df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "        \n",
    "        if not df.empty:\n",
    "            display(HTML('<b style=\"color:green;\">Group average abundance and Standard Error of Mean (SEM) columns have been successfully added to the DataFrame.</b>'))\n",
    "        return df\n",
    "\n",
    "    \n",
    "    def process_data(self, group_data):\n",
    "        \"\"\"Main method to process all data.\"\"\"\n",
    "        if hasattr(self, 'pd_results') and self.pd_results is not None and not self.pd_results.empty:\n",
    "            try:\n",
    "                # Extract and process bioactive peptides\n",
    "                mbpdb_results_cleaned, self.mbpdb_results_grouped = self.extract_bioactive_peptides()\n",
    "                \n",
    "                if not hasattr(self, 'pd_results_cleaned') or self.pd_results_cleaned is None:\n",
    "                    self.pd_results_cleaned = self.pd_results.copy()\n",
    "                \n",
    "                # Process PD results and merge with MBPDB\n",
    "                merged_df_temp = self.process_pd_results(self.mbpdb_results_grouped)\n",
    "                \n",
    "                # Calculate abundance averages if group_data exists\n",
    "                if group_data:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                        final_df_temp = self.calculate_group_abundance_sem_averages(merged_df_temp, group_data)\n",
    "                else:\n",
    "                    final_df_temp = merged_df_temp\n",
    "                    display(HTML(\"<b style='color:orange;'>No group data provided. Skipping abundance calculations.</b>\"))\n",
    "        \n",
    "                \n",
    "                # Store the final DataFrame and add protien name and species \n",
    "                final_df = self.add_protein_info(final_df_temp)\n",
    "                self._merged_df = final_df\n",
    "\n",
    "                return final_df\n",
    "\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red;'>Error processing data: {str(e)}</b>\"))\n",
    "                return None\n",
    "        else:\n",
    "            display(HTML(\"<b style='color:red;'>No PD results data available for processing.</b>\"))\n",
    "            return None\n",
    "            \n",
    "    def update_data(self, pd_results):\n",
    "        \"\"\"Update data and refresh filtered columns\"\"\"\n",
    "        self.pd_results = pd_results\n",
    "        \n",
    "        # Only update if we have valid data\n",
    "        if pd_results is not None and not pd_results.empty:\n",
    "            self.setup_data()\n",
    "            \n",
    "            # Update the dropdown with new filtered columns\n",
    "            self.column_dropdown.options = self.filtered_columns\n",
    "            \n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:green;\">Data updated successfully. Column selection refreshed.</b>'))\n",
    "        else:\n",
    "            # Clear options if no data\n",
    "            self.column_dropdown.options = []\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:orange;\">No data available for column selection.</b>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20958467-4a02-4b10-afbb-1a3224430ce5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ExportManager:\n",
    "    \"\"\"Class to manage all export operations with notebook-compatible lazy loading\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.output_style = \"\"\"\n",
    "            <style>\n",
    "            .download-link {\n",
    "                background-color: #4CAF50;\n",
    "                border: none;\n",
    "                color: white;\n",
    "                padding: 10px 20px;\n",
    "                text-align: center;\n",
    "                text-decoration: none;\n",
    "                display: inline-block;\n",
    "                font-size: 14px;\n",
    "                margin: 4px 2px;\n",
    "                cursor: pointer;\n",
    "                border-radius: 4px;\n",
    "            }\n",
    "            .download-link:hover {\n",
    "                background-color: #45a049;\n",
    "            }\n",
    "            .download-link:disabled {\n",
    "                background-color: #cccccc;\n",
    "                cursor: not-allowed;\n",
    "            }\n",
    "            .export-section {\n",
    "                margin-bottom: 20px;\n",
    "                padding: 15px;\n",
    "                border-radius: 5px;\n",
    "                background-color: #f8f9fa;\n",
    "            }\n",
    "            .export-description {\n",
    "                color: #666;\n",
    "                margin: 5px 0 15px 0;\n",
    "                font-style: italic;\n",
    "            }\n",
    "            </style>\n",
    "        \"\"\"\n",
    "\n",
    "    def _create_download_section(self, title, description, data_generator, mime_type):\n",
    "        \"\"\"Create a download section with direct data generation\"\"\"\n",
    "        try:\n",
    "            # Generate the data immediately but efficiently\n",
    "            content, filename = data_generator()\n",
    "            \n",
    "            if isinstance(content, str):\n",
    "                content = content.encode('utf-8')\n",
    "            \n",
    "            # Convert to base64\n",
    "            b64_data = base64.b64encode(content).decode('utf-8')\n",
    "            file_data = f\"data:{mime_type};base64,{b64_data}\"\n",
    "            \n",
    "            html_content = f\"\"\"\n",
    "            <div class=\"export-section\">\n",
    "                <h3><u>{title}</u></h3>\n",
    "                <div class=\"export-description\">\n",
    "                    {description}\n",
    "                </div>\n",
    "                <a href=\"{file_data}\" \n",
    "                   download=\"{filename}\" \n",
    "                   class=\"download-link\"\n",
    "                   title=\"Click to download\">\n",
    "                    Download Data\n",
    "                </a>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            display(HTML(self.output_style + html_content))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating download: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def export_mbpdb_results(self, df):\n",
    "        \"\"\"Export MBPDB results as TSV\"\"\"\n",
    "        if not 'function' in df.columns:\n",
    "            display(HTML(\"<b style='color:red'>No bioactivity data avlaible to download.</b>\"))\n",
    "            return\n",
    "            \n",
    "        def generate_mbpdb_data():\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"MBPDB_SEARCH_{timestamp}.tsv\"\n",
    "            content = df.to_csv(sep='\\t', index=False)\n",
    "            return content.encode(), filename\n",
    "            \n",
    "        self._create_download_section(\n",
    "            \n",
    "            \"Export MBPDB Search Results\",\n",
    "            \"Download the results from searching your peptides against the MBPDB database\",\n",
    "            generate_mbpdb_data,\n",
    "            'text/tab-separated-values'\n",
    "        )\n",
    "\n",
    "    def export_group_data(self, group_data):\n",
    "        \"\"\"Export group data as JSON\"\"\"\n",
    "        if not group_data:\n",
    "            display(HTML(\"<b style='color:red'>No group variables declared. Can not return group variable export</b>\"))\n",
    "            return\n",
    "        def generate_group_data():\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"Categorical_variable_definitions_{timestamp}.json\"\n",
    "            content = json.dumps(group_data, indent=4)\n",
    "            return content.encode(), filename\n",
    "            \n",
    "        self._create_download_section(\n",
    "            \"Export Group Data\",\n",
    "            \"Download the categorical variable definitions used for data grouping and analysis\",\n",
    "            generate_group_data,\n",
    "            'application/json'\n",
    "        )\n",
    "\n",
    "    def export_dataframe(self, df):\n",
    "        \"\"\"Export DataFrame as CSV\"\"\"\n",
    "        def generate_df_data():\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"Merged_Dataframe_{timestamp}.csv\"\n",
    "            content = df.to_csv(index=False)\n",
    "            return content.encode(), filename\n",
    "            \n",
    "        self._create_download_section(\n",
    "            \"Export Full Dataset\",\n",
    "            \"Download the complete merged dataset containing all processed data\",\n",
    "            generate_df_data,\n",
    "            'text/csv'\n",
    "        )\n",
    "\n",
    "    def setup_pivoted_data_export(self, merged_df, group_data):\n",
    "        \"\"\"Setup pivoted data export\"\"\"\n",
    "        if not group_data:\n",
    "            display(HTML(\"<b style='color:red'>No group variables declared. Can not return pivoted data tables.</b>\"))\n",
    "            return\n",
    "        def generate_pivoted_data():\n",
    "            def create_pivoted_df(df, abundance_columns):\n",
    "                melted_df = df.melt(\n",
    "                    id_vars=['unique ID'],\n",
    "                    value_vars=abundance_columns,\n",
    "                    var_name='Sample',\n",
    "                    value_name='Abundance'\n",
    "                )\n",
    "                pivoted = melted_df.pivot_table(\n",
    "                    index='Sample',\n",
    "                    columns='unique ID',\n",
    "                    values='Abundance'\n",
    "                )\n",
    "                pivoted.index.name = 'Abundance Values'\n",
    "                return pivoted\n",
    "\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"pivoted_data_{timestamp}.xlsx\"\n",
    "            \n",
    "            output = io.BytesIO()\n",
    "            with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
    "                for group_key, group_info in group_data.items():\n",
    "                    pivoted_df = create_pivoted_df(\n",
    "                        merged_df, \n",
    "                        group_info['abundance_columns']\n",
    "                    )\n",
    "                    if not pivoted_df.empty:\n",
    "                        pivoted_df.to_excel(\n",
    "                            writer, \n",
    "                            sheet_name=group_info['grouping_variable'],\n",
    "                            index=True\n",
    "                        )\n",
    "            \n",
    "            return output.getvalue(), filename\n",
    "            \n",
    "        self._create_download_section(\n",
    "            \"Pivoted Peptide Data Export\",\n",
    "            \"Download abundance values organized by sample and peptide ID\",\n",
    "            generate_pivoted_data,\n",
    "            'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "        )\n",
    "\n",
    "    def export_bioactive_data(self, merged_df, group_data):\n",
    "        \"\"\"Export bioactive function analysis\"\"\"\n",
    "        if not 'function' in merged_df.columns:\n",
    "            display(HTML(\"<b style='color:red'>No bioactive data to export. Can not export bioactive function analysis.</b>\"))\n",
    "            return\n",
    "        \n",
    "        if not group_data:\n",
    "            display(HTML(\"<b style='color:red'>No group variables declared.  Can not export bioactive function analysis</b>\"))\n",
    "            return\n",
    "\n",
    "          \n",
    "        def generate_bioactive_data():\n",
    "            results = self._bioactive_function_count_and_abundance_sum_avg(merged_df, group_data)\n",
    "            if not results:\n",
    "                raise ValueError(\"No bioactive data to export\")\n",
    "            \n",
    "            (summed_function_count, unique_function_counts, \n",
    "             unique_function_count_averages, unique_function_absorbance, \n",
    "             summed_function_abundance) = results\n",
    "\n",
    "            # Create DataFrames\n",
    "            peptide_count_df = pd.DataFrame.from_dict(\n",
    "                summed_function_count, \n",
    "                orient='index', \n",
    "                columns=['Counts of peptides']\n",
    "            )\n",
    "            function_count_df = pd.DataFrame.from_dict(\n",
    "                unique_function_counts, \n",
    "                orient='index'\n",
    "            ).fillna(0).astype(int)\n",
    "            combined_count_df = pd.concat([peptide_count_df, function_count_df], axis=1).T\n",
    "\n",
    "            peptide_absorbance_df = pd.DataFrame.from_dict(\n",
    "                summed_function_abundance, \n",
    "                orient='index', \n",
    "                columns=['Summed Abundance']\n",
    "            )\n",
    "            function_absorbance_df = pd.DataFrame.from_dict(\n",
    "                unique_function_absorbance, \n",
    "                orient='index'\n",
    "            ).fillna(0)\n",
    "            combined_absorbance_df = pd.concat(\n",
    "                [peptide_absorbance_df, function_absorbance_df], \n",
    "                axis=1\n",
    "            ).T\n",
    "\n",
    "            combined_df = pd.DataFrame(\n",
    "                index=combined_absorbance_df.index, \n",
    "                columns=combined_absorbance_df.columns\n",
    "            )\n",
    "            \n",
    "            for col in combined_absorbance_df.columns:\n",
    "                for idx in combined_absorbance_df.index:\n",
    "                    abundance = combined_absorbance_df.loc[idx, col]\n",
    "                    count = (combined_count_df.loc['Counts of peptides', col] \n",
    "                            if idx == 'Summed Abundance' \n",
    "                            else combined_count_df.loc[idx, col])\n",
    "                    combined_df.loc[idx, col] = \"-\" if (abundance == 0 and count == 0) else f\"{abundance:.2e} ({round(count)})\"\n",
    "            \n",
    "            combined_df.rename(index={'Summed Abundance': 'Total'}, inplace=True)\n",
    "\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"Processed_mbpdb_results_{timestamp}.xlsx\"\n",
    "            \n",
    "            output = io.BytesIO()\n",
    "            with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
    "                combined_df.to_excel(writer, sheet_name='combined', index=True)\n",
    "                combined_count_df.to_excel(writer, sheet_name='count', index=True)\n",
    "                combined_absorbance_df.to_excel(writer, sheet_name='absorbance', index=True)\n",
    "            \n",
    "            return output.getvalue(), filename\n",
    "        \n",
    "        if 'function' in merged_df.columns:    \n",
    "            if group_data:\n",
    "                self._create_download_section(\n",
    "                    \"Export Bioactive Function Analysis\",\n",
    "                    \"Download the bioactive function analysis results in Excel format containing three sheets: combined, count, and absorbance\",\n",
    "                    generate_bioactive_data,\n",
    "                    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "                    )\n",
    "\n",
    "    def _bioactive_function_count_and_abundance_sum_avg(self, df, group_data):\n",
    "        \"\"\"Debug version of bioactive function counting and abundance calculation\"\"\"\n",
    "        \n",
    "        # Initialize result dictionaries\n",
    "        summed_function_count = {}\n",
    "        unique_function_counts = {}\n",
    "        unique_function_count_averages = {}\n",
    "        unique_function_absorbance = {}\n",
    "        summed_function_abundance = {}\n",
    "    \n",
    "\n",
    "    \n",
    "        # Iterate over each group\n",
    "        for group_id, group_info in group_data.items():\n",
    "            grouping_variable = group_info['grouping_variable']\n",
    "            abundance_column = f'Avg_{grouping_variable}'\n",
    "            \n",
    "            \n",
    "            # Check if abundance column exists\n",
    "            if abundance_column not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Filter and process data\n",
    "            temp_filter_df = df[['unique ID', 'function', abundance_column]].copy()\n",
    "            \n",
    "            # Filter non-zero and non-NaN values\n",
    "            temp_filter_df = temp_filter_df[\n",
    "                (temp_filter_df[abundance_column] != 0) & \n",
    "                temp_filter_df[abundance_column].notna() &\n",
    "                temp_filter_df['function'].notna()\n",
    "            ]\n",
    "            \n",
    "            # Drop duplicates\n",
    "            filtered_df = temp_filter_df.drop_duplicates(subset='unique ID')\n",
    "            \n",
    "            if filtered_df.empty:\n",
    "                continue\n",
    "                \n",
    "            # Calculate metrics\n",
    "            unique_peptide_count = filtered_df['unique ID'].nunique()\n",
    "            total_sum = filtered_df[abundance_column].sum()\n",
    "            \n",
    "            \n",
    "            # Store the totals\n",
    "            summed_function_abundance[grouping_variable] = total_sum\n",
    "            summed_function_count[grouping_variable] = unique_peptide_count\n",
    "            \n",
    "            # Process functions\n",
    "            # Create an explicit copy first\n",
    "            filtered_df = filtered_df.copy()\n",
    "            \n",
    "            # Use loc to modify the DataFrame\n",
    "            filtered_df.loc[:, 'function'] = filtered_df['function'].fillna('')\n",
    "            filtered_df.loc[:, 'function'] = filtered_df['function'].str.split(';')\n",
    "            \n",
    "            # Explode and continue processing\n",
    "            exploded_df = filtered_df.explode('function')\n",
    "            exploded_df.loc[:, 'function'] = exploded_df['function'].str.strip()\n",
    "            exploded_df = exploded_df[exploded_df['function'] != '']\n",
    "            if not exploded_df.empty:\n",
    "                \n",
    "                # Count functions\n",
    "                function_counts = exploded_df['function'].value_counts().to_dict()\n",
    "                unique_function_counts[grouping_variable] = function_counts\n",
    "                \n",
    "                # Calculate function abundances\n",
    "                function_grouped = exploded_df.groupby('function')[abundance_column].sum()\n",
    "                unique_function_absorbance[grouping_variable] = function_grouped.to_dict()\n",
    "                \n",
    "                # Calculate averages\n",
    "                num_columns_in_group = 1  # Since using averaged columns\n",
    "                function_averages = {func: count / num_columns_in_group \n",
    "                                   for func, count in function_counts.items()}\n",
    "                unique_function_count_averages[grouping_variable] = function_averages\n",
    "            \n",
    "        \n",
    "        return (summed_function_count, unique_function_counts, \n",
    "                unique_function_count_averages, unique_function_absorbance, \n",
    "                summed_function_abundance)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b506a35-a853-46ee-b193-3fce79decf57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48c146c0f97477cbc2eb865ee189108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Process Data', style=ButtonStyle(), tooltip='Click â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63017fdde96841f28e1bb8d4bff1e9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069ad96a5c47441794c6ffc64addb078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a34d526be194094b2af262c291fbdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481c421ab1024ad39ff8dc11d31c2bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2fdc27a7ef4cc28fabbc838fc42116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class DataProcessingController:\n",
    "    def __init__(self, workflow):\n",
    "        self.workflow = workflow  # Store reference to workflow\n",
    "        self.export_manager = ExportManager()\n",
    "        self.combiner = None\n",
    "        self.merged_df = None\n",
    "        \n",
    "        # Create processing button\n",
    "        self.process_button = widgets.Button(\n",
    "            description='Process Data',\n",
    "            button_style='success',\n",
    "            tooltip='Click to start data processing'\n",
    "        )\n",
    "        \n",
    "        # Create export button (initially disabled)\n",
    "        self.export_button = widgets.Button(\n",
    "            description='Export Data',\n",
    "            button_style='info',\n",
    "            tooltip='Process data first to enable export',\n",
    "            layout=widgets.Layout(margin='0 0 0 10px'),  # Add margin to separate buttons\n",
    "            disabled=True  # Start disabled\n",
    "        )\n",
    "        \n",
    "        # Create button container\n",
    "        self.button_container = widgets.HBox([self.process_button, self.export_button])\n",
    "        \n",
    "        # Create separate output areas\n",
    "        self.process_output = widgets.Output()\n",
    "        self.export_output = widgets.Output()\n",
    "        self.search_output = widgets.Output()\n",
    "        self.stats_output = widgets.Output()\n",
    "        self.grid_output = widgets.Output()\n",
    "        \n",
    "        # Set up button callbacks\n",
    "        self.process_button.on_click(self._on_process_clicked)\n",
    "        self.export_button.on_click(self._on_export_clicked)\n",
    "        \n",
    "    def display_interactive_results(self, df):\n",
    "        \"\"\"Display interactive grid with row search functionality\"\"\"\n",
    "        if df is not None:\n",
    "            # Create search widget\n",
    "            search_widget = widgets.Text(\n",
    "                placeholder='Search for data in rows...',\n",
    "                description='Search:',\n",
    "                layout=widgets.Layout(width='50%'),\n",
    "                style={'description_width': 'initial'}\n",
    "            )\n",
    "            \n",
    "            def get_column_category(col):\n",
    "                \"\"\"Determine category for each column\"\"\"\n",
    "                if col.startswith('Avg_'):\n",
    "                    return 'Average Abundance'\n",
    "                if col.startswith('SEM_'):\n",
    "                    return 'Standard Error Mean'\n",
    "                elif col in self.workflow.data_transformer.mbpdb_results.columns:\n",
    "                    return 'MBPDB Search Results'\n",
    "                else:\n",
    "                    return 'Peptidomic Data'\n",
    "\n",
    "            # Create multi-level columns while preserving order\n",
    "            column_tuples = [(get_column_category(col), col) for col in df.columns]\n",
    "            \n",
    "            df_display = df.copy()\n",
    "            df_display.columns = pd.MultiIndex.from_tuples(column_tuples)\n",
    "            \n",
    "            def create_grid(df_to_display):\n",
    "                grid = DataGrid(\n",
    "                    df_to_display, \n",
    "                    selection_mode='cell', \n",
    "                    editable=False,\n",
    "                    layout=widgets.Layout(height='600px')\n",
    "                )\n",
    "                grid.auto_fit_columns = True\n",
    "                grid.base_row_size = 25\n",
    "                grid.base_column_size = 150\n",
    "                grid.auto_fit_params = {'area': 'column', 'padding': 10}\n",
    "                return grid\n",
    "            \n",
    "            def on_search_change(change):\n",
    "                with self.grid_output:\n",
    "                    self.grid_output.clear_output()\n",
    "                    \n",
    "                    search_term = change['new'].strip()\n",
    "                    if search_term:\n",
    "                        str_df = df_display.astype(str)\n",
    "                        mask = str_df.apply(\n",
    "                            lambda row: row.str.contains(search_term, case=False, na=False).any(),\n",
    "                            axis=1\n",
    "                        )\n",
    "                        filtered_df = df_display[mask]\n",
    "                        \n",
    "                        with self.stats_output:\n",
    "                            self.stats_output.clear_output()\n",
    "                            print(f\"Found {len(filtered_df)} matching rows out of {len(df_display)} total rows\")\n",
    "                    else:\n",
    "                        filtered_df = df_display\n",
    "                        with self.stats_output:\n",
    "                            self.stats_output.clear_output()\n",
    "                    \n",
    "                    display(create_grid(filtered_df))\n",
    "            \n",
    "            search_widget.observe(on_search_change, names='value')\n",
    "\n",
    "            # Display search interface\n",
    "            with self.search_output:\n",
    "                self.search_output.clear_output()\n",
    "                display(search_widget)\n",
    "            \n",
    "            # Initialize grid display\n",
    "            with self.grid_output:\n",
    "                self.grid_output.clear_output()\n",
    "                display(create_grid(df_display))\n",
    "            \n",
    "        else:\n",
    "            print(\"No data to display\")\n",
    "\n",
    "        \n",
    "    def _on_process_clicked(self, b):\n",
    "        # Clear all outputs except export\n",
    "        self.process_output.clear_output()\n",
    "        self.search_output.clear_output()\n",
    "        self.stats_output.clear_output()\n",
    "        self.grid_output.clear_output()\n",
    "        \n",
    "        with self.process_output:           \n",
    "            # Pass the actual data_transformer, not the workflow\n",
    "            self.combiner = CombineAverageDataframes(\n",
    "                self.workflow.data_transformer,  # Pass the data_transformer directly\n",
    "                self.workflow.group_processor, \n",
    "                self.workflow.protein_handler\n",
    "            )\n",
    "            self.merged_df = self.combiner.process_data(self.workflow.group_processor.group_data)\n",
    "            \n",
    "            if self.merged_df is not None:\n",
    "                print(\"\\nData processing completed successfully!\")\n",
    "                print(f\"Final results row count: {self.merged_df.shape[0]}\")\n",
    "                print(f\"Final results column count: {self.merged_df.shape[1]}\")\n",
    "                \n",
    "                # Enable export button after successful processing\n",
    "                self.export_button.disabled = False\n",
    "                self.export_button.tooltip = 'Click to show export options'\n",
    "                \n",
    "                self.display_interactive_results(self.merged_df)\n",
    "            else:\n",
    "                print(\"Error: No data was processed\")\n",
    "                # Keep export button disabled if processing failed\n",
    "                self.export_button.disabled = True\n",
    "                    \n",
    "    def _on_export_clicked(self, b):\n",
    "        with self.export_output:\n",
    "            clear_output()\n",
    "            #display(HTML(\"<h2>Export:</h2>\"))\n",
    "            \n",
    "            # Use workflow's data_transformer instance\n",
    "            if (hasattr(self.workflow.data_transformer, 'mbpdb_results') and \n",
    "                self.workflow.data_transformer.mbpdb_results is not None):\n",
    "                self.export_manager.export_mbpdb_results(self.workflow.data_transformer.mbpdb_results)\n",
    "            \n",
    "            # Use workflow's group_processor instance\n",
    "            self.export_manager.export_group_data(self.workflow.group_processor.group_data)\n",
    "            \n",
    "            if self.merged_df is not None:\n",
    "                self.export_manager.export_bioactive_data(\n",
    "                    self.merged_df, \n",
    "                    self.workflow.group_processor.group_data\n",
    "                )\n",
    "                self.export_manager.export_dataframe(self.merged_df)\n",
    "                self.export_manager.setup_pivoted_data_export(\n",
    "                    self.merged_df,\n",
    "                    self.workflow.group_processor.group_data\n",
    "                )\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the complete interface\"\"\"\n",
    "        display(self.button_container)\n",
    "        display(self.process_output)\n",
    "        display(self.export_output)\n",
    "        display(self.search_output)\n",
    "        display(self.stats_output)\n",
    "        display(self.grid_output)\n",
    "\n",
    "# Initialize the controller\n",
    "controller = DataProcessingController(workflow)\n",
    "# Display the interface\n",
    "controller.display()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Vt5dV-pTHRWngML-2nDQAR6_P_KFsIx4",
     "timestamp": 1712158917217
    },
    {
     "file_id": "1l7fpCQepyE1pJq2O5QfOHVv9a4VFpr-B",
     "timestamp": 1712094574841
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
