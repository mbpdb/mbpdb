{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278kpom78fOP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23025,
     "status": "ok",
     "timestamp": 1712094448807,
     "user": {
      "displayName": "Russell Kuhfeld",
      "userId": "14760569517288879712"
     },
     "user_tz": 420
    },
    "id": "278kpom78fOP",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "95f300e9-4d05-4fe1-a725-f5c3eea6cf80",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Install/Import packages & define key varribles and functions\n",
    "# Run install script\n",
    "# %chmod +x setup_jupyterlab.sh\n",
    "# %./setup_jupyterlab.sh\n",
    "\n",
    "# Import necessary libraries for the script to function.\n",
    "import pandas as pd\n",
    "import tempfile, requests,time, csv, json, re, os, shutil, io, base64, time, subprocess, sqlite3, zipfile, base64\n",
    "from io import StringIO, BytesIO\n",
    "import numpy as np\n",
    "\n",
    "# Uniprot cliebnt   \n",
    "from xml.etree import ElementTree\n",
    "from utils.uniprot_client import fetch_uniprot_info_batch, fetch_uniprot_info, UniProtClient\n",
    "#from django.conf import settings\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.formula.api import ols\n",
    "#from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "import warnings\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from itertools import combinations\n",
    "from ipydatagrid import DataGrid\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "import traitlets\n",
    "from traitlets import HasTraits, Instance, observe\n",
    "\n",
    "# Global variable declaration\n",
    "\n",
    "import _settings as settings\n",
    "global spec_translate_list\n",
    "spec_translate_list = settings.SPEC_TRANSLATE_LIST\n",
    "# Set the default font to Calibri\n",
    "#matplotlib.rcParams['font.family'] = 'Calibri'\n",
    "\n",
    "\n",
    "def find_species(header, spec_translate_list):\n",
    "    \"\"\"Search for a species in the header and return the first element (species name) from the list.\"\"\"\n",
    "    header_lower = header.lower()\n",
    "    for spec_group in spec_translate_list:\n",
    "        for term in spec_group[1:]:  # Iterate over possible species names/terms except the first element\n",
    "            if term.lower() in header_lower:\n",
    "                return spec_group[0]  # Return the first element of the list (main species name)\n",
    "    return \"unknown\"  # Return unknown if no species match is found\n",
    "\n",
    "def parse_headers():\n",
    "    fasta_dict = {}\n",
    "    with open(\"protein_headers.txt\", 'r') as file:\n",
    "        protein_id = \"\"\n",
    "        protein_name = \"\"\n",
    "        species = \"\"\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if protein_id:\n",
    "                    # Save the previous protein entry in the dictionary\n",
    "                    fasta_dict[protein_id] = {\n",
    "                        \"name\": protein_name,\n",
    "                        \"species\": species\n",
    "                    }\n",
    "                sequence = \"\"\n",
    "                header_parts = line[1:].split('|')\n",
    "                if len(header_parts) > 2:\n",
    "                    protein_id = header_parts[1]\n",
    "                    protein_name_full = re.split(r' OS=', header_parts[2])[0]\n",
    "                    if ' ' in protein_name_full:\n",
    "\n",
    "                        protein_name = protein_name_full#.split()[1]\n",
    "                    else:\n",
    "                        protein_name = protein_name_full\n",
    "                    # Find species in the header\n",
    "                    species = find_species(line, spec_translate_list)\n",
    "\n",
    "        if protein_id:\n",
    "            # Save the last protein entry in the dictionary\n",
    "            fasta_dict[protein_id] = {\n",
    "                \"name\": protein_name,\n",
    "                \"species\": species\n",
    "            }\n",
    "    return fasta_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7255b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation(HasTraits):\n",
    "    pd_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    mbpdb_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    # pd_results_cleaned = Instance(pd.DataFrame, allow_none=True)\n",
    "    search_results = Instance(pd.DataFrame, allow_none=True)\n",
    "    protein_dict = {}  # Add explicit trait for protein_dict\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pd_results = pd.DataFrame()\n",
    "        # self.pd_results_cleaned = pd.DataFrame()\n",
    "        self.mbpdb_results = pd.DataFrame()\n",
    "        self.search_results = pd.DataFrame()\n",
    "        self.protein_dict = parse_headers()\n",
    "        self.mbpdb_results_from_search_placeholder = widgets.Checkbox(value=False)\n",
    "        self.fasta_uploader_placeholder = widgets.Checkbox(value=False)\n",
    "        self.missing_proteins = set()\n",
    "        self.setup_data_loading_ui()\n",
    "\n",
    "    def create_download_link(self, file_path, label):\n",
    "        \"\"\"Create a download link for a file.\"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            # Read file content and encode it as base64\n",
    "            with open(file_path, 'rb') as f:\n",
    "                content = f.read()\n",
    "            b64_content = base64.b64encode(content).decode('utf-8')\n",
    "\n",
    "            # Generate the download link HTML\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <a download=\"{os.path.basename(file_path)}\" \n",
    "                   href=\"data:application/octet-stream;base64,{b64_content}\" \n",
    "                   style=\"color: #0366d6; text-decoration: none; margin-left: 20px; font-size: 14px;\">\n",
    "                    {label}\n",
    "                </a>\n",
    "            \"\"\")\n",
    "        else:\n",
    "            # Show an error message if the file does not exist\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <span style=\"color: red; margin-left: 20px; font-size: 14px;\">\n",
    "                    File \"{file_path}\" not found!\n",
    "                </span>\n",
    "            \"\"\")\n",
    "\n",
    "    \"\"\"\n",
    "    def setup_search_ui(self, peptides):\n",
    "        # Create dropdown for similarity threshold\n",
    "        self.threshold_dropdown = widgets.Dropdown(\n",
    "            options=list(range(0, 101, 10)),\n",
    "            value=80,\n",
    "            description='Similarity Threshold:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "\n",
    "        # Create search button\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search Peptides',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "\n",
    "        # Progress indicator\n",
    "        self.search_progress = widgets.HTML(\n",
    "            value=\"\",\n",
    "            layout=widgets.Layout(margin='10px 0px')\n",
    "        )\n",
    "\n",
    "        # Connect button click to handler\n",
    "        self.search_button.on_click(lambda b: self._on_search_click(b, ))\n",
    "\n",
    "        # Create layout\n",
    "        self.search_widget = widgets.VBox([\n",
    "            widgets.HBox([\n",
    "                self.threshold_dropdown,\n",
    "                self.search_button\n",
    "            ], layout=widgets.Layout(align_items='center')),\n",
    "            self.search_progress\n",
    "        ])\n",
    "\n",
    "        display(self.search_widget)\n",
    "    \"\"\"\n",
    "    \n",
    "    def _on_search_click(self, b):\n",
    "        \"\"\"Handle search button click\"\"\"\n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "\n",
    "            if self.pd_results is None or self.pd_results.empty:\n",
    "                display(HTML(\"<b style='color:red'>Please upload peptidomic data first.</b>\"))\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                # Extract sequences from peptidomic data\n",
    "                self.peptides = self._extract_sequences(self.pd_results)\n",
    "\n",
    "                if not self.peptides:\n",
    "                    display(HTML(\"<b style='color:red'>No valid sequences found in peptidomic data.</b>\"))\n",
    "                    return\n",
    "\n",
    "                display(HTML(f\"<b style='color:blue'>Found {len(self.peptides)} unique peptide sequences. Searching database...</b>\"))\n",
    "                \n",
    "                # Perform search\n",
    "                results = self._search_peptides_comprehensive(\n",
    "                    self.peptides,\n",
    "                    similarity_threshold=self.threshold_dropdown.value\n",
    "                )\n",
    "                # Format results if we have any matches\n",
    "                if not results.empty:\n",
    "                    self.mbpdb_results = self._format_search_results_with_matches(results)\n",
    "                    clear_output()\n",
    "                    display(HTML(f\"<b style='color:green'>MBPDB Search complete! Found {len(self.mbpdb_results)} matches</b>\"))\n",
    "                    self.mbpdb_results_from_search_placeholder.value=True\n",
    "\n",
    "                else:\n",
    "                    self.mbpdb_results = results\n",
    "                    display(HTML(\"<b style='color:orange'>No matches found in the database.</b>\"))\n",
    "\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red'>Error: {str(e)}</b>\"))\n",
    "                self.mbpdb_results = pd.DataFrame()\n",
    "\n",
    "    def _search_peptides_comprehensive(self, peptides, similarity_threshold=100):\n",
    "        \"\"\"Search for peptides with BLAST-based similarity matching\"\"\"\n",
    "\n",
    "        # WORK_DIRECTORY = '/home/kuhfeldrf/mbpdb/include/peptide/uploads/temp'\n",
    "        # conn = sqlite3.connect('/home/kuhfeldrf/mbpdb/include/peptide/db.sqlite3')\n",
    "\n",
    "        WORK_DIRECTORY = '../../uploads/temp'\n",
    "        conn = sqlite3.connect('../../db.sqlite3')\n",
    "        work_path = self._create_work_directory(WORK_DIRECTORY)\n",
    "\n",
    "        fasta_db_path = os.path.join(work_path, \"db.fasta\")\n",
    "        results = []\n",
    "        extra_info = defaultdict(list)\n",
    "\n",
    "        # Create database with all peptides for BLAST\n",
    "        query = \"SELECT p.id, p.peptide FROM peptide_peptideinfo p\"\n",
    "        db_peptides = pd.read_sql_query(query, conn)\n",
    "\n",
    "        # Create BLAST database\n",
    "        with open(fasta_db_path, 'w') as f:\n",
    "            for _, row in db_peptides.iterrows():\n",
    "                f.write(f\">{row['id']}\\n{row['peptide']}\\n\")\n",
    "\n",
    "        self._make_blast_db(fasta_db_path)\n",
    "\n",
    "        for peptide in self.peptides:\n",
    "            if similarity_threshold == 100 or len(peptide) <4:\n",
    "                query = \"\"\"\n",
    "                SELECT DISTINCT\n",
    "                    ? as search_peptide,\n",
    "                    pi.pid as protein_id,\n",
    "                    p.id as peptide_id,\n",
    "                    p.peptide,\n",
    "                    pi.desc as protein_description,\n",
    "                    pi.species,\n",
    "                    p.intervals,\n",
    "                    f.function,\n",
    "                    r.additional_details,\n",
    "                    r.ic50,\n",
    "                    r.inhibition_type,\n",
    "                    r.inhibited_microorganisms,\n",
    "                    r.ptm,\n",
    "                    r.title,\n",
    "                    r.authors,\n",
    "                    r.abstract,\n",
    "                    r.doi,\n",
    "                    'sequence' as search_type,\n",
    "                    'IDENTITY' as scoring_matrix\n",
    "                FROM peptide_peptideinfo p\n",
    "                JOIN peptide_proteininfo pi ON p.protein_id = pi.id\n",
    "                LEFT JOIN peptide_function f ON f.pep_id = p.id\n",
    "                LEFT JOIN peptide_reference r ON r.func_id = f.id\n",
    "                WHERE p.peptide = ?\n",
    "                \"\"\"\n",
    "                df = pd.read_sql_query(query, conn, params=[peptide, peptide])\n",
    "                results.append(df)\n",
    "            else:\n",
    "                # Run BLASTP search for similarity matching\n",
    "                query_path = os.path.join(work_path, \"query.fasta\")\n",
    "                with open(query_path, \"w\") as query_file:\n",
    "                    query_file.write(f\">pep_query\\n{peptide}\\n\")\n",
    "\n",
    "                output_path = os.path.join(work_path, \"blastp_short.out\")\n",
    "                blast_args = [\n",
    "                    \"blastp\",\n",
    "                    \"-query\", query_path,\n",
    "                    \"-db\", fasta_db_path,\n",
    "                    \"-outfmt\", \"6 std ppos qcovs qlen slen positive\",\n",
    "                    \"-evalue\", \"1000\",\n",
    "                    \"-word_size\", \"2\",\n",
    "                    \"-matrix\", \"IDENTITY\",\n",
    "                    \"-threshold\", \"1\",\n",
    "                    \"-task\", \"blastp-short\",\n",
    "                    \"-out\", output_path\n",
    "                ]\n",
    "\n",
    "                subprocess.check_output(blast_args, stderr=subprocess.STDOUT)\n",
    "\n",
    "                # Process BLAST results\n",
    "                search_ids = self._process_blast_results(output_path, similarity_threshold, extra_info)\n",
    "\n",
    "                if search_ids:\n",
    "                    df = self._fetch_peptide_data(conn, peptide, search_ids)\n",
    "                    self._add_blast_details(df, extra_info)\n",
    "                    results.append(df)\n",
    "\n",
    "        conn.close()\n",
    "        self._cleanup_work_directory(WORK_DIRECTORY)\n",
    "\n",
    "        return self._combine_results(results)\n",
    "\n",
    "    def _create_work_directory(self, base_dir):\n",
    "        \"\"\"Create a working directory for BLAST operations\"\"\"\n",
    "        path = os.path.join(base_dir, f'work_{int(round(time.time() * 1000))}')\n",
    "        os.makedirs(path)\n",
    "        return path\n",
    "\n",
    "    def _make_blast_db(self, library_fasta_path):\n",
    "        \"\"\"Create BLAST database from FASTA file\"\"\"\n",
    "        subprocess.check_output(\n",
    "            ['makeblastdb', '-in', library_fasta_path, '-dbtype', 'prot'],\n",
    "            stderr=subprocess.STDOUT\n",
    "        )\n",
    "\n",
    "    def _process_blast_results(self, output_path, similarity_threshold, extra_info):\n",
    "        \"\"\"Process BLAST results and collect search IDs\"\"\"\n",
    "        search_ids = []\n",
    "        csv.register_dialect('blast_dialect', delimiter='\\t')\n",
    "\n",
    "        with open(output_path, \"r\") as output_file:\n",
    "            blast_data = csv.DictReader(\n",
    "                output_file,\n",
    "                fieldnames=['query', 'subject', 'percid', 'align_len', 'mismatches',\n",
    "                            'gaps', 'qstart', 'qend', 'sstart', 'send', 'evalue',\n",
    "                            'bitscore', 'ppos', 'qcov', 'qlen', 'slen', 'numpos'],\n",
    "                dialect='blast_dialect'\n",
    "            )\n",
    "\n",
    "            for row in blast_data:\n",
    "                tlen = float(row['slen']) if float(row['slen']) > float(row['qlen']) else float(row['qlen'])\n",
    "                simcalc = 100 * ((float(row['numpos']) - float(row['gaps'])) / tlen)\n",
    "\n",
    "                if simcalc >= similarity_threshold:\n",
    "                    search_ids.append(row['subject'])\n",
    "                    extra_info[row['subject']] = [\n",
    "                        f\"{simcalc:.2f}\", row['qstart'], row['qend'], row['sstart'],\n",
    "                        row['send'], row['evalue'], row['align_len'], row['mismatches'],\n",
    "                        row['gaps']\n",
    "                    ]\n",
    "\n",
    "        return search_ids\n",
    "\n",
    "    def _fetch_peptide_data(self, conn, peptide, search_ids):\n",
    "        \"\"\"Fetch peptide data from database\"\"\"\n",
    "        placeholders = ','.join(['?' for _ in search_ids])\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT\n",
    "            ? as search_peptide,\n",
    "            pi.pid as protein_id,\n",
    "            p.id as peptide_id,\n",
    "            p.peptide,\n",
    "            pi.desc as protein_description,\n",
    "            pi.species,\n",
    "            p.intervals,\n",
    "            f.function,\n",
    "            r.additional_details,\n",
    "            r.ic50,\n",
    "            r.inhibition_type,\n",
    "            r.inhibited_microorganisms,\n",
    "            r.ptm,\n",
    "            r.title,\n",
    "            r.authors,\n",
    "            r.abstract,\n",
    "            r.doi,\n",
    "            'sequence' as search_type,\n",
    "            'IDENTITY' as scoring_matrix\n",
    "        FROM peptide_peptideinfo p\n",
    "        JOIN peptide_proteininfo pi ON p.protein_id = pi.id\n",
    "        LEFT JOIN peptide_function f ON f.pep_id = p.id\n",
    "        LEFT JOIN peptide_reference r ON r.func_id = f.id\n",
    "        WHERE p.id IN ({placeholders})\n",
    "        \"\"\"\n",
    "\n",
    "        return pd.read_sql_query(query, conn, params=[peptide] + search_ids)\n",
    "\n",
    "    def _add_blast_details(self, df, extra_info):\n",
    "        \"\"\"Add BLAST details to DataFrame\"\"\"\n",
    "        for idx, row in df.iterrows():\n",
    "            if str(row['peptide_id']) in extra_info:\n",
    "                blast_details = extra_info[str(row['peptide_id'])]\n",
    "                df.at[idx, '% Alignment'] = blast_details[0]\n",
    "                df.at[idx, 'Query start'] = blast_details[1]\n",
    "                df.at[idx, 'Query end'] = blast_details[2]\n",
    "                df.at[idx, 'Subject start'] = blast_details[3]\n",
    "                df.at[idx, 'Subject end'] = blast_details[4]\n",
    "                df.at[idx, 'e-value'] = blast_details[5]\n",
    "                df.at[idx, 'Alignment length'] = blast_details[6]\n",
    "                df.at[idx, 'Mismatches'] = blast_details[7]\n",
    "                df.at[idx, 'Gap opens'] = blast_details[8]\n",
    "\n",
    "    def _cleanup_work_directory(self, work_directory):\n",
    "        \"\"\"Clean up old work directories\"\"\"\n",
    "        try:\n",
    "            dirs = [f for f in os.scandir(work_directory) if f.is_dir()]\n",
    "            dirs.sort(key=lambda x: os.path.getmtime(x.path), reverse=True)\n",
    "\n",
    "            for dir_entry in dirs[25:]:\n",
    "                try:\n",
    "                    shutil.rmtree(dir_entry.path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def _combine_results(self, results):\n",
    "        \"\"\"Combine and format final results\"\"\"\n",
    "        if not results:\n",
    "            mbpdb_columns = [\n",
    "                'search_peptide', 'protein_id', 'peptide', 'protein_description',\n",
    "                'species', 'intervals', 'function', 'additional_details', 'ic50',\n",
    "                'inhibition_type', 'inhibited_microorganisms', 'ptm', 'title',\n",
    "                'authors', 'abstract', 'doi', 'search_type', 'scoring_matrix'\n",
    "            ]\n",
    "            return pd.DataFrame(columns=mbpdb_columns)\n",
    "\n",
    "        final_results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "        if 'peptide_id' in final_results.columns:\n",
    "            final_results = final_results.drop('peptide_id', axis=1)\n",
    "\n",
    "        sort_columns = ['search_peptide']\n",
    "        if '% Alignment' in final_results.columns:\n",
    "            sort_columns.append('% Alignment')\n",
    "\n",
    "        return final_results.sort_values(\n",
    "            sort_columns,\n",
    "            ascending=[True] + [False] * (len(sort_columns) - 1)\n",
    "        )\n",
    "\n",
    "    def _format_search_results_with_matches(self, final_results):\n",
    "        \"\"\"Format search results with matches\"\"\"\n",
    "        if '% Alignment' in final_results.columns:\n",
    "            final_results['% Alignment'] = pd.to_numeric(\n",
    "                final_results['% Alignment'],\n",
    "                errors='coerce'\n",
    "            )\n",
    "\n",
    "        grouped = final_results.groupby([\"search_peptide\", \"function\"], as_index=False)\n",
    "        aggregated_results = []\n",
    "        processed_indices = set()\n",
    "\n",
    "        for _, group in grouped:\n",
    "            if len(group) > 1:\n",
    "                aggregated_row = self._aggregate_group_data(group)\n",
    "                aggregated_results.append(aggregated_row)\n",
    "                processed_indices.update(group.index)\n",
    "\n",
    "        remaining_rows = final_results.loc[~final_results.index.isin(processed_indices)]\n",
    "        aggregated_df = pd.DataFrame(aggregated_results)\n",
    "\n",
    "        return pd.concat([aggregated_df, remaining_rows], ignore_index=True)\n",
    "\n",
    "    def _aggregate_group_data(self, group):\n",
    "        \"\"\"Aggregate data for a group of results\"\"\"\n",
    "\n",
    "        def enumerate_field(field):\n",
    "            if field in group.columns and not group[field].dropna().empty:\n",
    "                valid_values = set(group[field].dropna().astype(str).str.strip())\n",
    "                valid_values = {val for val in valid_values if val != ''}\n",
    "                if len(valid_values) > 1:\n",
    "                    return \"; \".join([f\"{i + 1}) {val}\" for i, val in enumerate(valid_values)])\n",
    "                elif len(valid_values) == 1:\n",
    "                    return next(iter(valid_values))\n",
    "                return ''\n",
    "            return ''\n",
    "\n",
    "        return {col: enumerate_field(col) for col in group.columns}\n",
    "\n",
    "    def setup_data_loading_ui(self):\n",
    "        \"\"\"Initialize and display the data loading UI with integrated search and help tooltips\"\"\"\n",
    "\n",
    "        def create_help_icon(tooltip_text):\n",
    "            \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "            help_icon = widgets.HTML(\n",
    "                value='<i class=\"fa fa-question-circle\" style=\"color: #007bff;\"></i>',\n",
    "                layout=widgets.Layout(width='25px', margin='2px 5px')\n",
    "            )\n",
    "            help_icon.add_class('jupyter-widgets')\n",
    "            help_icon.add_class('widget-html')\n",
    "            return widgets.HTML(\n",
    "                f'<div title=\"{tooltip_text}\" style=\"display: inline-block;\">{help_icon.value}</div>'\n",
    "            )\n",
    "\n",
    "        def create_labeled_uploader(widget, label, tooltip):\n",
    "            \"\"\"Create an uploader with label and help icon\"\"\"\n",
    "            return widgets.HBox([\n",
    "                widget,\n",
    "                create_help_icon(tooltip)\n",
    "            ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        # Create file upload widgets with the same configurations\n",
    "        self.mbpdb_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload MBPDB File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        self.pd_uploader = widgets.FileUpload(\n",
    "            accept='.csv,.txt,.tsv,.xlsx',\n",
    "            multiple=False,\n",
    "            description='Upload Peptidomic File',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        self.fasta_uploader = widgets.FileUpload(\n",
    "            accept='.fasta',\n",
    "            multiple=True,\n",
    "            description='Upload FASTA Files',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        # Create search interface\n",
    "        self.threshold_dropdown = widgets.Dropdown(\n",
    "            options=list(range(0, 101, 10)),\n",
    "            value=80,\n",
    "            description='Similarity Threshold (%):',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='225px')\n",
    "        )\n",
    "\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search Database',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "\n",
    "\n",
    "        # Create output areas\n",
    "        self.output_area = widgets.Output()\n",
    "\n",
    "        mbpdb_box = widgets.HBox([\n",
    "            widgets.HTML(\"\"\"\n",
    "                    <div margin-bottom: 5px;'>\n",
    "                        <b>Option 1: Upload File</b>\n",
    "                    </div>\n",
    "                \"\"\"),\n",
    "            self.create_download_link(\n",
    "                \"examples/example_MBPDB_search.tsv\",\n",
    "                \"Example\"\n",
    "            )\n",
    "        ])\n",
    "        # Create MBPDB options section\n",
    "        mbpdb_options = widgets.HBox([widgets.VBox([\n",
    "            mbpdb_box,\n",
    "            create_labeled_uploader(\n",
    "                self.mbpdb_uploader,\n",
    "                \"MBPDB File\",\n",
    "                \"Upload your own MBPDB file (optional)\"\n",
    "            )\n",
    "        ]),\n",
    "            widgets.HTML(\"<div style='margin: 0 20px; line-height: 100px;'><b>OR</b></div>\"),\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(\"<div style='font-weight: bold; margin-bottom: 5px;'>Option 2: Search Database</div>\"),\n",
    "                widgets.HBox([\n",
    "                    self.threshold_dropdown,\n",
    "                    self.search_button,\n",
    "                    create_help_icon(\"Search peptides against the MBPDB (optional)\")\n",
    "                ], layout=widgets.Layout(align_items='center'))\n",
    "            ])\n",
    "        ], layout=widgets.Layout(align_items='center', margin='0'))\n",
    "\n",
    "        # Create peptide file uploader box with example link\n",
    "        peptide_box = widgets.HBox([\n",
    "            create_labeled_uploader(\n",
    "                self.pd_uploader,\n",
    "                \"Peptidomic File\",\n",
    "                \"Upload peptide groups data from Proteome Discover export file (required)\"\n",
    "            ),\n",
    "            self.create_download_link(\n",
    "                \"examples/example_peptide_data.csv\",\n",
    "                \"Example\"\n",
    "            )\n",
    "        ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        # Create FASTA uploader box with example link\n",
    "        fasta_box = widgets.HBox([\n",
    "            create_labeled_uploader(\n",
    "                self.fasta_uploader,\n",
    "                \"FASTA Files\",\n",
    "                \"Upload Protein FASTA file used in Proteome Discoverer Search (optional)\"\n",
    "            ),\n",
    "            self.create_download_link(\n",
    "                \"examples/example_fasta.fasta\",\n",
    "                \"Example\"\n",
    "            )\n",
    "        ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        # Create main container\n",
    "        self.first_main_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Upload Peptidomic Data Files:</u></h3>\"),\n",
    "            peptide_box,\n",
    "            widgets.HTML(\"<h3 style='margin-bottom: 0;'><u>MBPDB Data (Optional):</u></h3>\"),\n",
    "            mbpdb_options,\n",
    "            widgets.HTML(\"<h3><u>Upload Protein FASTA Files (Optional):</u></h3>\"),\n",
    "            fasta_box,\n",
    "            widgets.HTML(\"<br>\"),\n",
    "            widgets.HTML(\"<div style='margin-top: 10px;'></div>\"),\n",
    "            self.output_area,\n",
    "        ])\n",
    "\n",
    "        # Register observers\n",
    "        self.pd_uploader.observe(self._on_pd_upload_change, names='value')\n",
    "        self.mbpdb_uploader.observe(self._on_mbpdb_upload_change, names='value')\n",
    "        self.fasta_uploader.observe(self._on_fasta_upload_change, names='value')\n",
    "        self.search_button.on_click(self._on_search_click)\n",
    "\n",
    "    def _extract_sequences(self, df):\n",
    "        \"\"\"Extract sequences from peptidomic data\"\"\"\n",
    "        if 'Sequence' not in df.columns:\n",
    "            # First create Sequence column with NaN values\n",
    "            df['Sequence'] = pd.NA\n",
    "            \n",
    "            def extract_sequence(annotated_seq):\n",
    "                if pd.isna(annotated_seq):\n",
    "                    return pd.NA\n",
    "                \n",
    "                # Split by comma if present to handle multiple sequences\n",
    "                if ',' in annotated_seq:\n",
    "                    sequences = []\n",
    "                    for seq in annotated_seq.split(','):\n",
    "                        seq = seq.strip()\n",
    "                        # Handle [X].SEQUENCE.[X] format\n",
    "                        if '.' in seq:\n",
    "                            parts = seq.split('.')\n",
    "                            if len(parts) > 1:\n",
    "                                sequences.append(parts[1])\n",
    "                        # Handle plain sequence\n",
    "                        else:\n",
    "                            sequences.append(seq)\n",
    "                    return sequences\n",
    "                \n",
    "                # Single sequence case\n",
    "                # Handle [X].SEQUENCE.[X] format\n",
    "                if '.' in annotated_seq:\n",
    "                    parts = annotated_seq.split('.')\n",
    "                    if len(parts) > 1:\n",
    "                        return parts[1]\n",
    "                \n",
    "                # Handle plain sequence\n",
    "                return annotated_seq\n",
    "            \n",
    "            # Apply the extraction function and explode the results\n",
    "            df['Sequence'] = df['Annotated Sequence'].apply(extract_sequence)\n",
    "            # Explode sequences if they're in a list (from comma separation)\n",
    "            df = df.explode('Sequence')\n",
    "            \n",
    "        return df['Sequence'].dropna().unique().tolist()\n",
    "\n",
    "    def _on_pd_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    self.pd_results, pd_status = self._load_data(\n",
    "                        file_data,\n",
    "                        required_columns=['Positions in Proteins'],\n",
    "                        file_type='Peptidomic'\n",
    "                    )\n",
    "                    if pd_status == 'yes' and self.pd_results is not None:\n",
    "                        self._find_missing_proteins()\n",
    "                        pass\n",
    "                        #display(HTML(f'<b style=\"color:green;\">Peptidomic data imported with {self.pd_results.shape[0]} rows and {self.pd_results.shape[1]} columns.</b>'))\n",
    "\n",
    "    def _on_mbpdb_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    self.mbpdb_results, mbpdb_status = self._load_data(\n",
    "                        file_data,\n",
    "                        required_columns=['Search peptide', 'Protein ID', 'Peptide'],\n",
    "                        file_type='MBPDB'\n",
    "                    )\n",
    "                    if mbpdb_status == 'yes' and self.mbpdb_results is not None:\n",
    "                        self.mbpdb_results.rename(columns={\n",
    "                            'Search peptide': 'search_peptide',\n",
    "                            'Protein ID': 'protein_id',\n",
    "                            'Peptide': 'peptide',\n",
    "                            'Protein description': 'protein_description',\n",
    "                            'Species': 'species',\n",
    "                            'Intervals': 'intervals',\n",
    "                            'Function': 'function',\n",
    "                            'Additional details': 'additional_details',\n",
    "                            'IC50 (Î¼M)': 'ic50',\n",
    "                            'Inhibition type': 'inhibition_type',\n",
    "                            'Inhibited microorganisms': 'inhibited_microorganisms',\n",
    "                            'PTM': 'ptm',\n",
    "                            'Title': 'title',\n",
    "                            'Authors': 'authors',\n",
    "                            'Abstract': 'abstract',\n",
    "                            'DOI': 'doi',\n",
    "                            'Search type': 'search_type',\n",
    "                            'Scoring matrix': 'scoring_matrix',\n",
    "                        }, inplace=True)\n",
    "                        # display(HTML(f'<b style=\"color:green;\">MBPDB file imported with {self.mbpdb_results.shape[0]} rows and {self.mbpdb_results.shape[1]} columns</b>'))\n",
    "\n",
    "    def _on_fasta_upload_change(self, change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            new_proteins = {}\n",
    "            with self.output_area:\n",
    "                self.output_area.clear_output()\n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    for file_data in change['new']:\n",
    "                        try:\n",
    "                            if file_data.name.endswith('.fasta'):\n",
    "                                self.fasta_filename = file_data.name\n",
    "                                parsed = self._parse_uploaded_fasta(file_data)\n",
    "                                new_proteins.update(parsed)\n",
    "                                #print(f\" DebugParsed {len(parsed)} proteins from {file_data.name}\")\n",
    "                                #display(HTML(f'<b style=\"color:green;\">Successfully imported {file_data.name}</b>'))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error: {str(e)}\")\n",
    "\n",
    "                    # Update protein_dict with new data\n",
    "                    self.protein_dict = new_proteins\n",
    "                    self.fasta_uploader_placeholder.value=True\n",
    "                    self._find_missing_proteins()\n",
    "                    #print(f\"Debug: Updated protein_dict with {len(new_proteins)} entries\")\n",
    "\n",
    "    def _load_data(self, file_obj, required_columns, file_type):\n",
    "        \"\"\"\n",
    "        Load and validate uploaded data files, cleaning empty rows and validating data.\n",
    "        \n",
    "        Args:\n",
    "            file_obj: Uploaded file object\n",
    "            required_columns (list): List of required column names (either single names or pairs)\n",
    "            file_type (str): Type of file being loaded ('MBPDB' or 'Peptidomic')\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (DataFrame or None, status string 'yes'/'no')\n",
    "        \"\"\"\n",
    "        try:\n",
    "            content = file_obj.content\n",
    "            filename = file_obj.name\n",
    "            extension = filename.split('.')[-1].lower()\n",
    "            \n",
    "            file_stream = io.BytesIO(content)\n",
    "            \n",
    "            # Load data based on file extension with multiple delimiter attempts\n",
    "            if extension == 'csv':\n",
    "                # Try different delimiters in order of common usage\n",
    "                delimiters = [',', ';', '|', '\\t']\n",
    "                df = None\n",
    "                successful_delimiter = None\n",
    "                \n",
    "                for delimiter in delimiters:\n",
    "                    try:\n",
    "                        # Reset file stream position\n",
    "                        file_stream.seek(0)\n",
    "                        temp_df = pd.read_csv(file_stream, sep=delimiter)\n",
    "                        \n",
    "                        # Check if we got more than one column\n",
    "                        if len(temp_df.columns) > 1:\n",
    "                            df = temp_df\n",
    "                            successful_delimiter = delimiter\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "                if df is None:\n",
    "                    raise ValueError(\"Could not parse CSV file with any common delimiter (tried: comma, semicolon, pipe, tab)\")\n",
    "                \n",
    "                # Show which delimiter was used\n",
    "                #display(HTML(f'<b style=\"color:blue;\">File parsed using delimiter: {successful_delimiter}</b>'))\n",
    "                \n",
    "            elif extension in ['txt', 'tsv']:\n",
    "                # For txt/tsv files, try tab first, then other delimiters\n",
    "                delimiters = ['\\t', ',', ';', '|']\n",
    "                df = None\n",
    "                successful_delimiter = None\n",
    "                \n",
    "                for delimiter in delimiters:\n",
    "                    try:\n",
    "                        file_stream.seek(0)\n",
    "                        temp_df = pd.read_csv(file_stream, sep=delimiter)\n",
    "                        if len(temp_df.columns) > 1:\n",
    "                            df = temp_df\n",
    "                            successful_delimiter = delimiter\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                        \n",
    "                if df is None:\n",
    "                    raise ValueError(\"Could not parse TXT/TSV file with any common delimiter\")\n",
    "                    \n",
    "                #display(HTML(f'<b style=\"color:blue;\">File parsed using delimiter: {successful_delimiter}</b>'))\n",
    "                \n",
    "            elif extension == 'xlsx':\n",
    "                df = pd.read_excel(file_stream)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file format. Please upload .csv, .txt, .tsv, or .xlsx files.\")\n",
    "            \n",
    "            # Clean column names\n",
    "            df.columns = df.columns.str.strip()\n",
    "            \n",
    "            # Drop empty rows\n",
    "            df = df.dropna(how='all')\n",
    "            df = df[~(df.astype(str).apply(lambda x: x.str.strip().eq('')).all(axis=1))]\n",
    "            \n",
    "            # Handle validation differently based on file type\n",
    "            if file_type == 'MBPDB':\n",
    "                self.mbpdb_filename = filename\n",
    "\n",
    "                # Use column pairs for MBPDB validation\n",
    "                column_pairs = {\n",
    "                    'Search peptide': 'search_peptide',\n",
    "                    'Protein ID': 'protein_id',\n",
    "                    'Peptide': 'peptide'\n",
    "                }\n",
    "                \n",
    "                # Check for required columns in either format\n",
    "                missing_pairs = []\n",
    "                for orig_col, std_col in column_pairs.items():\n",
    "                    if not (orig_col in df.columns or std_col in df.columns):\n",
    "                        missing_pairs.append(f\"'{orig_col}' or '{std_col}'\")\n",
    "                \n",
    "                if missing_pairs:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Missing required columns: {\", \".join(missing_pairs)}</b>'))\n",
    "                    return None, 'no'\n",
    "                \n",
    "                # Validate non-empty required columns\n",
    "                empty_pairs = []\n",
    "                for orig_col, std_col in column_pairs.items():\n",
    "                    col_to_check = orig_col if orig_col in df.columns else std_col\n",
    "                    if df[col_to_check].isna().all() or (df[col_to_check].astype(str).str.strip() == '').all():\n",
    "                        empty_pairs.append(f\"'{orig_col}' or '{std_col}'\")\n",
    "                \n",
    "                if empty_pairs:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Required columns are empty: {\", \".join(empty_pairs)}</b>'))\n",
    "                    return None, 'no'\n",
    "                    \n",
    "            else:\n",
    "                self.pd_filename = filename\n",
    "\n",
    "                # Additional column mapping specifically for peptidomic data\n",
    "                if file_type == 'Peptidomic':\n",
    "                    peptidomic_column_mapping = {\n",
    "                        'Position.in.Proteins': 'Positions in Proteins',\n",
    "                        'Positions.in.Proteins': 'Positions in Proteins',\n",
    "                        'Master.Protein.Accessions': 'Master Protein Accessions',\n",
    "                        'Master.Protein.Accession': 'Master Protein Accessions',\n",
    "                        'Protein.Accessions': 'Protein Accessions',\n",
    "                        'Protein.Accession': 'Protein Accessions',\n",
    "                    }\n",
    "                    # Apply peptidomic-specific column mapping\n",
    "                    df.columns = [peptidomic_column_mapping.get(col, col) for col in df.columns]\n",
    "\n",
    "                # Standard validation for other file types\n",
    "                if not set(required_columns).issubset(df.columns):\n",
    "                    missing = set(required_columns) - set(df.columns)\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Missing required columns: {\", \".join(missing)}</b>'))\n",
    "                    return None, 'no'\n",
    "                \n",
    "                # Validate non-empty required columns\n",
    "                empty_required = []\n",
    "                for col in required_columns:\n",
    "                    if df[col].isna().all() or (df[col].astype(str).str.strip() == '').all():\n",
    "                        empty_required.append(col)\n",
    "                \n",
    "                if empty_required:\n",
    "                    display(HTML(f'<b style=\"color:red;\">{file_type} File Error: Required columns are empty: {\", \".join(empty_required)}</b>'))\n",
    "                    return None, 'no'\n",
    "            \n",
    "            # Show success message\n",
    "            #display(HTML(f'<b style=\"color:green;\">{file_type} file loaded successfully with {len(df)} rows after cleaning.</b>'))\n",
    "            \n",
    "            return df, 'yes'\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(HTML(f'<b style=\"color:red;\">{file_type} File Error: {str(e)}</b>'))\n",
    "            return None, 'no'\n",
    "    \n",
    "    def _parse_uploaded_fasta(self, file_data):\n",
    "        \"\"\"Parse uploaded FASTA file content\"\"\"\n",
    "        fasta_dict = {}\n",
    "        fasta_text = bytes(file_data.content).decode('utf-8')\n",
    "        lines = fasta_text.split('\\n')\n",
    "\n",
    "        protein_id = \"\"\n",
    "        protein_name = \"\"\n",
    "        sequence = \"\"\n",
    "        species = \"\"\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if protein_id:\n",
    "                    fasta_dict[protein_id] = {\n",
    "                        \"name\": protein_name,\n",
    "                        \"sequence\": sequence,\n",
    "                        \"species\": species\n",
    "                    }\n",
    "                sequence = \"\"\n",
    "                header_parts = line[1:].split('|')\n",
    "                if len(header_parts) > 2:\n",
    "                    protein_id = header_parts[1]\n",
    "                    protein_name_full = re.split(r' OS=', header_parts[2])[0]\n",
    "                    if ' ' in protein_name_full:\n",
    "                        protein_name = protein_name_full\n",
    "                    else:\n",
    "                        protein_name = protein_name_full\n",
    "                    species = self._find_species(line)\n",
    "            else:\n",
    "                sequence += line\n",
    "\n",
    "        if protein_id:\n",
    "            fasta_dict[protein_id] = {\n",
    "                \"name\": protein_name,\n",
    "                \"sequence\": sequence,\n",
    "                \"species\": species\n",
    "            }\n",
    "\n",
    "        return fasta_dict\n",
    "\n",
    "    def _find_species(self, header):\n",
    "        \"\"\"Find species in FASTA header\"\"\"\n",
    "        header_lower = header.lower()\n",
    "        for spec_group in spec_translate_list:\n",
    "            for term in spec_group[1:]:\n",
    "                if term.lower() in header_lower:\n",
    "                    return spec_group[0]\n",
    "        return \"unknown\"\n",
    "    \n",
    "    def _find_missing_proteins(self):\n",
    "        \"\"\"\n",
    "        Find proteins in the data that are not in the protein dictionary.\n",
    "        \n",
    "        Returns:\n",
    "            set: Set of protein accessions not in the dictionary\n",
    "        \"\"\"\n",
    "        # Get the protein dictionary\n",
    "        #protein_dict = self.protein_dict.copy()\n",
    "        \n",
    "        # Get all protein accessions from pd_results\n",
    "        if self.pd_results is None or self.pd_results.empty:\n",
    "            return set()\n",
    "            \n",
    "        if 'Master Protein Accessions' not in self.pd_results.columns:\n",
    "            return set()\n",
    "        self.missing_proteins = set()\n",
    "        # Extract all protein accessions and split any that contain semicolons\n",
    "        protein_accessions = self.pd_results['Master Protein Accessions'].dropna().unique()\n",
    "        unique_protein_list = set()\n",
    "        \n",
    "        for protein in protein_accessions:\n",
    "            if isinstance(protein, str) and ';' in protein:\n",
    "                # Split the string by semicolon and strip whitespace\n",
    "                split_proteins = [p.strip() for p in protein.split(';')]\n",
    "                unique_protein_list.update(split_proteins)\n",
    "            elif isinstance(protein, str):\n",
    "                unique_protein_list.add(protein.strip())\n",
    "            elif protein is not None:\n",
    "                unique_protein_list.add(str(protein))\n",
    "        \n",
    "        # Also check Positions in Proteins column if available\n",
    "        if 'Positions in Proteins' in self.pd_results.columns:\n",
    "            pos_proteins = self.pd_results['Positions in Proteins'].dropna().unique()\n",
    "            \n",
    "            for position in pos_proteins:\n",
    "                if isinstance(position, str) and ';' in position:\n",
    "                    # Split the string by semicolon\n",
    "                    for pos in position.split(';'):\n",
    "                        parts = pos.strip().split()\n",
    "                        if parts and not parts[0].startswith('['):  # Skip bracketed parts\n",
    "                            unique_protein_list.add(parts[0])\n",
    "                elif isinstance(position, str):\n",
    "                    parts = position.strip().split()\n",
    "                    if parts and not parts[0].startswith('['):\n",
    "                        unique_protein_list.add(parts[0])\n",
    "        \n",
    "        # Remove None, nan, and 'Unknown' values\n",
    "        clean_protein_list = set()\n",
    "        for protein in unique_protein_list:\n",
    "            if protein and protein != 'Unknown' and protein != 'nan':\n",
    "                clean_protein_list.add(protein)\n",
    "        \n",
    "        # Find proteins not in the dictionary\n",
    "        self.missing_proteins = {p for p in clean_protein_list if p not in self.protein_dict}\n",
    "        \n",
    "    def display_widgets(self):\n",
    "        \"\"\"Return the data loading UI with integrated search and help tooltips\"\"\"\n",
    "        # Create and return a VBox containing both the CSS and main container\n",
    "        css = widgets.HTML(\"\"\"\n",
    "            <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css\">\n",
    "        \"\"\")\n",
    "        \n",
    "        return widgets.VBox([css, self.first_main_container])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0ca14a-1665-43ec-abde-8adb7fc5c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupProcessing:\n",
    "    def __init__(self):\n",
    "        self.group_data = {}\n",
    "        self.jsonfilename = None\n",
    "        self.group_number = 1\n",
    "        self.filtered_columns = []\n",
    "        self.notification_widget = widgets.IntText(\n",
    "            value=0,\n",
    "            description='',\n",
    "            layout=widgets.Layout(display='none')  # Hidden from view\n",
    "        )\n",
    "        \n",
    "        self.group_uploader = widgets.FileUpload(\n",
    "        accept='.json',\n",
    "        multiple=False,\n",
    "        description='Upload Groups File',\n",
    "        layout=widgets.Layout(width='200px'),\n",
    "        style={'description_width': 'initial'},\n",
    "        disabled = True\n",
    "        )\n",
    "\n",
    "        # Initialize output areas\n",
    "        self.output = widgets.Output(overflow='hidden')\n",
    "        # Initialize widgets for group selection\n",
    "        self.column_dropdown = widgets.SelectMultiple(\n",
    "            #description='Absorbance ',\n",
    "            style={'description_width': 'initial'},\n",
    "            disabled=True,\n",
    "            layout=widgets.Layout(width='225px', height='300px')\n",
    "        )\n",
    "\n",
    "        self.column_dropdown_box = widgets.HBox([\n",
    "            widgets.HTML(\"Absorbance Columns:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"),\n",
    "            self.column_dropdown\n",
    "            ],layout=widgets.Layout(width='400px', height='310px')\n",
    "            )\n",
    "\n",
    "        self.grouping_variable_text = widgets.Text(\n",
    "            #description='Assign New Group Name',\n",
    "            layout=widgets.Layout(width='230px'),\n",
    "            style={'description_width': 'initial'},\n",
    "            disabled=True,\n",
    "            placeholder='Search columns or enter new group name'\n",
    "        )\n",
    "        self.no_group_checkbox = widgets.Checkbox(\n",
    "            value=False,\n",
    "        )\n",
    "        self.text_box = widgets.HBox([\n",
    "            widgets.HTML(\"Search or Assigned Name:\"),\n",
    "            self.grouping_variable_text\n",
    "        ], layout=widgets.Layout(margin='5px', height = 'auto'))       \n",
    "\n",
    "        # Initialize buttons\n",
    "        self.no_group_button = widgets.Button(\n",
    "            description='No Groups',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(margin='5px', height = 'auto'),\n",
    "            disabled=True  # Start disabled\n",
    "        )\n",
    "\n",
    "        self.search_button = widgets.Button(\n",
    "            description='Search',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(margin='5px', height = 'auto'),\n",
    "            disabled=True  # Start disabled\n",
    "        )\n",
    "        \n",
    "        self.add_group_button = widgets.Button(\n",
    "            description='Add Group',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(margin='5px', height = 'auto'),\n",
    "            disabled=True  # Start disabled\n",
    "        )\n",
    "        \n",
    "        self.reset_file_button = widgets.Button(\n",
    "            description='Reset Selection',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(margin='10px 0', width = '90%'),\n",
    "            disabled=True  # Start disabled\n",
    "        )\n",
    "            \n",
    "        # Set up button callbacks\n",
    "        self.search_button.on_click(self._search_columns)\n",
    "        self.add_group_button.on_click(self._add_group)\n",
    "        self.no_group_button.on_click(self._no_group)\n",
    "        self.reset_file_button.on_click(self._reset_selection)\n",
    "\n",
    "        # set up observer for group uploader and no group checkbox\n",
    "        self.group_uploader.observe(self._on_group_upload_change, names='value')\n",
    "    \n",
    "    def enable_widgets(self, enable=True):\n",
    "        \"\"\"Enable or disable group processing widgets based on protein mapping completion\"\"\"\n",
    "        # Enable/disable the main input widgets\n",
    "        self.column_dropdown.disabled = not enable\n",
    "        self.grouping_variable_text.disabled = not enable\n",
    "        self.no_group_button.disabled = not enable\n",
    "        self.search_button.disabled = not enable\n",
    "        self.add_group_button.disabled = not enable\n",
    "        self.reset_file_button.disabled = not enable\n",
    "        self.group_uploader.disabled = not enable\n",
    "        \n",
    "        # If enabling, also update the column options from the current data\n",
    "        if enable and hasattr(self, 'pd_results_cleaned') and self.pd_results_cleaned is not None:\n",
    "            self.update_data(self.pd_results_cleaned)    \n",
    "\n",
    "    def setup_data(self):\n",
    "        \"\"\"Initialize data and filters for the analysis\"\"\"\n",
    "        # Define columns to exclude with more flexible matching\n",
    "        columns_to_exclude = [\n",
    "            # Space-separated and dot-separated variants included\n",
    "            'Marked as', 'Marked.as',\n",
    "            'Number of Missed Cleavages', 'Number.of.Missed.Cleavages',\n",
    "            'Missed Cleavages', 'Missed.Cleavages',\n",
    "            'Checked', 'Confidence', 'Annotated Sequence', 'Annotated.Sequence',\n",
    "            'Unnamed: 3', 'Unnamed:.3',\n",
    "            'Modifications', 'Modifications.in.Proteins', 'Modifications.in.Master.Proteins',\n",
    "            'Protein Groups', 'Protein.Groups',\n",
    "            'Proteins', 'PSMs',\n",
    "            'Master Protein Accessions', 'Master.Protein.Accessions',\n",
    "            'Master Protein Descriptions', 'Master.Protein.Descriptions',\n",
    "            'Description',\n",
    "            'Positions in Master Proteins', 'Positions.in.Master.Proteins',\n",
    "            'Positions in Proteins', 'Positions.in.Proteins',\n",
    "            'Modifications in Master Proteins', 'Modifications.in.Master.Proteins',\n",
    "            'Modifications in Master Proteins all Sites', 'Modifications.in.Master.Proteins.all.Sites',\n",
    "            'Theo MHplus in Da', 'Theo.MHplus.in.Da',\n",
    "            'Quan Info', 'Quan.Info',\n",
    "            \"Theo. MH+ [Da]\", \"Theo.MH+.[Da]\",\n",
    "            'Confidence by Search Engine', 'Confidence.by.Search.Engine',\n",
    "            'q-Value by Search Engine', 'q-Value.by.Search.Engine',\n",
    "            'XCorr by Search Engine', 'XCorr.by.Search.Engine',\n",
    "            'Percolator PEP by Search Engine', 'Percolator.PEP.by.Search.Engine',\n",
    "            'Percolator q-Value by Search Engine', 'Percolator.q-Value.by.Search.Engine',\n",
    "            'Percolator SVMScore by Search Engine', 'Percolator.SVMScore.by.Search.Engine',\n",
    "            'PEP', 'q-Value', 'RT in min', 'RT.in.min',\n",
    "            'RT in min by Search Engine', 'RT.in.min.by.Search.Engine',\n",
    "            'Sequence', 'Sequence Length', 'Sequence.Length',\n",
    "            'search_peptide', 'Peptide', 'protein_id', 'protein_description',\n",
    "            'Alignment', 'Species',\n",
    "            'Intervals', 'function', 'unique ID', 'unique.ID',\n",
    "            'PEP (by Search Engine): Sequest HT', 'PEP.(by.Search.Engine):.Sequest.HT',\n",
    "            'SVM Score (by Search Engine): Sequest HT', 'SVM.Score.(by.Search.Engine):.Sequest.HT',\n",
    "            'SVM_Score',\n",
    "            'XCorr (by Search Engine): Sequest HT', 'XCorr.(by.Search.Engine):.Sequest.HT',\n",
    "            'Qvality PEP', 'Qvality.PEP',\n",
    "            'Qvality q-value', 'Qvality.q-value',\n",
    "            'Top Apex RT [min]', 'Top.Apex.RT.[min]',\n",
    "            'Top Apex RT in min', 'Top.Apex.RT.in.min',\n",
    "            'start', 'stop',\n",
    "            'Abundance Ratio', 'Abundance.Ratio',\n",
    "            'Abundance Ratio Adj P-Value', 'Abundance.Ratio.Adj.P-Value',\n",
    "            'Abundance Ratio log2', 'Abundance.Ratio.log2',\n",
    "            'Abundance Ratio P-Value', 'Abundance.Ratio.P-Value',\n",
    "            'Abundances', 'Abundances.Counts', 'Abundances.Grouped', 'Abundances.Grouped.Count',\n",
    "            'Abundances.Grouped.CV', 'Abundances.Normalized', 'Abundances.Scaled',\n",
    "            'Charge by Search Engine', 'Charge.by.Search.Engine',\n",
    "            'Concatenated Rank by Search Engine', 'Concatenated.Rank.by.Search.Engine',\n",
    "            'Delta Cn by Search Engine', 'Delta.Cn.by.Search.Engine',\n",
    "            'Delta M in ppm by Search Engine', 'Delta.M.in.ppm.by.Search.Engine',\n",
    "            'Delta mz in Da by Search Engine', 'Delta.mz.in.Da.by.Search.Engine',\n",
    "            'Delta Score by Search Engine', 'Delta.Score.by.Search.Engine',\n",
    "            'Found in Sample Groups', 'Found.in.Sample.Groups',\n",
    "            'Found in Samples', 'Found.in.Samples',\n",
    "            'Modifications all possible sites', 'Modifications.all.possible.sites',\n",
    "            'mz in Da by Search Engine', 'mz.in.Da.by.Search.Engine',\n",
    "            'Number of Isoforms', 'Number.of.Isoforms',\n",
    "            'Number of Protein Groups', 'Number.of.Protein.Groups',\n",
    "            'Number of Proteins', 'Number.of.Proteins',\n",
    "            'Protein Accessions', 'Protein.Accessions',\n",
    "            'PSM Ambiguity', 'PSM.Ambiguity',\n",
    "            'Rank by Search Engine', 'Rank.by.Search.Engine',\n",
    "            'Search Engine Rank by Search Engine', 'Search.Engine.Rank.by.Search.Engine',\n",
    "            'Score CHIMERYS Identification (by search engine)', 'Score.CHIMERYS.Identification.(by.search.engine)'\n",
    "        ]\n",
    "\n",
    "        exclude_substrings = [\n",
    "            'Abundances by Bio Rep', 'Abundances.by.Bio.Rep',\n",
    "            'Count', 'count',\n",
    "            'Origin', 'origin',\n",
    "            'Average_Abundance', 'Average.Abundance',\n",
    "            'Avg_', 'Avg.',\n",
    "            'PEP by Search Engine', 'PEP.by.Search.Engine',\n",
    "            'SVM Score by Search Engine', 'SVM.Score.by.Search.Engine',\n",
    "            'XCorr by Search Engine', 'XCorr.by.Search.Engine',\n",
    "            'Top Apex RT', 'Top.Apex.RT'\n",
    "        ]\n",
    "    \n",
    "        # Use cleaned data if available, otherwise use original\n",
    "        df = self.pd_results_cleaned if (hasattr(self, 'pd_results_cleaned') and \n",
    "                                       not self.pd_results_cleaned.empty) else self.pd_results\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            # More flexible column filtering\n",
    "            self.filtered_columns = []\n",
    "            for col in df.columns:\n",
    "                # Check if any exclusion pattern matches the column name\n",
    "                should_exclude = any(excl.lower() in col.lower() for excl in columns_to_exclude)\n",
    "                # Check if any substring pattern matches\n",
    "                has_excluded_substring = any(sub.lower() in col.lower() for sub in exclude_substrings)\n",
    "                \n",
    "                if not should_exclude and not has_excluded_substring:\n",
    "                    self.filtered_columns.append(col)\n",
    "              \n",
    "            # Update dropdown options\n",
    "            self.column_dropdown.options = self.filtered_columns\n",
    "            self._reset_inputs()\n",
    "        else:\n",
    "            self.filtered_columns = []\n",
    "            self.column_dropdown.options = []\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:red;\">No valid data available for processing.</b>'))\n",
    "   \n",
    "    def update_data(self, pd_results):\n",
    "        \"\"\"Update data and refresh filtered columns\"\"\"\n",
    "        self.pd_results = pd_results\n",
    "        \n",
    "        # Only update if we have valid data\n",
    "        if pd_results is not None:\n",
    "            self.setup_data()\n",
    "            \n",
    "            # Update the dropdown with new filtered columns\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                #display(widgets.HTML('<b style=\"color:green;\">Data updated successfully. Column selection refreshed.</b>'))\n",
    "    \n",
    "    def create_download_link(self, file_path, label):\n",
    "        \"\"\"Create a download link for a file.\"\"\"\n",
    "        if os.path.exists(file_path):\n",
    "            # Read file content and encode it as base64\n",
    "            with open(file_path, 'rb') as f:\n",
    "                content = f.read()\n",
    "            b64_content = base64.b64encode(content).decode('utf-8')\n",
    "    \n",
    "            # Generate the download link HTML\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <a download=\"{os.path.basename(file_path)}\" \n",
    "                   href=\"data:application/octet-stream;base64,{b64_content}\" \n",
    "                   style=\"color: #0366d6; text-decoration: none; margin-left: 20px; font-size: 14px;\">\n",
    "                    {label}\n",
    "                </a>\n",
    "            \"\"\")\n",
    "        else:\n",
    "            # Show an error message if the file does not exist\n",
    "            return widgets.HTML(f\"\"\"\n",
    "                <span style=\"color: red; margin-left: 20px; font-size: 14px;\">\n",
    "                    File \"{file_path}\" not found!\n",
    "                </span>\n",
    "            \"\"\")\n",
    "\n",
    "    def display_widgets(self):\n",
    "        \"\"\"Display the main UI for group selection\"\"\"\n",
    "        group_box_uploader = widgets.HBox([\n",
    "            self.group_uploader,\n",
    "            self.create_download_link(\"examples/example_group_definition.json\", \"Example\")\n",
    "        ], layout=widgets.Layout(align_items='center', overflow='hidden'))\n",
    "\n",
    "        self.group_box_uploader = widgets.VBox([\n",
    "            widgets.HTML(\"<u><b>Option 1:</b> Upload Existing Group Dictionary:</u>\"),\n",
    "            group_box_uploader,\n",
    "        ])\n",
    "        \n",
    "        self.group_box = widgets.VBox([\n",
    "            self.column_dropdown_box,\n",
    "            #widgets.HBox([widgets.HTML(\"Then Choose Option 2 <b>OR</b> Option 3\")], layout=widgets.Layout(width='100%', height='50px', overflow='hidden', justify_content='center')),\n",
    "            widgets.HTML(\"<u><b>Option 2:</b> Assigning Selected Columns to a Group:</u>\"),\n",
    "            self.text_box,\n",
    "            widgets.HBox([self.search_button, self.add_group_button], layout=widgets.Layout(width='100%', height='auto', overflow='hidden', justify_content='center')),\n",
    "            widgets.HBox([widgets.HTML(\"<h4><b>-OR-</b></h4>\")], layout=widgets.Layout(width='100%', height='50px', overflow='hidden', justify_content='center')),\n",
    "            widgets.HTML(\"<u><b>Option 3:</b> Use Selected Columns without Assigning to a Group:</u>\"),\n",
    "            widgets.HBox([self.no_group_button], layout=widgets.Layout(width='100%', height='auto', overflow='hidden', justify_content='center')),\n",
    "            ], layout=widgets.Layout(width='100%', overflow='hidden', height='auto', maxheigh='550px'))\n",
    "        \n",
    "        # Create main grid container\n",
    "        grid = widgets.GridspecLayout(1, 2,  # Number of rows and columns\n",
    "                                     width='850px', \n",
    "                                     heigth='700px',\n",
    "                                     grid_gap='5px',  # Adjust spacing between grid elements\n",
    "                                     )\n",
    "        \n",
    "        # Create input container with vertical scroll\n",
    "        input_container = widgets.VBox([\n",
    "            self.group_box_uploader,\n",
    "            widgets.HBox([widgets.HTML(\"<h4><b>-OR-</b></h4>\")], layout=widgets.Layout(width='100%', height='auto', overflow='hidden', justify_content='center')),\n",
    "            widgets.HTML('Select the <u>absorbance columns</u>'),\n",
    "            self.group_box,\n",
    "            widgets.HBox([self.reset_file_button], layout=widgets.Layout(width='100%', height='auto', overflow='hidden', justify_content='center'))\n",
    "        ], layout=widgets.Layout(\n",
    "            width='400px',\n",
    "            height='100%',\n",
    "            overflow='hidden'  # Add vertical scroll\n",
    "        ))\n",
    "        \n",
    "        # Create output container with vertical scroll\n",
    "        output_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3><u>Group Selection Results:</u></h3>\"),\n",
    "            self.output\n",
    "        ], layout=widgets.Layout(\n",
    "            width='400px',\n",
    "            height='680px',\n",
    "            max_height='680px',\n",
    "            overflow='auto',  # Add vertical scroll\n",
    "            padding='10px'\n",
    "        ))\n",
    "        \n",
    "        # Add to grid\n",
    "        grid[0, 0] = input_container  # Left column\n",
    "        grid[0, 1] = output_container  # Right column\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "    def _on_gd_submit(self, b, dropdown):\n",
    "        \"\"\"Handle JSON file submission\"\"\"\n",
    "        selected_file = dropdown.value\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            \n",
    "            if selected_file == 'Select an existing grouping dictionary file':\n",
    "                print(\"Please select a valid file.\")\n",
    "                return\n",
    "                \n",
    "            try:\n",
    "                # Load and process JSON file\n",
    "                with open(selected_file, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                self.group_data = {}\n",
    "                \n",
    "\n",
    "                for group_number, group_info in data.items():\n",
    "                    group_name = group_info.get('grouping_variable')\n",
    "                    selected_columns = group_info.get('abundance_columns')\n",
    "                    \n",
    "                    self.group_data[group_number] = {\n",
    "                        'grouping_variable': group_name,\n",
    "                        'abundance_columns': selected_columns\n",
    "                    }\n",
    "                    \n",
    "                    display(widgets.HTML(\n",
    "                        f\"<b>Group {group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"\n",
    "                    ))\n",
    "                    display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "                    display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "                    display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "                    \n",
    "            #display(widgets.HTML(f'<b style=\"color:green;\">Successfully uploaded: {selected_file}</b>'))\n",
    "                \n",
    "            except Exception as e:\n",
    "                display(widgets.HTML(f\"<b style='color:red;'>An error occurred while processing the file: {str(e)}</b>\"))\n",
    "    \n",
    "    def _search_columns(self, b):\n",
    "        \"\"\"Search for columns based on group name\"\"\"\n",
    "        group_name = self.grouping_variable_text.value\n",
    "        if group_name:\n",
    "            matching_columns = [col for col in self.filtered_columns if group_name in col]\n",
    "            self.column_dropdown.value = matching_columns\n",
    "        else:\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:red;\">Please enter a group name to search.</b>'))\n",
    "    \n",
    "    def _reset_selection(self, b):\n",
    "        \"\"\"Reset all selections and data\"\"\"\n",
    "        self.group_data = {}\n",
    "        self.group_number = 1\n",
    "        # Add this line to reset notification widget\n",
    "        self.notification_widget.value = 0\n",
    "        with self.output:\n",
    "            clear_output()\n",
    "        self._reset_inputs()\n",
    "        self.no_group_checkbox.value = False\n",
    "    \n",
    "    def _reset_inputs(self):\n",
    "        \"\"\"Reset input fields\"\"\"\n",
    "        self.grouping_variable_text.value = ''\n",
    "        self.column_dropdown.value = ()\n",
    "    \n",
    "    def _add_group(self, b):\n",
    "        \"\"\"Add a new group to the data\"\"\"\n",
    "        group_name = self.grouping_variable_text.value\n",
    "        selected_columns = list(self.column_dropdown.value)\n",
    "        \n",
    "        if not (group_name and selected_columns):\n",
    "            with self.output:\n",
    "                display(widgets.HTML('<b style=\"color:red;\">Please enter a group name and select at least one column.</b>'))\n",
    "            return\n",
    "        \n",
    "        # If group_data exists, use next number, otherwise start at 1\n",
    "        if self.group_data:\n",
    "            # Convert existing keys to integers and find max\n",
    "            existing_numbers = [int(k) for k in self.group_data.keys()]\n",
    "            next_number = max(existing_numbers) + 1\n",
    "            self.group_number = str(next_number)\n",
    "        else:\n",
    "            self.group_data = {}\n",
    "            self.group_number = \"1\"\n",
    "        \n",
    "        # Add new group data to the dictionary\n",
    "        self.group_data[self.group_number] = {\n",
    "            'grouping_variable': group_name,\n",
    "            'abundance_columns': selected_columns\n",
    "        }\n",
    "        \n",
    "        # Display output\n",
    "        with self.output:\n",
    "            display(widgets.HTML(f\"<b>Group {self.group_number}</b> created with <b>{len(selected_columns)} columns assigned</b>.\"))\n",
    "            display(widgets.HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "            display(widgets.HTML(f\"<b>Selected Columns:</b> {', '.join(selected_columns)}\"))\n",
    "            display(widgets.HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "\n",
    "        # Update the Step 3 message to show option 2 as completed\n",
    "        self.notification_widget.value = 2\n",
    "\n",
    "        self._reset_inputs()\n",
    "            \n",
    "    def _on_group_upload_change(self, change):\n",
    "        \"\"\"Handle JSON file upload\"\"\"\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with self.output:\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                if change['new'] and len(change['new']) > 0:\n",
    "                    file_data = change['new'][0]\n",
    "                    try:\n",
    "                        content = bytes(file_data.content).decode('utf-8')\n",
    "                        simplified_data = json.loads(content)\n",
    "                        self.jsonfilename = file_data.name\n",
    "                        \n",
    "                        # Check for available columns in the dataset\n",
    "                        available_columns = set(self.filtered_columns)\n",
    "                        missing_columns = []\n",
    "                        \n",
    "                        # Check each column in each group for availability\n",
    "                        for group_name, columns in simplified_data.items():\n",
    "                            for column in columns:\n",
    "                                if column not in available_columns:\n",
    "                                    missing_columns.append(column)\n",
    "                            \n",
    "                        # Handle missing columns\n",
    "                        if missing_columns:\n",
    "                            missing_unique = list(set(missing_columns))  # Remove duplicates\n",
    "                            missing_unique.sort()  # Sort for consistent display\n",
    "                            \n",
    "                            # Create HTML list for missing columns\n",
    "                            missing_list_html = \"<ul style='color:red; margin-top: 5px; margin-bottom: 5px;'>\"\n",
    "                            for col in missing_unique:\n",
    "                                missing_list_html += f\"<li>{col}</li>\"\n",
    "                            missing_list_html += \"</ul>\"\n",
    "                            \n",
    "                            # Display error message with HTML list\n",
    "                            display(HTML(f\"\"\"\n",
    "                                <div style='color:red; background-color: #fff3e0; padding: 10px; border-left: 5px solid #ff9800; margin: 10px 0;'>\n",
    "                                    <p><b>Error:</b> The following columns from your <b>{file_data.name}</b> are not present in the current dataset:</p>\n",
    "                                    {missing_list_html}\n",
    "                                    <p>Please check your file or adjust your column selection or uploaded peptidomic data.</p>\n",
    "                                </div>\n",
    "                            \"\"\"))\n",
    "                            return\n",
    "                        \n",
    "                        # If no missing columns, proceed with conversion to enumerated format\n",
    "                        self.group_data = {}\n",
    "                        for i, (group_name, abundance_cols) in enumerate(simplified_data.items(), 1):\n",
    "                            group_number = str(i)\n",
    "                            self.group_data[group_number] = {\n",
    "                                'grouping_variable': group_name,\n",
    "                                'abundance_columns': abundance_cols\n",
    "                            }\n",
    "                            \n",
    "                            # Display information about the group\n",
    "                            display(HTML(\n",
    "                                f\"<b>Group {group_number}</b> created with <b>{len(abundance_cols)} columns assigned</b>.\"\n",
    "                            ))\n",
    "                            display(HTML(f\"<b>Grouping Variable:</b> {group_name}\"))\n",
    "                            display(HTML(f\"<b>Selected Columns:</b> {', '.join(abundance_cols)}\"))\n",
    "                            display(HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "                        \n",
    "                        # Update the Step 3 message to show option 1 as completed\n",
    "                        self.notification_widget.value = 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        display(HTML(f\"<b style='color:red;'>An error occurred while processing the file: {str(e)}</b>\"))\n",
    "\n",
    "    def _no_group(self, b):\n",
    "        \"\"\"Handle no-group button click to create individual groups for selected columns\"\"\"\n",
    "        selected_columns = list(self.column_dropdown.value)\n",
    "        \n",
    "        if not selected_columns:\n",
    "            with self.output:\n",
    "                display(HTML('<b style=\"color:red;\">Please select at least one absorbance column before clicking the button.</b>'))\n",
    "                self.no_group_checkbox.value = False  # Reset checkbox\n",
    "            return\n",
    "        \n",
    "        else:    \n",
    "            # Check if the checkbox is checked (using the button's parent widget)\n",
    "            self.no_group_checkbox.value = True  # Set checkbox to checked\n",
    "            \n",
    "            # Clear existing groups\n",
    "            self.group_data = {}\n",
    "            \n",
    "            # Create a new group for each selected column\n",
    "            for i, column in enumerate(selected_columns, 1):\n",
    "                group_number = str(i)\n",
    "                self.group_data[group_number] = {\n",
    "                    'grouping_variable': column,  # Use column name as grouping variable\n",
    "                    'abundance_columns': [column]  # Single column in abundance columns\n",
    "                }\n",
    "            \n",
    "            # Display the results with a simpler message\n",
    "            with self.output:\n",
    "                clear_output()\n",
    "                display(HTML(f\"<b>Individual groups created for {len(selected_columns)} columns:</b>\"))\n",
    "                display(HTML(\"<ul>\"))\n",
    "                for column in selected_columns:\n",
    "                    display(HTML(f\"<li>{column}</li>\"))\n",
    "                display(HTML(\"</ul>\"))\n",
    "                display(HTML(\"<hr style='border: 1px solid black;'>\"))\n",
    "            # Update the Step 3 message to show option 3 as completed\n",
    "            self.notification_widget.value = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7fd494-b273-4018-8b3a-9e0e31867530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinCombinationHandler(HasTraits):\n",
    "    def __init__(self, data_transformer, workflow=None):\n",
    "        super().__init__()\n",
    "        self.data_transformer = data_transformer  # Store reference to data_transformer\n",
    "        self.workflow = workflow  # Store reference to workflow\n",
    "        self.pd_results = data_transformer.pd_results\n",
    "        self.pd_results_cleaned = None\n",
    "        self.protein_output_area = widgets.Output(layout=widgets.Layout(width='100%', margin='5px 0'))\n",
    "        self.user_decisions = {}\n",
    "        self.decision_inputs = []\n",
    "        self.multi_position_combinations = []\n",
    "        self.submit_button = None\n",
    "        self.reset_button = None\n",
    "        self.progress = None\n",
    "        self.protein_mapping_output_area = widgets.Output()\n",
    "        self.uniprot_output_area = widgets.Output()\n",
    "        self.uniprot_client = UniProtClient()\n",
    "        self.bad_protein = set()\n",
    "        self.dead_proteins = set()\n",
    "        self.buttonuniprot_output_area_output = widgets.Output()\n",
    "        self.protein_mapping_widget = widgets.RadioButtons(\n",
    "            options=[('Yes', True), ('No (skip)', False)],\n",
    "            description='Process peptides mapped to multiple proteins?',\n",
    "            disabled=True, \n",
    "            style={'description_width': 'initial'},\n",
    "            value=None\n",
    "        )\n",
    "                # Create a single status message area that will be reused\n",
    "\n",
    "        \n",
    "        self.protein_mapping_widget.observe(self.process_protein_mapping, names='value')\n",
    "        \n",
    "        # Progress indicator\n",
    "        self.progress_uniprot = widgets.FloatProgress(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=100,\n",
    "            description='Progress:',\n",
    "            bar_style='info',\n",
    "            style={'description_width': 'initial', 'bar_color': '#0080ff'},\n",
    "            orientation='horizontal',\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        # Found proteins counter\n",
    "        self.counter_text = widgets.HTML(\n",
    "            value=\"0 / 0 proteins processed\",\n",
    "            layout=widgets.Layout(width='200px'),\n",
    "        )\n",
    "\n",
    "    @property  # Make protein_dict a property that always reads from data_transformer\n",
    "    def protein_dict(self):\n",
    "        return self.data_transformer.protein_dict\n",
    "            \n",
    "    def _fetch_missing_proteins(self, callback=None):\n",
    "        \"\"\"\n",
    "        Fetch missing proteins from UniProt and add them to the protein dictionary.\n",
    "        Only fetch protein name and species (no sequence).\n",
    "        \"\"\"\n",
    "        if not self.data_transformer.missing_proteins:\n",
    "            if callback:\n",
    "                callback()\n",
    "            return\n",
    "            \n",
    "        with self.buttonuniprot_output_area_output:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<u>Protein Information Retrieval from UniProt</u>\"))\n",
    "            display(widgets.VBox([self.progress_uniprot, self.counter_text]))\n",
    "\n",
    "        # Initialize tracking variables\n",
    "        protein_ids = [pid for pid in list(self.data_transformer.missing_proteins) \n",
    "                      if pid not in self.protein_dict and pid not in self.dead_proteins]\n",
    "        \n",
    "        self.progress_uniprot.value = 0\n",
    "        self.counter_text.value = f\"0 / {len(protein_ids)} proteins processed\"\n",
    "        \n",
    "        batch_size = 10\n",
    "        success_count = 0\n",
    "        processed_count = 0\n",
    "        recent_messages = []\n",
    "        max_recent_messages = 3\n",
    "        all_messages = []\n",
    "        try:\n",
    "            # Process in batches\n",
    "            batches = [protein_ids[i:i+batch_size] for i in range(0, len(protein_ids), batch_size)]\n",
    "            \n",
    "            for batch_idx, batch in enumerate(batches):\n",
    "                self.progress_uniprot.value = (batch_idx / len(batches)) * 100\n",
    "                self.counter_text.value = f\"{processed_count} / {len(protein_ids)} proteins processed\"\n",
    "                \n",
    "                try:\n",
    "                    # Batch fetch\n",
    "                    results = self.uniprot_client.fetch_proteins_batch(batch)\n",
    "                    \n",
    "                    # Process batch results\n",
    "                    for protein_id in batch:\n",
    "                        if protein_id in results:\n",
    "                            name, uniprot_species = results[protein_id]\n",
    "                            self.protein_dict[protein_id] = {\n",
    "                                \"name\": name if name else protein_id,\n",
    "                                \"species\": uniprot_species\n",
    "                            }\n",
    "                            success_count += 1\n",
    "                            message = f'<span style=\"color:green;\">â Added {protein_id}: {name} ({uniprot_species})</span>'\n",
    "                        else:\n",
    "                            self.bad_protein.add(protein_id)\n",
    "                            message = f'<span style=\"color:orange;\">Ã No data found for {protein_id} in batch fetch</span>'\n",
    "                        \n",
    "                        recent_messages.append(message)\n",
    "                        if len(recent_messages) > max_recent_messages:\n",
    "                            recent_messages.pop(0)\n",
    "                        \n",
    "                        with self.uniprot_output_area:\n",
    "                            clear_output(wait=True)\n",
    "                            for msg in recent_messages:\n",
    "                                display(HTML(msg))\n",
    "                        \n",
    "                        processed_count += 1\n",
    "                        self.counter_text.value = f\"{processed_count} / {len(protein_ids)} proteins processed\"\n",
    "                \n",
    "                except Exception as batch_error:\n",
    "                    with self.uniprot_output_area:\n",
    "                        display(HTML(f'<span style=\"color:red;\">Batch processing failed: {str(batch_error)}</span>'))\n",
    "                    # Add failed batch proteins to bad_protein set\n",
    "                    self.bad_protein.update(batch)\n",
    "                \n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "            # After all batches, try individual fetches for bad proteins once\n",
    "            bad_proteins_to_process = self.bad_protein.copy()  # Create a copy to iterate over\n",
    "            for protein_id in bad_proteins_to_process:\n",
    "                try:\n",
    "                    name, species, _ = fetch_uniprot_info(protein_id)\n",
    "                    if name is not None and species is not None:\n",
    "                        self.protein_dict[protein_id] = {\n",
    "                            \"name\": name,\n",
    "                            \"species\": species\n",
    "                        }\n",
    "                        self.bad_protein.remove(protein_id)\n",
    "                        success_count += 1\n",
    "                        message = f'<span style=\"color:green;\">â Added {protein_id}: {name} ({species}) [Individual fetch]</span>'\n",
    "                    else:\n",
    "                        self.dead_proteins.add(protein_id)\n",
    "                        self.bad_protein.remove(protein_id)\n",
    "                        self.protein_dict[protein_id] = {\n",
    "                            \"name\": protein_id,\n",
    "                            \"species\": \"Unknown\"\n",
    "                        }\n",
    "                        message = f'<span style=\"color:red;\">Ã No data found for {protein_id} [Individual fetch]</span>'\n",
    "                \n",
    "                except Exception as individual_error:\n",
    "                    self.dead_proteins.add(protein_id)\n",
    "                    self.bad_protein.remove(protein_id)\n",
    "                    self.protein_dict[protein_id] = {\n",
    "                        \"name\": protein_id,\n",
    "                        \"species\": \"Unknown\"\n",
    "                    }\n",
    "                    message = f'<span style=\"color:red;\">Ã Failed individual fetch for {protein_id}: {str(individual_error)}</span>'\n",
    "                \n",
    "                recent_messages.append(message)\n",
    "                all_messages.append(message)\n",
    "                if len(recent_messages) > max_recent_messages:\n",
    "                    recent_messages.pop(0)\n",
    "                \n",
    "                with self.uniprot_output_area:\n",
    "                    clear_output(wait=True)\n",
    "                    for msg in recent_messages:\n",
    "\n",
    "                        display(HTML(msg))\n",
    "\n",
    "                \n",
    "                time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "            # Final status display\n",
    "            self.progress_uniprot.value = 100\n",
    "            with self.uniprot_output_area:\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(f'<b style=\"color:green;\">Successfully processed {success_count} out of {len(protein_ids)} proteins.</b>'))\n",
    "                if self.dead_proteins:\n",
    "                    display(HTML(f'<b style=\"color:orange;\">Unable to fetch data for {len(self.dead_proteins)} proteins: {\", \".join(self.dead_proteins)}</b>'))\n",
    "                display(HTML(f'<b style=\"color:green;\">Protein fetch complete! Moving to protein combination processing...</b>'))\n",
    "            \n",
    "        except Exception as e:\n",
    "            with self.uniprot_output_area:\n",
    "                display(HTML(f'<b style=\"color:red;\">Error in fetch process: {str(e)}</b>'))\n",
    "        \n",
    "        finally:\n",
    "            with self.protein_mapping_output_area:\n",
    "                clear_output(wait=True)\n",
    "                #display(HTML('<b style=\"color:green; margin-top: 10px;\">Proceeding to protein combination processing...</b>'))\n",
    "                display(HTML(\"<br><u>Peptide-to-Protein Mapping</u>\"))\n",
    "                self.pd_results_cleaned, main_container = self.process_protein_combinations()\n",
    "                display(main_container)\n",
    "                if hasattr(self.data_transformer, 'workflow'):\n",
    "                    self.workflow = self.data_transformer.workflow\n",
    "    \n",
    "    def process_protein_mapping(self, change):\n",
    "        \"\"\"Process protein mapping based on user selection\"\"\"\n",
    "        #with self.protein_mapping_output_area:\n",
    "        #    self.protein_mapping_output_area.clear_output(wait=True)\n",
    "            \n",
    "        if self.protein_mapping_widget.value == True:\n",
    "            # First display the widgets for UniProt search\n",
    "            #display(HTML(\"<h3>Step 1: Fetching Missing Protein Information</h3>\"))\n",
    "            # Display the UniProt search components\n",
    "            #uniprot_widgets = self.display_widgets()\n",
    "            #display(uniprot_widgets)\n",
    "            \n",
    "            # Create a callback that will execute after UniProt search completes\n",
    "            def on_uniprot_complete():\n",
    "                # Now we need to directly execute the protein combination processing\n",
    "                with self.protein_mapping_output_area:\n",
    "                    clear_output(wait=True) \n",
    "\n",
    "            \n",
    "            # When UniProt search completes, the callback will be executed\n",
    "            self._on_update_from_uniprot(callback=on_uniprot_complete)\n",
    "        else:\n",
    "            # No (skip) option selected - just process without changes\n",
    "            self.pd_results_cleaned = self.pd_results.copy()\n",
    "            \n",
    "            # For immediate completion in the \"No (skip)\" case\n",
    "            if hasattr(self.data_transformer, 'workflow') and hasattr(self.data_transformer.workflow, '_on_submit_complete'):\n",
    "                self.data_transformer.workflow._on_submit_complete(True)\n",
    "\n",
    "        return self.pd_results_cleaned\n",
    "    \n",
    "    def _on_update_from_uniprot(self, callback=None):\n",
    "        \"\"\"Handle update button click with improved callback handling\"\"\"\n",
    "    \n",
    "        with self.uniprot_output_area:\n",
    "            # Initialize UniProt client if needed\n",
    "            if self.uniprot_client is None:\n",
    "                try:\n",
    "                    self.uniprot_client = UniProtClient()\n",
    "                    display(HTML('<b style=\"color:green;\">UniProt client initialized successfully.</b>'))\n",
    "                except Exception as e:\n",
    "                    display(HTML(f'<b style=\"color:red;\">Error initializing UniProt client: {str(e)}</b>'))\n",
    "                    if callback:\n",
    "                        callback()\n",
    "                    return\n",
    "            \n",
    "            # Find missing proteins\n",
    "            if self.data_transformer.missing_proteins:\n",
    "                # Show only the count of missing proteins\n",
    "                display(HTML(\n",
    "                    f'<b style=\"color:orange;\">Found {len(self.data_transformer.missing_proteins)} proteins missing information.</b><br>' +\n",
    "                    f'<b>Starting UniProt fetch process...</b>'\n",
    "                ))            \n",
    "            else:\n",
    "                with self.protein_mapping_output_area:\n",
    "                    clear_output(wait=True)\n",
    "                    #display(HTML(\"<b style='color:green;'>All proteins in the data have information in the protein dictionary!</b>\"))\n",
    "                    # Directly call process_protein_combinations to create the UI\n",
    "                    # Directly call process_protein_combinations to create the U\n",
    "                    display(HTML(\"<u>Peptide-to-Protein Mapping</u>\"))\n",
    "                    self.pd_results_cleaned, main_container = self.process_protein_combinations()\n",
    "                    display(main_container)\n",
    "                    # Connect the workflow _on_submit_complete method if needed\n",
    "                    if hasattr(self.data_transformer, 'workflow'): \n",
    "                        self.workflow = self.data_transformer.workflow\n",
    "                    if callback:\n",
    "                        callback()\n",
    "                return\n",
    "            \n",
    "\n",
    "           \n",
    "           \n",
    "            # Start the fetch process\n",
    "            #def extended_callback():\n",
    "            #    if callback:\n",
    "            #        # Add a small delay to ensure UI updates\n",
    "            #        import time\n",
    "            #        time.sleep(0.5)\n",
    "            #        callback()\n",
    "\n",
    "        # Fetch missing proteins with our extended callback\n",
    "        self._fetch_missing_proteins()\n",
    "\n",
    "    def process_protein_combinations(self):\n",
    "        \"\"\"Process protein combinations in pd_results with Unknown handling\"\"\"\n",
    "        if not self.pd_results.empty:\n",
    "            df = self.pd_results.copy()\n",
    "\n",
    "            # Fill NaN values with \"Unknown\"\n",
    "            df['Positions in Proteins'] = df['Positions in Proteins'].fillna('Unknown')\n",
    "            df['Master Protein Accessions'] = df['Master Protein Accessions'].fillna('Unknown')\n",
    "\n",
    "            # Create warning display area\n",
    "            warning_area = widgets.HTML(\n",
    "                layout=widgets.Layout(\n",
    "                    margin='10px 0',\n",
    "                    padding='10px',\n",
    "                    border='1px solid #ffeeba',\n",
    "                    background_color='#fff3cd',\n",
    "                    border_radius='4px'\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Get combinations and track Unknown statistics\n",
    "            combinations = self.get_protein_combinations()\n",
    "\n",
    "            # Update warning area with statistics\n",
    "            unknown_positions = (df['Positions in Proteins'] == 'Unknown').sum()\n",
    "            unknown_master_acc = (df['Master Protein Accessions'] == 'Unknown').sum()\n",
    "\n",
    "            if unknown_positions > 0 or unknown_master_acc > 0:\n",
    "                warning_html = \"<div><b>â¹ï¸ Notice:</b><ul style='margin: 5px 0'>\"\n",
    "                if unknown_positions > 0:\n",
    "                    warning_html += f\"<li>{unknown_positions} rows with missing 'Positions in Proteins' are marked as Unknown</li>\"\n",
    "                if unknown_master_acc > 0:\n",
    "                    warning_html += f\"<li>{unknown_master_acc} rows with missing 'Master Protein Accessions' are marked as Unknown</li>\"\n",
    "                warning_html += \"</ul>These peptides will be preserved in the output.</div>\"\n",
    "                warning_area.value = warning_html\n",
    "            else:\n",
    "                warning_area = widgets.HTML(f\"<br>\")\n",
    "\n",
    "            # Main container with warning area\n",
    "            main_container = widgets.VBox([\n",
    "                warning_area], layout=widgets.Layout(width='100%', padding='5px'))\n",
    "\n",
    "\n",
    "            #def create_help_icon(self, tooltip_text):\n",
    "            #    \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "            #    return f'<div title=\"{tooltip_text}\" style=\"display: inline-block; margin-left: 4px;\">' \\\n",
    "            #           '<i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>' \\\n",
    "            #           '</div>'\n",
    "\n",
    "            table_header = widgets.HTML(\"\"\"\n",
    "                            <div style=\"display: grid; grid-template-columns: 100px 100px 225px 150px 200px; gap: 2px; margin-bottom: 10px; font-weight: bold; align-items: center;\">\n",
    "                                <div>\n",
    "                                    Protein ID\n",
    "                                    <span title=\"Unique identifier for the protein\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Species\n",
    "                                    <span title=\"Source organism of the protein\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Description\n",
    "                                    <span title=\"Full protein name or description\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Decision\n",
    "                                    <span title=\"Available options:\\n\n",
    "            - 'new' - Create a separate row for this protein\\n\n",
    "            - 'remove' - Remove this protein from combination\\n\n",
    "            - 'asis' - Keep as part of current combination\\n\n",
    "            - 'Custom: (protein ID)': ie. Custom: P02666A1\"\n",
    "            style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                                <div>\n",
    "                                    Status\n",
    "                                    <span title=\"Color indicators:\\n\n",
    "            - Grey - Default option (not yet submitted)\\n\n",
    "            - Green - Option has been submitted\" style=\"display: inline-block; margin-left: 4px;\">\n",
    "                                        <i class=\"fa fa-question-circle\" style=\"color: #007bff; font-size: 14px;\"></i>\n",
    "                                    </span>\n",
    "                                </div>\n",
    "                            </div>\n",
    "                            <hr style=\"margin: 0 0 10px 0;\">\n",
    "                        \"\"\")\n",
    "\n",
    "            # Create input area\n",
    "            input_area = widgets.VBox(\n",
    "                #[table_header],\n",
    "                layout=widgets.Layout(\n",
    "                width='100%',\n",
    "                min_height='50px',\n",
    "                margin='10px 0',\n",
    "                max_height='500px',  # Set a maximum height\n",
    "                overflow='auto'      # Enable scrolling\n",
    "            ))\n",
    "\n",
    "            # Add rows for each combination\n",
    "            self.decision_inputs = []\n",
    "            self.status_displays = {}\n",
    "\n",
    "            for combo_idx, combo in enumerate(combinations, 1):\n",
    "                proteins = combo.split('; ')\n",
    "\n",
    "                # Find rows with this combination\n",
    "                combo_rows = []\n",
    "                for _, row in df.iterrows():\n",
    "                    if pd.isna(row['Positions in Proteins']):\n",
    "                        continue\n",
    "                    row_proteins = set(p.split()[0] for p in row['Positions in Proteins'].split('; '))\n",
    "                    if row_proteins == set(proteins):\n",
    "                        combo_rows.append(row)\n",
    "\n",
    "                occurrences = len(combo_rows)\n",
    "\n",
    "                # Add combination header\n",
    "                row_header = widgets.HTML(f\"\"\"\n",
    "                    <div style=\"background-color: #f8f9fa; padding: 2px; margin: 5px 0; border-radius: 5px;\">\n",
    "                        <b>Combination {combo_idx}</b> ({occurrences} occurrences)\n",
    "                    </div>\n",
    "                \"\"\")\n",
    "                combo_box = widgets.VBox(\n",
    "                \n",
    "                layout=widgets.Layout(\n",
    "                width='100%',\n",
    "                min_height='50px',\n",
    "                margin='10px 0',\n",
    "                max_height='500px',  # Set a maximum height\n",
    "                    overflow='auto'      # Enable scrolling\n",
    "                ))\n",
    "                # Process each protein in the combination\n",
    "                for protein in proteins:\n",
    "                    species = \"Unknown\" if protein == 'Unknown' else self.protein_dict.get(protein, {}).get('species',\n",
    "                                                                                                            \"Unknown\")\n",
    "                    name = \"Unknown Protein\" if protein == 'Unknown' else self.protein_dict.get(protein, {}).get('name',\n",
    "                                                                                                                 \"Unknown\")\n",
    "\n",
    "                    # Set default decision based on Master Protein Accessions\n",
    "                    default_decision = 'asis'  # Always keep Unknown proteins as-is\n",
    "                    if protein != 'Unknown' and combo_rows:\n",
    "                        first_row = combo_rows[0]\n",
    "                        if not pd.isna(first_row['Master Protein Accessions']):\n",
    "                            master_proteins = first_row['Master Protein Accessions'].split(';')\n",
    "                            master_proteins = [p.strip() for p in master_proteins]\n",
    "                            default_decision = 'new' if protein in master_proteins else 'remove'\n",
    "\n",
    "                    # Create decision input\n",
    "                    decision_input = widgets.Text(\n",
    "                        layout=widgets.Layout(width='125px', max_width='125px', overflow='hidden'),\n",
    "                        value=default_decision\n",
    "                    )\n",
    "                    self.decision_inputs.append((combo, protein, decision_input))\n",
    "\n",
    "                    # Create status display with initial status\n",
    "                    status_text = {\n",
    "                        'new': \"Will be created as new row\",\n",
    "                        'remove': \"Will be removed\",\n",
    "                        'asis': \"Will keep as is\",\n",
    "                        'Custom: (protein ID)': \"ie. Custom: P02666A1\"\n",
    "                    }\n",
    "                    initial_status = status_text.get(default_decision, '')\n",
    "                    status_display = widgets.HTML(f'<span style=\"color: gray\">{initial_status}</span>')\n",
    "                    self.status_displays[(combo, protein)] = status_display\n",
    "\n",
    "                    # Create the row content\n",
    "                    row_content = widgets.HTML(f\"\"\"\n",
    "                    <div style=\"display: grid; grid-template-columns: 100px 100px 225px; gap: 2px; align-items: center;\">\n",
    "                            <div style=\"white-space: nowrap; overflow: hidden; text-overflow: ellipsis;\">{protein}</div>\n",
    "                            <div style=\"white-space: nowrap; overflow: hidden; text-overflow: ellipsis;\">{species}</div>\n",
    "                            <div style=\"white-space: nowrap; overflow: hidden; text-overflow: ellipsis;\">{name}</div>\n",
    "                        </div>\n",
    "                    \"\"\")\n",
    "\n",
    "                    \n",
    "                    # Create container with all elements\n",
    "                    container = widgets.HBox([\n",
    "                        row_content,\n",
    "                        widgets.HBox([decision_input], layout=widgets.Layout(width='150px', padding='0')),\n",
    "                        widgets.HBox([status_display], layout=widgets.Layout(width='200px', padding='0'))\n",
    "                    ], layout=widgets.Layout(\n",
    "                        margin='2px 0',\n",
    "                        display='flex',\n",
    "                        height='30px',\n",
    "                        min_height='50px',\n",
    "                        align_items='center',\n",
    "                        overflow='hidden',\n",
    "                        width='100%'\n",
    "                    ))\n",
    "                    combo_box.children += (container,)\n",
    "                row_box = widgets.VBox(\n",
    "                    [row_header, combo_box],\n",
    "                    layout=widgets.Layout(\n",
    "                        width='100%',\n",
    "                        padding='0',\n",
    "                        min_height='250px',   # or whatever you want\n",
    "                        max_height='750px',\n",
    "                        overflow='auto'\n",
    "                    )\n",
    "                )\n",
    "                input_area.children += (row_box,)\n",
    "\n",
    "            # Create buttons\n",
    "            button_box = self._create_buttons()\n",
    "\n",
    "            # Add output area\n",
    "            self.protein_output_area = widgets.Output(\n",
    "                layout=widgets.Layout(width='100%', margin='0', max_height='50px', max_width='auto', overflow='hidden')\n",
    "            )\n",
    "            # Add all components\n",
    "\n",
    "            main_container.children += (table_header,input_area, button_box, self.protein_output_area)\n",
    "            #main_container.layout.max_height = '700px'\n",
    "            main_container.layout.width = '825px'\n",
    "            #main_container.layout.overflow = 'auto'\n",
    "            self.pd_results_cleaned = df\n",
    "            #display(main_container)\n",
    "            return df, main_container\n",
    "\n",
    "    def get_protein_combinations(self):\n",
    "        \"\"\"Extract unique protein combinations from the dataset with NaN handling\"\"\"\n",
    "        if self.pd_results is None or self.pd_results.empty:\n",
    "            return []\n",
    "\n",
    "        protein_combinations = set()\n",
    "        nan_warnings = {\n",
    "            'positions': 0,\n",
    "            'master_acc': 0,\n",
    "            'unknown_added': 0\n",
    "        }\n",
    "\n",
    "        # Create a working copy of the dataframe\n",
    "        working_df = self.pd_results.copy()\n",
    "\n",
    "        # Track NaN counts before modification\n",
    "        nan_warnings['positions'] = working_df['Positions in Proteins'].isna().sum()\n",
    "        nan_warnings['master_acc'] = working_df['Master Protein Accessions'].isna().sum()\n",
    "\n",
    "        # Replace NaN values with \"Unknown\" instead of dropping\n",
    "        working_df['Positions in Proteins'] = working_df['Positions in Proteins'].fillna('Unknown')\n",
    "        working_df['Master Protein Accessions'] = working_df['Master Protein Accessions'].fillna('Unknown')\n",
    "\n",
    "        for _, row in working_df.iterrows():\n",
    "            try:\n",
    "                # Handle \"Unknown\" case specially\n",
    "                if row['Positions in Proteins'] == 'Unknown':\n",
    "                    position_proteins = ['Unknown']\n",
    "                else:\n",
    "                    position_proteins = [p.split()[0] for p in row['Positions in Proteins'].split('; ')]\n",
    "\n",
    "                master_acc = row['Master Protein Accessions']\n",
    "\n",
    "                # Check species of proteins in Positions in Proteins\n",
    "                species_set = set()\n",
    "                for protein in position_proteins:\n",
    "                    if protein in self.protein_dict:\n",
    "                        species_set.add(self.protein_dict[protein]['species'])\n",
    "                    elif protein == 'Unknown':\n",
    "                        species_set.add('Unknown')\n",
    "\n",
    "                if (';' in master_acc or\n",
    "                        ';' in row['Positions in Proteins'] or\n",
    "                        len(species_set) > 1 or\n",
    "                        'Unknown' in species_set):  # Include Unknown combinations\n",
    "                    protein_combinations.add('; '.join(sorted(position_proteins)))\n",
    "                    if 'Unknown' in position_proteins:\n",
    "                        nan_warnings['unknown_added'] += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error processing row {_}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Store warning message for display\n",
    "        warning_message = []\n",
    "\n",
    "        if nan_warnings['positions'] > 0:\n",
    "            warning_message.append(\n",
    "                f\"{nan_warnings['positions']} rows with missing 'Positions in Proteins' were marked as Unknown\")\n",
    "        if nan_warnings['master_acc'] > 0:\n",
    "            warning_message.append(\n",
    "                f\"{nan_warnings['master_acc']} rows with missing 'Master Protein Accessions' were marked as Unknown\")\n",
    "        if nan_warnings['unknown_added'] > 0:\n",
    "            warning_message.append(f\"{nan_warnings['unknown_added']} combinations now include Unknown proteins\")\n",
    "\n",
    "        # if warning_message:\n",
    "        #   print( Debug: \"Warning: \" + \"; \".join(warning_message))\n",
    "\n",
    "        self.multi_position_combinations = list(protein_combinations)\n",
    "        return self.multi_position_combinations\n",
    "    \n",
    "    def _on_submit(self, button, df):\n",
    "        \"\"\"Handle submit button click with enhanced position handling\"\"\"\n",
    "        self.submit_button.disabled = True\n",
    "        self.reset_button.disabled = True\n",
    "        self.progress.value = 0\n",
    "        \n",
    "        with self.protein_output_area:\n",
    "            try:\n",
    "                self.protein_output_area.clear_output()\n",
    "                decisions_by_combo = {}\n",
    "                rows_to_remove = set()\n",
    "                new_rows = []\n",
    "                total_inputs = len(self.decision_inputs)\n",
    "                \n",
    "                # First pass: collect all decisions\n",
    "                for i, (combo, protein, input_widget) in enumerate(self.decision_inputs):\n",
    "                    try:\n",
    "                        decision = input_widget.value.strip()\n",
    "                        if decision:\n",
    "                            status_display = self.status_displays[(combo, protein)]\n",
    "                            status_display.value = f'<span style=\"color: green\">Decision: {decision}</span>'\n",
    "                            \n",
    "                            if combo not in decisions_by_combo:\n",
    "                                decisions_by_combo[combo] = {}\n",
    "                            decisions_by_combo[combo][protein] = decision\n",
    "                    except Exception as e:\n",
    "                        display(HTML(f\"<b style='color:red;'>Error processing decision for {protein}: {str(e)}</b>\"))\n",
    "                        continue\n",
    "\n",
    "                    self.progress.value = ((i + 1) / total_inputs * 25)\n",
    "                \n",
    "                # Second pass: validate decisions\n",
    "                validation_errors = []\n",
    "                \n",
    "                for combo, protein_decisions in decisions_by_combo.items():\n",
    "                    # Check if decisions are valid for this combination\n",
    "                    has_asis = any(decision.upper() == 'ASIS' for decision in protein_decisions.values())\n",
    "                    has_custom = any(decision.upper().startswith('CUSTOM:') or decision.startswith('Custom:') \n",
    "                                    for decision in protein_decisions.values())\n",
    "                    has_new = any(decision.upper() == 'NEW' for decision in protein_decisions.values())\n",
    "                    has_remove = any(decision.upper() == 'REMOVE' for decision in protein_decisions.values())\n",
    "                    \n",
    "                    # Validation rules\n",
    "                    if has_asis and (has_custom or has_new or has_remove):\n",
    "                        validation_errors.append(f\"Combination '{combo}': ASIS cannot be used with other decision types\")\n",
    "                    \n",
    "                    #if has_new and (has_custom or has_asis):\n",
    "                    #    validation_errors.append(f\"Combination '{combo}': NEW cannot be used with CUSTOM or ASIS\")\n",
    "                    \n",
    "                    # Validate individual decision formats\n",
    "                    for protein, decision in protein_decisions.items():\n",
    "                        decision_upper = decision.upper()\n",
    "                        if (decision_upper not in ['NEW', 'REMOVE', 'ASIS'] and \n",
    "                            not decision_upper.startswith('CUSTOM:') and \n",
    "                            not decision.startswith('Custom:')):\n",
    "                            validation_errors.append(f\"Protein '{protein}': Invalid decision format '{decision}'\")\n",
    "                        \n",
    "                        if (decision_upper.startswith('CUSTOM:') or decision.startswith('Custom:')) and len(decision.split(':', 1)[1].strip()) == 0:\n",
    "                            validation_errors.append(f\"Protein '{protein}': CUSTOM decision requires a protein ID after the colon\")\n",
    "                \n",
    "                # If validation errors, stop processing\n",
    "                if validation_errors:\n",
    "                    error_message = \"Cannot process due to the following errors:<br>\"\n",
    "                    for error in validation_errors:\n",
    "                        error_message += f\"â¢ {error}<br>\"\n",
    "                    error_message += \"<br>Valid combinations:<br>\"\n",
    "                    error_message += \"â¢ All proteins can be ASIS (no changes)<br>\"\n",
    "                    error_message += \"â¢ CUSTOM, NEW and REMOVE can be used together<br>\"\n",
    "                    error_message += \"â¢ ASIS cannot be used with other decision types (CUSTOM, NEW or REMOVE)<br>\"\n",
    "                    \n",
    "                    display(HTML(f\"<b style='color:red;'>{error_message}</b>\"))\n",
    "                    self.progress.value = 0\n",
    "                    self.submit_button.disabled = False\n",
    "                    self.reset_button.disabled = False\n",
    "                    return df\n",
    "                \n",
    "                # Third pass: process the dataframe\n",
    "                if decisions_by_combo:\n",
    "                    processed_df = df.copy()\n",
    "                    processed_count = 0\n",
    "                    total_combinations = len(decisions_by_combo)\n",
    "\n",
    "                    for combo, protein_decisions in decisions_by_combo.items():\n",
    "                        try:\n",
    "                            # Extract protein IDs for pattern matching\n",
    "                            proteins = []\n",
    "                            for part in combo.split('; '):\n",
    "                                if not part.startswith('['):\n",
    "                                    protein_id = part.split()[0]\n",
    "                                    proteins.append(protein_id)\n",
    "                            \n",
    "                            # Create regex pattern\n",
    "                            pattern_parts = []\n",
    "                            for protein in proteins:\n",
    "                                escaped_protein = re.escape(protein)\n",
    "                                pattern_parts.append(f'(?=.*{escaped_protein})')\n",
    "                            pattern = ''.join(pattern_parts)\n",
    "\n",
    "                            try:\n",
    "                                # Find matching rows\n",
    "                                valid_rows = processed_df['Positions in Proteins'].notna()\n",
    "                                mask = processed_df['Positions in Proteins'].fillna('').str.contains(pattern, regex=True)\n",
    "                                mask = valid_rows & mask\n",
    "                                matched_indices = processed_df[mask].index\n",
    "\n",
    "                                for idx in matched_indices:\n",
    "                                    row = processed_df.loc[idx]\n",
    "                                    positions = row['Positions in Proteins'].split('; ')\n",
    "                                    master_accs = row['Master Protein Accessions'].split('; ') if '; ' in row['Master Protein Accessions'] else [row['Master Protein Accessions']]\n",
    "                                    \n",
    "                                    # Extract protein IDs from positions\n",
    "                                    current_proteins = []\n",
    "                                    for pos in positions:\n",
    "                                        parts = pos.split()\n",
    "                                        if parts and not parts[0].startswith('['):\n",
    "                                            current_proteins.append(parts[0])\n",
    "                                    \n",
    "                                    if set(current_proteins) == set(proteins):\n",
    "                                        # Check if all decisions are ASIS\n",
    "                                        all_asis = all(decision.upper() == 'ASIS' for decision in protein_decisions.values())\n",
    "                                        if all_asis:\n",
    "                                            continue\n",
    "                                        \n",
    "                                        # Process decisions\n",
    "                                        proteins_to_remove = []\n",
    "                                        custom_changes = {}\n",
    "                                        new_proteins = []\n",
    "                                        \n",
    "                                        for protein, decision in protein_decisions.items():\n",
    "                                            decision_upper = decision.upper()\n",
    "                                            \n",
    "                                            if decision_upper == 'NEW':\n",
    "                                                new_proteins.append(protein)\n",
    "                                                proteins_to_remove.append(protein)\n",
    "                                                \n",
    "                                                # Create new row for this protein\n",
    "                                                matching_position = next((p for p in positions if p.startswith(protein)), None)\n",
    "                                                if matching_position:\n",
    "                                                    new_row = row.copy()\n",
    "                                                    new_row['Positions in Proteins'] = matching_position\n",
    "                                                    new_row['Master Protein Accessions'] = protein\n",
    "                                                    new_rows.append(new_row)\n",
    "                                                    \n",
    "                                            elif decision_upper == 'REMOVE':\n",
    "                                                proteins_to_remove.append(protein)\n",
    "                                                \n",
    "                                            elif decision_upper.startswith('CUSTOM:') or decision.startswith('Custom:'):\n",
    "                                                parts = decision.split(':', 1)\n",
    "                                                if len(parts) > 1:\n",
    "                                                    new_protein_id = parts[1].strip()\n",
    "                                                    custom_changes[protein] = new_protein_id\n",
    "                                        \n",
    "                                        # First apply custom changes\n",
    "                                        for protein, new_protein_id in custom_changes.items():\n",
    "                                            for i, pos in enumerate(positions):\n",
    "                                                if pos.startswith(protein):\n",
    "                                                    pos_parts = pos.split(' ', 1)\n",
    "                                                    if len(pos_parts) > 1:\n",
    "                                                        pos_range = pos_parts[1]\n",
    "                                                        new_position = f\"{new_protein_id} {pos_range}\"\n",
    "                                                        positions[i] = new_position\n",
    "                                            \n",
    "                                            for i, acc in enumerate(master_accs):\n",
    "                                                if acc == protein:\n",
    "                                                    master_accs[i] = new_protein_id\n",
    "                                        \n",
    "                                        # Then remove proteins\n",
    "                                        if proteins_to_remove:\n",
    "                                            positions = [pos for pos in positions if not any(pos.startswith(p) for p in proteins_to_remove)]\n",
    "                                            master_accs = [acc for acc in master_accs if acc not in proteins_to_remove]\n",
    "                                        \n",
    "                                        # Update the original row if there are positions left\n",
    "                                        if positions:\n",
    "                                            processed_df.at[idx, 'Positions in Proteins'] = '; '.join(positions)\n",
    "                                            processed_df.at[idx, 'Master Protein Accessions'] = '; '.join(master_accs) if master_accs else \"Unknown\"\n",
    "                                        else:\n",
    "                                            rows_to_remove.add(idx)\n",
    "\n",
    "                            except Exception as regex_error:\n",
    "                                display(HTML(f\"<b style='color:orange;'>Warning: Error in pattern matching: {str(regex_error)}</b>\"))\n",
    "                                continue\n",
    "\n",
    "                        except Exception as combo_error:\n",
    "                            display(HTML(f\"<b style='color:orange;'>Warning: Error processing combination {combo}: {str(combo_error)}</b>\"))\n",
    "                            continue\n",
    "\n",
    "                        processed_count += 1\n",
    "                        progress = 30 + (processed_count / total_combinations * 70)\n",
    "                        self.progress.value = progress\n",
    "\n",
    "                    # Final processing\n",
    "                    if rows_to_remove:\n",
    "                        processed_df = processed_df.drop(index=list(rows_to_remove))\n",
    "                    if new_rows:\n",
    "                        processed_df = pd.concat([processed_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "                    self.pd_results_cleaned = processed_df\n",
    "                    self.progress.value = 100\n",
    "                    display(HTML(\"<b style='color:green;'>Processing complete.</b>\"))\n",
    "                    \n",
    "                    # Notify the workflow that processing completed successfully\n",
    "                    if hasattr(self, 'workflow') and hasattr(self.workflow, '_on_submit_complete'):\n",
    "                        self.workflow._on_submit_complete(True)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red;'>Error in submit handler: {str(e)}</b>\"))\n",
    "                self.progress.value = 0\n",
    "                \n",
    "                # Notify the workflow that processing failed\n",
    "                if hasattr(self, 'workflow') and hasattr(self.workflow, '_on_submit_complete'):\n",
    "                    self.workflow._on_submit_complete(False)\n",
    "\n",
    "            finally:\n",
    "                self.submit_button.disabled = False\n",
    "                self.reset_button.disabled = False\n",
    "\n",
    "        return self.pd_results_cleaned\n",
    "\n",
    "    def create_help_icon(self, tooltip_text):\n",
    "        \"\"\"Create a help icon widget with tooltip\"\"\"\n",
    "        return widgets.HTML(\n",
    "            f'<div title=\"{tooltip_text}\" style=\"display: inline-block;\">'\n",
    "            '<i class=\"fa fa-question-circle\" style=\"color: #007bff;\"></i>'\n",
    "            '</div>'\n",
    "        )\n",
    "\n",
    "    def _create_buttons(self):\n",
    "        \"\"\"Create submit and reset buttons\"\"\"\n",
    "        self.submit_button = widgets.Button(\n",
    "            description=\"Submit\",\n",
    "            button_style='success',\n",
    "            disabled=False\n",
    "        )\n",
    "        self.reset_button = widgets.Button(\n",
    "            description=\"Reset\",\n",
    "            button_style='warning',\n",
    "            disabled=False\n",
    "        )\n",
    "        self.progress = widgets.FloatProgress(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=100,\n",
    "            description='Processing:',\n",
    "            bar_style='info',\n",
    "            style={'bar_color': '#0080ff'},\n",
    "            orientation='horizontal',\n",
    "            layout=widgets.Layout(width='50%')\n",
    "        )\n",
    "\n",
    "        button_box = widgets.VBox([\n",
    "            widgets.HBox([self.submit_button, self.reset_button]),\n",
    "            self.progress\n",
    "        ], layout=widgets.Layout(width='100%', margin='5px 0', max_height='150px', max_width='1000px', overflow='hidden'))\n",
    "\n",
    "        self.reset_button.on_click(self._on_reset_button_clicked)\n",
    "        self.submit_button.on_click(lambda b: self._on_submit(b, self.pd_results.copy()))\n",
    "\n",
    "        return button_box\n",
    "    \n",
    "    def _on_reset_button_clicked(self, b):\n",
    "        \"\"\"Handle reset button click by resetting options to default values\"\"\"\n",
    "        # Disable buttons during reset\n",
    "        self.submit_button.disabled = True\n",
    "        self.reset_button.disabled = True\n",
    "\n",
    "        # Clear output area\n",
    "        with self.protein_output_area:\n",
    "            self.protein_output_area.clear_output()\n",
    "            display(HTML(\"<b style='color:blue;'>Resetting options to defaults...</b>\"))\n",
    "\n",
    "        # Reset progress bar\n",
    "        self.progress.value = 0\n",
    "\n",
    "        try:\n",
    "            # Reset each input field to its default value based on Master Protein Accessions\n",
    "            df = self.pd_results.copy()\n",
    "            processed = 0\n",
    "            total_inputs = len(self.decision_inputs)\n",
    "\n",
    "            for combo, protein, input_field in self.decision_inputs:\n",
    "                # Find rows with this combination\n",
    "                proteins = combo.split('; ')\n",
    "                combo_rows = []\n",
    "                for _, row in df.iterrows():\n",
    "                    if pd.isna(row['Positions in Proteins']):\n",
    "                        continue\n",
    "                    row_proteins = set(p.split()[0] for p in row['Positions in Proteins'].split('; '))\n",
    "                    if row_proteins == set(proteins):\n",
    "                        combo_rows.append(row)\n",
    "\n",
    "                # Determine default decision\n",
    "                default_decision = 'asis'\n",
    "                if combo_rows:\n",
    "                    first_row = combo_rows[0]\n",
    "                    if not pd.isna(first_row['Master Protein Accessions']):\n",
    "                        master_proteins = first_row['Master Protein Accessions'].split(';')\n",
    "                        master_proteins = [p.strip() for p in master_proteins]\n",
    "                        default_decision = 'new' if protein in master_proteins else 'remove'\n",
    "\n",
    "                # Set input field value\n",
    "                input_field.value = default_decision\n",
    "\n",
    "                # Update status display\n",
    "                status_display = self.status_displays[(combo, protein)]\n",
    "                status_text = {\n",
    "                    'new': \"Will be created as new row\",\n",
    "                    'remove': \"Will be removed\",\n",
    "                    'asis': \"Will keep as is\",\n",
    "                    'Custom: (protein ID)': \"ie. Custom: P02666A1\"\n",
    "                }\n",
    "                status_display.value = f'<span style=\"color: gray\">{status_text[default_decision]}</span>'\n",
    "\n",
    "                # Update progress\n",
    "                processed += 1\n",
    "                self.progress.value = (processed / total_inputs) * 100\n",
    "\n",
    "            # Reset internal state\n",
    "            self.user_decisions = {}\n",
    "            self.pd_results_cleaned = self.pd_results.copy()\n",
    "\n",
    "            with self.protein_output_area:\n",
    "                self.protein_output_area.clear_output()\n",
    "                display(HTML(\"<b style='color:green;'>Reset complete. All options set to defaults.</b>\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            with self.protein_output_area:\n",
    "                self.protein_output_area.clear_output()\n",
    "                display(HTML(f\"<b style='color:red;'>Error during reset: {str(e)}</b>\"))\n",
    "\n",
    "        finally:\n",
    "            # Re-enable buttons\n",
    "            self.submit_button.disabled = False\n",
    "            self.reset_button.disabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b182afc-9770-4334-b931-7306109d5a35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ProcessingWorkflow:\n",
    "    def __init__(self):\n",
    "        self.data_transformer = DataTransformation()\n",
    "        self.protein_handler = ProteinCombinationHandler(self.data_transformer, self)\n",
    "        self.group_processor = GroupProcessing()\n",
    "        \n",
    "        self._initialize_instructions()\n",
    "        \n",
    "        # Set up observers\n",
    "        self.data_transformer.observe(self._handle_data_change, names=['pd_results'])\n",
    "        self.data_transformer.observe(self._handle_fasta_change, names=['protein_dict'])\n",
    "        \n",
    "        # Add property to track protein processing completion\n",
    "        self.protein_processing_complete = False\n",
    "        \n",
    "        # Connect the protein handler's widget to enable group processing when complete\n",
    "        self.protein_handler.protein_mapping_widget.observe(self._on_protein_mapping_change, names='value')\n",
    "\n",
    "    def _handle_data_change(self, change):\n",
    "        \"\"\"Handle changes in proteomics data\"\"\"\n",
    "        if change.name == 'pd_results':\n",
    "            if change.new is not None:\n",
    "                self.protein_handler.protein_mapping_widget.disabled = False\n",
    "                #display(HTML(\"<h3>Multiple Protein Mappings</h3>\"))\n",
    "                self.protein_handler.pd_results = change.new\n",
    "                #self.protein_handler.process_protein_mapping()\n",
    "\n",
    "                #self.protein_handler.process_protein_mapping()\n",
    "            self.group_processor.update_data(change.new)\n",
    "    \n",
    "    def _on_protein_mapping_change(self, change):\n",
    "        \"\"\"Handle changes to protein mapping widget selection\"\"\"\n",
    "        if change['name'] == 'value':\n",
    "            # If user selects to skip protein mapping, enable group processing immediately\n",
    "            if change['new'] is False:  # \"No (skip)\" option\n",
    "                self.protein_processing_complete = True\n",
    "                self.group_processor.enable_widgets(True)\n",
    "                # Update the message to indicate group processing is now available\n",
    "                with self.steptwo_status_output:\n",
    "                    clear_output(wait=True)\n",
    "                    display(HTML(self.steptwo_output_html_message))\n",
    "\n",
    "    def _handle_fasta_change(self, change):\n",
    "        \"\"\"Handle changes in FASTA data\"\"\"\n",
    "        if change.new != change.old:\n",
    "            #self.protein_handler.process_protein_mapping()\n",
    "            pass\n",
    "            #with protein_mapping_output:\n",
    "            #    protein_mapping_output.clear_output()\n",
    "            #    display(HTML(\"<h3>Multiple Protein Mappings</h3>\"))\n",
    "            #    print(f\"Using updated protein dictionary with {len(self.data_transformer.protein_dict)} proteins\")\n",
    "            #    #if self.protein_handler.pd_results is not None:\n",
    "            #    #    self.protein_handler.process_protein_mapping()\n",
    "    \n",
    "    def _initialize_instructions(self):\n",
    "        def stepone_instructions():\n",
    "            self.stepone_output_html_message = \"\"\"\n",
    "            <div style='padding: 10px; background-color: #f8f9fa; border-left: 5px solid #007bff; margin: 10px 0;'>\n",
    "                <h3>Step 1: Upload Data</h3>\n",
    "                <ul style='list-style-type: none;'>\n",
    "                    <li> <b>Required:</b> Upload a peptidomic file exported from your software of choice</li>\n",
    "                    <li> Optional: Upload Functional Data or Query the MBPDB for functional data</li>\n",
    "                    <li> Optional: Upload a FASTA file or by defulat our internal protein database or UniProt will be querried for protein information</li>\n",
    "                </ul>\n",
    "                <p>Start by uploading your peptidomic data file. This file should contain peptide sequences, \n",
    "                abundance values, and protein mapping information.</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            self.stepone_status_output = widgets.Output(\n",
    "                layout=widgets.Layout(\n",
    "                    max_width='1000px',\n",
    "                    width='100%'\n",
    "                )\n",
    "            )\n",
    "            with self.stepone_status_output:\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(self.stepone_output_html_message))\n",
    "                \n",
    "        def steptwo_instructions():\n",
    "            self.steptwo_output_html_message = \"\"\"\n",
    "            <div style='padding: 10px; background-color: #f8f9fa; border-left: 5px solid #007bff; margin: 10px 0;'>\n",
    "                <h3>Step 2 (Optional): Organize Peptides with Multiple Protein Mappings</h3>\n",
    "                <p>In peptidomic datasets, peptides may map to multiple proteins due to:</p>\n",
    "                <ul style='list-style-type: circle;'>\n",
    "                    <li>Multiple proteins sharing identical sequences</li>\n",
    "                    <li>Proteins from different species with similar sequences</li>\n",
    "                    <li>Ambiguous mapping from incomplete sequence information</li>\n",
    "                </ul>\n",
    "                <p>Please select whether you want to organize these mappings manually or use the default approach:</p>\n",
    "                <ul style='list-style-type: none;'>\n",
    "                    <li><b>Yes</b> - Process manually: Review each protein mapping combination and decide how to handle it</li>\n",
    "                    <li><b>No (skip)</b> - Default mapping: Returns the dataset unchanged keeping the current protein mapping.</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            self.steptwo_status_output = widgets.Output(\n",
    "                layout=widgets.Layout(\n",
    "                    max_width='1000px',\n",
    "                    width='100%'\n",
    "                )\n",
    "            )\n",
    "            with self.steptwo_status_output:\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(self.steptwo_output_html_message))\n",
    "\n",
    "        def stepthree_instructions():\n",
    "            self.stepthree_output_html_message = \"\"\"\n",
    "            <div style='padding: 10px; background-color: #f8f9fa; border-left: 5px solid #007bff; margin: 10px 0;'>\n",
    "                <h3>Step 3 (Optional): Assign Study Variables for Data Grouping</h3>\n",
    "                <p>This step allows you to define experimental variables by grouping absorbance data columns. Select one of these three options:</p>\n",
    "                \n",
    "                <details>\n",
    "                    <summary style='cursor: pointer; padding: 8px; background-color: #f8f9fa; border-radius: 5px; margin: 5px 0;'>\n",
    "                        <b>Option 1: Upload Existing Group Dictionary</b>\n",
    "                    </summary>\n",
    "                    <div style='padding: 8px; margin-left: 20px;'>\n",
    "                        <ul>\n",
    "                            <li>Import a predefined JSON file from previous analyses</li>\n",
    "                            <li>Useful for reusing experimental designs or standardized groupings</li>\n",
    "                        </ul>\n",
    "                    </div>\n",
    "                </details>\n",
    "                \n",
    "                <details>\n",
    "                    <summary style='cursor: pointer; padding: 8px; background-color: #f8f9fa; border-radius: 5px; margin: 5px 0;'>\n",
    "                        <b>Option 2: Assign Selected Columns to a Group</b>\n",
    "                    </summary>\n",
    "                    <div style='padding: 8px; margin-left: 20px;'>\n",
    "                        <ul>\n",
    "                            <li>Select absorbance columns and assign them a group name</li>\n",
    "                            <li>Use the search function to find columns containing specific text</li>\n",
    "                            <li>Click \"Add Group\" to create multiple groups as needed</li>\n",
    "                        </ul>\n",
    "                    </div>\n",
    "                </details>\n",
    "                \n",
    "                <details>\n",
    "                    <summary style='cursor: pointer; padding: 8px; background-color: #f8f9fa; border-radius: 5px; margin: 5px 0;'>\n",
    "                        <b>Option 3: Use Selected Columns Without Grouping</b>\n",
    "                    </summary>\n",
    "                    <div style='padding: 8px; margin-left: 20px;'>\n",
    "                        <ul>\n",
    "                            <li>Select absorbance columns and use them directly without assigning a common group name</li>\n",
    "                            <li>Each column will be treated as its own separate group</li>\n",
    "                            <li>Use \"No Groups\" button after selecting the columns</li>\n",
    "                        </ul>\n",
    "                    </div>\n",
    "                </details>\n",
    "                \n",
    "                <details>\n",
    "                    <summary style='cursor: pointer; padding: 8px; background-color: #f8f9fa; border-radius: 5px; margin: 5px 0;'>\n",
    "                        <b>How Grouping Works</b>\n",
    "                    </summary>\n",
    "                    <div style='padding: 8px; margin-left: 20px;'>\n",
    "                        <ul>\n",
    "                            <li><b>Purpose:</b> When you create a group, the system will calculate an average absorbance value across all selected columns.</li>\n",
    "                            <li><b>Example:</b> If you group 3 replicates named \"Sample1_Rep1\", \"Sample1_Rep2\", and \"Sample1_Rep3\" as \"Sample1\", you'll get a new \"Avg_Sample1\" column in your processed data.</li>\n",
    "                            <li><b>Multiple Groups:</b> The same column can be assigned to multiple groups for different analyses (e.g., a column could belong to both \"Treatment\" and \"Day1\" groups).</li>\n",
    "                        </ul>\n",
    "                    </div>\n",
    "                </details>\n",
    "                \n",
    "                <p style='margin-top: 10px;'>After defining your groups, proceed to Step 4 to generate the processed dataset with averaged values.</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            self.stepthree_status_output = widgets.Output(\n",
    "                layout=widgets.Layout(\n",
    "                    max_width='1000px',\n",
    "                    width='100%'\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            with self.stepthree_status_output:\n",
    "                display(HTML(self.stepthree_output_html_message))\n",
    "                \n",
    "        stepone_instructions()\n",
    "        steptwo_instructions()\n",
    "        stepthree_instructions()       \n",
    "\n",
    "\n",
    "        # Set up observers for the file uploaders and UniProt search checkbox\n",
    "        self.data_transformer.pd_uploader.observe(self._update_step1_status, names='value')\n",
    "        self.data_transformer.mbpdb_uploader.observe(self._update_step1_status, names='value')\n",
    "        self.data_transformer.fasta_uploader.observe(self._update_step1_status, names='value')\n",
    "        self.data_transformer.search_button.observe(self._update_step1_status, names='value')\n",
    "        self.data_transformer.mbpdb_results_from_search_placeholder.observe(self._update_step1_status, names='value')\n",
    "        self.data_transformer.fasta_uploader_placeholder.observe(self._update_step1_status, names='value')\n",
    "        #self.data_transformer.uniprot_search.observe(self._update_step1_status, names='value')\n",
    "        self.data_transformer.pd_uploader.observe(self._update_step2_status_skip, names='value')\n",
    "        self.protein_handler.protein_mapping_widget.observe(self._update_step2_status, names='value')\n",
    "        self.group_processor.notification_widget.observe(self._update_step3_message, names='value')\n",
    "\n",
    "    def _update_step1_status(self, change):\n",
    "        \"\"\"Update Step 1 status message based on uploaded data\"\"\"\n",
    "        # Check current status of uploads\n",
    "        has_peptide_data = hasattr(self.data_transformer, 'pd_results') and not self.data_transformer.pd_results.empty\n",
    "        has_mbpdb_data = hasattr(self.data_transformer, 'mbpdb_results') and self.data_transformer.mbpdb_results is not None and not self.data_transformer.mbpdb_results.empty\n",
    "        has_protein_data = hasattr(self.data_transformer, 'protein_dict') and len(self.data_transformer.protein_dict) > 0\n",
    "        has_fasta_data = self.data_transformer.fasta_uploader_placeholder.value\n",
    "        has_uniprot_enabled = hasattr(self.data_transformer, 'uniprot_search') and self.data_transformer.uniprot_search.value\n",
    "        \n",
    "        if has_peptide_data:\n",
    "            # Get row counts for informative message\n",
    "            peptide_count = len(self.data_transformer.pd_results)\n",
    "            protein_count = len(self.data_transformer.protein_dict) if has_protein_data else 0\n",
    "            mbpdb_count = len(self.data_transformer.mbpdb_results) if has_mbpdb_data else 0\n",
    "            fasta_count = len(self.data_transformer.protein_dict) if has_fasta_data else 0\n",
    "            \n",
    "            if has_mbpdb_data:\n",
    "                filename = f'<b>{self.data_transformer.mbpdb_filename}</b>' if self.data_transformer.mbpdb_results_from_search_placeholder.value == False else f'<b>MBPDB Successfully Queried</b>'\n",
    "            else:\n",
    "                filename = ''\n",
    "           \n",
    "            if has_protein_data:\n",
    "                protein_fimename = f'<b>{self.data_transformer.fasta_filename}</b>' if has_fasta_data else ''\n",
    "\n",
    "            # Create updated message showing success with checkmarks\n",
    "            self.stepone_output_html_message = f\"\"\"\n",
    "            <div style='padding: 10px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;'>\n",
    "                <h3>Step 1: Upload Data</h3>\n",
    "                <ul style='list-style-type: none;'>\n",
    "                    <li>â <b>{self.data_transformer.pd_filename}</b> Peptide data loaded with {peptide_count} rows of data</li>\n",
    "                    <li>{('â ' if has_mbpdb_data else 'â¤')} {str(filename) + ' returning ' + str(mbpdb_count) + ' rows of data' if has_mbpdb_data else 'No Functional data (optional)'}</li>\n",
    "                    <li>{('â ' if has_protein_data or has_uniprot_enabled or has_fasta_data else 'â¤')} Optional: {\n",
    "                        'Protein data loaded (' + str(protein_count) + ' proteins loaded from <b>Internal Protein Database</b>)' if has_protein_data and not has_fasta_data\n",
    "                        else 'Protein data loaded (' + str(fasta_count) + ' proteins loaded from ' + str(protein_fimename) + ')' if has_fasta_data\n",
    "                        else 'UniProt search enabled' if has_uniprot_enabled \n",
    "                        else 'No protein data (optional)'\n",
    "                    }</li>\n",
    "                </ul>\n",
    "                <p style='color: green;'><b>â All required data is loaded. You can proceed to Step 2.</b></p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # Still waiting for peptide data upload\n",
    "            self.stepone_output_html_message = \"\"\"\n",
    "            <div style='padding: 10px; background-color: #fff3e0; border-left: 5px solid #ff9800; margin: 10px 0;'>\n",
    "                <h3>Step 1: Upload Data</h3>\n",
    "                <ul style='list-style-type: none;'>\n",
    "                    <li>â¤ <b>Required:</b> Upload a peptidomic data file exported from your software of choice</li>\n",
    "                    <li>â¤ Optional: Upload Functional Data or Query the MBPDB for functional data</li>\n",
    "                    <li>â¤ Optional: Upload a FASTA file or enable UniProt search for protein information</li>\n",
    "                </ul>\n",
    "                <p style='color: #ff9800;'><b>â ï¸ Please upload your peptide data file to continue.</b></p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Update the status display\n",
    "        with self.stepone_status_output:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(self.stepone_output_html_message))\n",
    "    \n",
    "    def _update_step2_status_skip(self, change):\n",
    "        \"\"\"Update Step 2 status message based on protein mapping choice\"\"\"\n",
    "        combinations = self.protein_handler.get_protein_combinations()\n",
    "        if len(combinations) == 0:\n",
    "            # User selected to use default\n",
    "            self.steptwo_output_html_message = \"\"\"\n",
    "            <div style='padding: 10px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;'>\n",
    "                <h3>Step 2 (Optional): Organize Peptides with Multiple Protein Mappings</h3>\n",
    "                <p style='color: green; margin-top: 10px;'><b>â Peptides are not mapped to multiple proteins. No changes will be made to the protein mapping. You can proceed to Step 3.</b></p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            self.protein_handler.protein_mapping_widget.value = False\n",
    "            self.protein_handler.protein_mapping_widget.disabled = True\n",
    "\n",
    "        # Update the display\n",
    "        with self.steptwo_status_output:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(self.steptwo_output_html_message))\n",
    "\n",
    "    def _update_step2_status(self, change):\n",
    "        \"\"\"Update Step 2 status message based on protein mapping choice\"\"\"\n",
    "        combinations = self.protein_handler.get_protein_combinations()\n",
    "        if len(combinations) == 0:\n",
    "            return\n",
    "        else:\n",
    "            if change['name'] == 'value' and change['new'] is not None:\n",
    "                process_manually = change['new']  # True or False\n",
    "                \n",
    "                if process_manually:\n",
    "                    # User selected to process manually\n",
    "                    example_1 = \"\"\"\n",
    "                    <p><b>Example 1:</b> Peptide mapped to Beta-Casein varriant P02666A1, P02666A2</p>\n",
    "                    <ul>\n",
    "                        <li><b>Example 1.1:</b> Rename P02666A1/P02666A2 to P02666</li>\n",
    "                        <ul>\n",
    "                            <li>Choose <code>Custom: P02666</code> for both P02666A1 to rename one of the proteins in the combination to P02666</li>\n",
    "                            <li>Choose <code>remove</code> for P02666A2 to remove the protein from the combination leaving only P02666</li>\n",
    "                        </ul>\n",
    "                        <li><b>Example 1.2:</b> separate P02666A1/P02666A2 into new respective rows P02666A1 and P02666A2</li>\n",
    "                        <ul>\n",
    "                            <li>Choose <code>new</code> for both P02666A1 and P02666A2 to separate and create two new rows</li>\n",
    "                        </ul>\n",
    "                        <li><b>Example 1.3:</b> Remove P02666A2 from the combination leaving only P02666A1</li>\n",
    "                        <ul>\n",
    "                            <li>Choose <code>remove</code> for P02666A2 to remove the protein from the combination</li>\n",
    "                            <li>Choose <code>new</code> for P02666A1 to keep the combination returning only P02666A1 from the combination</li>\n",
    "                        </ul>\n",
    "                        <li><b>Example 1.4:</b> Return combination default by assign <code>asis</code> to the combination P02666A1/P02666A2</li>\n",
    "                        <ul>\n",
    "                            <li>Choose <code>asis</code> for both P02666A1 and P02666A2 to keep the combination as is, returning the current combination unchanged</li>         \n",
    "                        </ul>\n",
    "                    </ul>\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    example_2 = \"\"\"\n",
    "                    <p><b>Example 2:</b> Peptide with mapping to Bovine (P02666) and Human (P05814) Beta-Casein</p>\n",
    "                    <ul>\n",
    "                        <li><b>Example 2.1:</b> Remove Human Beta-casein as Bovine data is the only interests of the study</li>\n",
    "                        <ul>\n",
    "                            <li>Choose <code>new</code> for P02666 keeping the protein in the combination</li>\n",
    "                            <li>Choose <code>remove</code> for P05814 to remove the protein from the combination leaving only P02666</li>\n",
    "                        </ul>\n",
    "                        <li><b>Example 2.2:</b> separate P02666/P05814 into new respective rows P02666 and P05814</li>\n",
    "                        <ul>\n",
    "                            <li>Choose <code>new</code> for both P02666 and P05814 to separate and create two new rows</li>\n",
    "                        </ul>\n",
    "                        <li><b>Example 2.3:</b> Return combination default by assign <code>asis</code> to the combination P02666/P05814</li>\n",
    "                        <ul>\n",
    "                            <li>Choose <code>asis</code> for both P02666 and P05814 to keep the combination as is, returning the current combination unchanged</li>         \n",
    "                        </ul>\n",
    "                    </ul>\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    example_3 = \"\"\"\n",
    "                    <p><b>Example 3:</b> Peptide with mapping to protein of interest (P02663, \"Alpha-S2-casein\") and unimportant minor protein (A5D980, \"Protein tyrosine phosphatase\")</p>\n",
    "                    <ul>\n",
    "                        <li><b>Example 3.1:</b> Remove A5D980 as P02663 data is of interests in the study</li>\n",
    "                        <ul>\n",
    "                            <li>Choose <code>new</code> for P02663 keeping the protein in the combination</li>\n",
    "                            <li>Choose <code>remove</code> for A5D980 to remove the protein from the combination leaving only P02663</li>\n",
    "                        </ul>\n",
    "                    </ul>\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    example_4 = \"\"\"\n",
    "                    <p><b>Example 4:</b> Peptide with mapping to primary protein (P02754,\"Beta-lactoglobulin\") and primary protein frament or the same protein with different name (Q9BDG3, \"Beta lactoglobulin D (Fragment)\") </p>\n",
    "                    <ul>\n",
    "                        <li><b>Example 4.1:</b> Remove Q9BDG3 so all Beta-lactoglobulin peptides will be mapped to P02754 </li>\n",
    "                        <ul>\n",
    "                            <li>Choose <code>new</code> for P02754 keeping the protein in the combination</li>\n",
    "                            <li>Choose <code>remove</code> for Q9BDG3 to remove the protein from the combination leaving only P02754</li>\n",
    "                        </ul>\n",
    "                    </ul>\n",
    "                    \"\"\"\n",
    "\n",
    "                    self.steptwo_output_html_message = f\"\"\"\n",
    "                    <div style='padding: 10px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;'>\n",
    "                        <h3>Step 2 (Optional): Organize Peptides with Multiple Protein Mappings</h3>\n",
    "                        <p><b>â Manual processing selected.</b> You'll review each protein mapping combination and decide how to handle it.</p>\n",
    "                        <div style='background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin-top: 10px;'>\n",
    "                            <h4>Peptides Mapped to Multiple Proteins</h4>\n",
    "                            <p>For each combination of proteins, you can choose:</p>\n",
    "                            <ul>\n",
    "                                <li><b>new</b> - Create a separate row for this protein</li>\n",
    "                                <li><b>remove</b> - Remove this protein from the combination</li>\n",
    "                                <li><b>asis</b> - Returns current combination unchanged</li>\n",
    "                                <li><b>Custom: (protein ID)</b> - e.g., Custom: P02666A1 to assign a custom protein ID</li>\n",
    "                            </ul>\n",
    "                            <p>These options help you handle situations where:</p>\n",
    "                            <ul>\n",
    "                                <li>Multiple proteins in Master Protein Accessions</li>\n",
    "                                <li>Multiple proteins in Positions in Proteins</li>\n",
    "                                <li>Proteins from different species assigned to the same peptide</li>\n",
    "                                <li>Unknown protein mappings (from missing values)</li>\n",
    "                            </ul>\n",
    "                            <details>\n",
    "                                <summary style='cursor: pointer; font-weight: bold;'>Click to see examples</summary>\n",
    "                                <div style='margin-top: 10px; padding: 10px; background-color: #f1f8e9; border-radius: 5px;'>\n",
    "                                {example_1}\n",
    "                                {example_2}\n",
    "                                {example_3}\n",
    "                                {example_4}\n",
    "                                </div>\n",
    "                            </details>\n",
    "                        </div>\n",
    "                        <p style='margin-top: 10px;'>Please review the protein combinations below and click \"Submit\" when finished.</p>\n",
    "                    </div>\n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    # User selected to use default\n",
    "                    self.steptwo_output_html_message = \"\"\"\n",
    "                    <div style='padding: 10px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;'>\n",
    "                        <h3>Step 2 (Optional): Organize Peptides with Multiple Protein Mappings</h3>\n",
    "                        <p><b>â Default processing selected.</b> Peptides will be keep defualt mapping based on Master Protein Accessions.</p>\n",
    "                        <div style='background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin-top: 10px;'>\n",
    "                            <p>The system will follow these default rules:</p>\n",
    "                            <ul>\n",
    "                                <li>Use the first protein ID in the Master Protein Accessions field</li>\n",
    "                                <li>For peptides with multiple positions, prefer the master protein position</li>\n",
    "                                <li>Retain all peptides in the dataset without manual filtering</li>\n",
    "                            </ul>\n",
    "                        </div>\n",
    "                        <p style='color: green; margin-top: 10px;'><b>â No changes will be made to the protein mapping. You can proceed to Step 3.</b></p>\n",
    "                    </div>\n",
    "                    \"\"\"\n",
    "                \n",
    "                # Update the display\n",
    "                with self.steptwo_status_output:\n",
    "                    clear_output(wait=True)\n",
    "                    display(HTML(self.steptwo_output_html_message))\n",
    "            \n",
    "    def _update_step3_message(self, change):\n",
    "        \"\"\"Update Step 3 status message based on group processing status\"\"\"\n",
    "        # Get the value from the notification widget\n",
    "        option_completed = change['new']\n",
    "\n",
    "        # Check current status of group data\n",
    "        has_group_data = bool(self.group_processor.group_data)\n",
    "        group_count = len(self.group_processor.group_data) if has_group_data else 0\n",
    "        \n",
    "        # Determine which option was completed\n",
    "        option1_completed = has_group_data and option_completed == 1  # Uploaded from file\n",
    "        option2_completed = has_group_data and option_completed == 2  # Assigned groups\n",
    "        option3_completed = has_group_data and option_completed == 3  # No groups option\n",
    "        \n",
    "        if has_group_data:\n",
    "            # Create summary of group data for display\n",
    "            group_summary = []\n",
    "            for group_number, group_info in self.group_processor.group_data.items():\n",
    "                group_name = group_info['grouping_variable']\n",
    "                column_count = len(group_info['abundance_columns'])\n",
    "                group_summary.append(f\"{group_name} ({column_count} columns)\")\n",
    "            \n",
    "            # Limit to first 3 groups in the summary if there are many\n",
    "            if len(group_summary) > 2:\n",
    "                group_display = \", \".join(group_summary[:2]) + f\" and {len(group_summary)-2} more...\"\n",
    "            else:\n",
    "                group_display = \", \".join(group_summary)\n",
    "            \n",
    "            # Create updated message showing success with checkmarks\n",
    "            self.stepthree_output_html_message = f\"\"\"\n",
    "            <div style='padding: 10px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;'>\n",
    "                <h3>Step 3 (Optional): Assign Study Variables for Data Grouping</h3>\n",
    "                <p>This step allows you to define experimental variables by grouping absorbance data columns:</p>\n",
    "                \n",
    "                <ul style='list-style-type: none;'>\n",
    "                    <li>{('â ' if option1_completed else 'â¤')} <b>Option 1:</b> Existing Group Dictionary imported from <b>{self.group_processor.jsonfilename if option1_completed else ''}</b></li>\n",
    "                    <li>{('â ' if option2_completed else 'â¤')} <b>Option 2:</b> Assign Selected Columns to a Group {' - Completed' if option2_completed else ''}</li>\n",
    "                    <li>{('â ' if option3_completed else 'â¤')} <b>Option 3:</b> Use Selected Columns Without Grouping {' - Completed' if option3_completed else ''}</li>\n",
    "                </ul>\n",
    "                \n",
    "                <details>\n",
    "                    <summary style='cursor: pointer;'><b>How Grouping Works</b> (click to expand)</summary>\n",
    "                    <ul>\n",
    "                        <li><b>Purpose:</b> When you create a group, the system will calculate an average absorbance value across all selected columns.</li>\n",
    "                        <li><b>Example:</b> If you group 3 replicates named \"Sample1_Rep1\", \"Sample1_Rep2\", and \"Sample1_Rep3\" as \"Sample1\", you'll get a new \"Avg_Sample1\" column in your processed data.</li>\n",
    "                        <li><b>Multiple Groups:</b> The same column can be assigned to multiple groups for different analyses (e.g., a column could belong to both \"Treatment\" and \"Day1\" groups).</li>\n",
    "                    </ul>\n",
    "                </details>\n",
    "                \n",
    "                <div style='background-color: #d4edda; padding: 8px; border-radius: 5px; margin-top: 10px;'>\n",
    "                    {f\"<p style='color: green; margin: 0;'><b>â Group data successfully configured:</b> {group_count} groups defined ({group_display}).</p>\" if (option1_completed or option3_completed) else \"\"}\n",
    "                    {f\"<p style='color: green; margin: 0;'><b>â At least one group has been created.</b> You can create more groups or proceed to Step 4.</p>\" if option2_completed else \"<p style='color: green; margin: 0;'><b>You can now proceed to Step 4.</b></p>\"}\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # Default message when no groups are defined\n",
    "            self.stepthree_output_html_message = \"\"\"\n",
    "            <div style='padding: 10px; background-color: #f8f9fa; border-left: 5px solid #007bff; margin: 10px 0;'>\n",
    "                <h3>Step 3 (Optional): Assign Study Variables for Data Grouping</h3>\n",
    "                <p>This step allows you to define experimental variables by grouping absorbance data columns:</p>\n",
    "                \n",
    "                <ul style='list-style-type: none;'>\n",
    "                    <li>â¤ <b>Option 1:</b> Upload Existing Group Dictionary - Import a predefined JSON file from previous analyses</li>\n",
    "                    <li>â¤ <b>Option 2:</b> Assign Selected Columns to a Group - Select columns and assign them a group name</li>\n",
    "                    <li>â¤ <b>Option 3:</b> Use Selected Columns Without Grouping - Create individual groups for each column</li>\n",
    "                </ul>\n",
    "                \n",
    "                <details>\n",
    "                    <summary style='cursor: pointer;'><b>How Grouping Works</b> (click to expand)</summary>\n",
    "                    <ul>\n",
    "                        <li><b>Purpose:</b> When you create a group, the system will calculate an average absorbance value across all selected columns.</li>\n",
    "                        <li><b>Example:</b> If you group 3 replicates named \"Sample1_Rep1\", \"Sample1_Rep2\", and \"Sample1_Rep3\" as \"Sample1\", you'll get a new \"Avg_Sample1\" column in your processed data.</li>\n",
    "                        <li><b>Multiple Groups:</b> The same column can be assigned to multiple groups for different analyses (e.g., a column could belong to both \"Treatment\" and \"Day1\" groups).</li>\n",
    "                    </ul>\n",
    "                </details>\n",
    "                \n",
    "                <p style='color: #007bff; margin-top: 10px;'><b>Choose one of the three options above to configure your data groups.</b></p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Update the status display\n",
    "        with self.stepthree_status_output:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(self.stepthree_output_html_message))\n",
    "\n",
    "    def _create_option_summary(self, number, title, is_selected):\n",
    "        \"\"\"Create the summary text for an option, with checkmark if selected\"\"\"\n",
    "        if is_selected:\n",
    "            return f\"Option {number}: {title} â\"\n",
    "        else:\n",
    "            return f\"Option {number}: {title}\"\n",
    "    # Add a method to update status after submit button is clicked (for manual processing)\n",
    "    def _on_submit_complete(self, success=True):\n",
    "        \"\"\"Update Step 2 status after manual processing is completed\"\"\"\n",
    "        if success:\n",
    "            self.steptwo_output_html_message = \"\"\"\n",
    "            <div style='padding: 10px; background-color: #e8f5e9; border-left: 5px solid #4caf50; margin: 10px 0;'>\n",
    "                <h3>Step 2 (Optional): Organize Peptides with Multiple Protein Mappings</h3>\n",
    "                <p><b>â Manual processing completed successfully.</b> Your protein mapping choices have been applied.</p>\n",
    "                <div style='background-color: #f8f9fa; padding: 10px; border-radius: 5px; margin-top: 10px;'>\n",
    "                    <p>The system has processed your selections for each protein combination.</p>\n",
    "                    <ul>\n",
    "                        <li>New protein entries have been created where requested</li>\n",
    "                        <li>Proteins marked for removal have been filtered out</li>\n",
    "                        <li>Custom protein IDs have been applied where specified</li>\n",
    "                    </ul>\n",
    "                </div>\n",
    "                <p style='color: green; margin-top: 10px;'><b>â All protein mappings have been processed. You can proceed to Step 3.</b></p>\n",
    "            </div>\n",
    "            \"\"\"        \n",
    "            # Mark protein processing as complete and enable group processing\n",
    "            self.protein_processing_complete = True\n",
    "            self.group_processor.enable_widgets(True)\n",
    "        else:\n",
    "            self.steptwo_output_html_message = \"\"\"\n",
    "            <div style='padding: 10px; background-color: #fff3e0; border-left: 5px solid #ff9800; margin: 10px 0;'>\n",
    "                <h3>Step 2 (Optional): Organize Peptides with Multiple Protein Mappings</h3>\n",
    "                <p><b>â ï¸ An error occurred during protein mapping processing.</b></p>\n",
    "                <p>Please check your selections and try again. Make sure all entries have valid decision values.</p>\n",
    "            </div>\n",
    "            \"\"\"        \n",
    "            # Keep protein processing as incomplete\n",
    "            self.protein_processing_complete = False\n",
    "            self.group_processor.enable_widgets(False)\n",
    "    \n",
    "        # Update the display\n",
    "        with self.steptwo_status_output:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(self.steptwo_output_html_message)) \n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display the complete workflow interface\"\"\"\n",
    "        # Step 1: Upload Data\n",
    "        widgets_ui = self.data_transformer.display_widgets()\n",
    "        step1_box = widgets.VBox([\n",
    "            self.stepone_status_output,\n",
    "            widgets_ui\n",
    "        ], layout=widgets.Layout(width='auto', padding='5px', height='auto', overflow='hidden'))\n",
    "        display(step1_box)\n",
    "        # Step 2 (Optional): Organize proteins\n",
    "        #if self.data_transformer.missing_proteins:\n",
    "        #    protein_mapping_widget_outputs = self.protein_handler.display_widgets()\n",
    "        #else:\n",
    "        #    protein_mapping_widget_outputs = widgets.VBox([self.protein_handler.protein_mapping_output_area])\n",
    "        \n",
    "        step2_box = widgets.VBox([\n",
    "            self.steptwo_status_output,\n",
    "            #widgets.HTML(\"<h3><u>Protein Mapping</u></h3>\"),\n",
    "            self.protein_handler.protein_mapping_widget,\n",
    "            self.protein_handler.buttonuniprot_output_area_output,\n",
    "            #self.protein_handler.uniprot_output_area,\n",
    "            #widgets.VBox([self.protein_handler.progress_uniprot, self.protein_handler.counter_text]),\n",
    "            self.protein_handler.uniprot_output_area,\n",
    "            self.protein_handler.protein_mapping_output_area,\n",
    "        ], layout=widgets.Layout(width='auto', padding='5px', height='auto', overflow='hidden'))\n",
    "        display(step2_box)\n",
    "\n",
    "        # Step 3: Assgin Study Varriable Gouping\n",
    "        #display(self.stepthree_status_output)\n",
    "\n",
    "        #input_group_selector1 = self.group_processor.display_group_selector()\n",
    "        input_group_selector2 = self.group_processor.display_widgets()\n",
    "        # Step 3: Group Selection\n",
    "        step3_box = widgets.VBox([\n",
    "            self.stepthree_status_output,\n",
    "            #input_group_selector1,\n",
    "            input_group_selector2,\n",
    "        ], layout=widgets.Layout(width='auto', padding='5px', height='auto', overflow='hidden'))\n",
    "        display(step3_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f50828-45c1-444e-be98-4ed3ff7a947e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CombineAverageDataframes:\n",
    "    def __init__(self, data_transformer, group_processor, protein_handler):\n",
    "        self.data_transformer = data_transformer\n",
    "        self.group_processor = group_processor\n",
    "        self.pd_results = data_transformer.pd_results\n",
    "        self.mbpdb_results = data_transformer.mbpdb_results\n",
    "        self.pd_results_cleaned = protein_handler.pd_results_cleaned if hasattr(protein_handler, 'pd_results_cleaned') and protein_handler.pd_results_cleaned is not None else pd.DataFrame()\n",
    "        self._merged_df = None\n",
    "        # Set up observer for data changes\n",
    "        self.data_transformer.observe(self._handle_data_change, names=['pd_results', 'mbpdb_results'])\n",
    "        \n",
    "    @property  # Make protein_dict a property that always reads from data_transformer\n",
    "    def protein_dict(self):\n",
    "        return self.data_transformer.protein_dict\n",
    "        \n",
    "    def _handle_data_change(self, change):\n",
    "        \"\"\"Handle changes in the input data.\"\"\"\n",
    "        if change.name == 'pd_results':\n",
    "            self.pd_results = change.new\n",
    "        elif change.name == 'mbpdb_results':\n",
    "            self.mbpdb_results = change.new\n",
    "        elif change.name == 'pd_results_cleaned':\n",
    "            self.pd_results_cleaned = change.new        # Re-run interactive display\n",
    "        clear_output()        \n",
    "    @property\n",
    "    def merged_df(self):\n",
    "        \"\"\"Property to access the merged DataFrame.\"\"\"\n",
    "        return self._merged_df\n",
    "        \n",
    "    def add_protein_info(self, df):\n",
    "        \"\"\"\n",
    "        Adds protein species and name information to the dataframe based on Master Protein Accessions,\n",
    "        inserting them after Master Protein Accessions and before Positions in Proteins.\n",
    "        \n",
    "        Args:\n",
    "            df (pandas.DataFrame): Input dataframe containing 'Master Protein Accessions' column\n",
    "            \n",
    "        Returns:\n",
    "            pandas.DataFrame: DataFrame with added 'protein_species' and 'protein_name' columns\n",
    "        \"\"\"\n",
    "        # First, make a copy to avoid modifying the original\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Create temporary columns\n",
    "        df['protein_species'] = 'Unknown'\n",
    "        df['protein_name'] = 'Unknown Protein'\n",
    "        \n",
    "        # Process each row\n",
    "        for idx, row in df.iterrows():\n",
    "            # Get the protein accessions - handle potential multiple proteins\n",
    "            proteins = str(row['Master Protein Accessions']).split(';')\n",
    "            \n",
    "            # Process first protein in the list (primary protein)\n",
    "            if proteins and proteins[0] != '' and proteins[0] != 'nan':\n",
    "                protein = proteins[0].strip()\n",
    "                df.at[idx, 'protein_species'] = self.protein_dict.get(protein, {}).get('species', \"Unknown\")\n",
    "                df.at[idx, 'protein_name'] = self.protein_dict.get(protein, {}).get('name', \"Unknown Protein\")\n",
    "        \n",
    "        # Get all column names\n",
    "        all_cols = list(df.columns)\n",
    "        \n",
    "        # Remove the new columns from their current position\n",
    "        remaining_cols = [col for col in all_cols if col not in ['protein_species', 'protein_name']]\n",
    "        \n",
    "        # Find the position after 'Master Protein Accessions'\n",
    "        insert_pos = remaining_cols.index('Master Protein Accessions') + 1\n",
    "        \n",
    "        # Create the new column order\n",
    "        new_cols = (\n",
    "            remaining_cols[:insert_pos] +  # Columns before and including Master Protein Accessions\n",
    "            ['protein_species', 'protein_name'] +  # New columns\n",
    "            remaining_cols[insert_pos:]  # Remaining columns\n",
    "        )\n",
    "        \n",
    "        # Reorder the DataFrame with the new column order\n",
    "        result_df = df.reindex(columns=new_cols)\n",
    "        \n",
    "        # Verify column order (optional debug print)\n",
    "        # print(\"DEBUG: Column order:\", new_cols)\n",
    "        # print(\"DEBUG: Position of Master Protein Accessions:\", insert_pos)\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    def extract_bioactive_peptides(self):\n",
    "        \"\"\"\n",
    "        Extracts the list of bioactive peptide matches from the imported MBPDB search.\n",
    "        \"\"\"\n",
    "        if not self.mbpdb_results.empty:\n",
    "            # Drop rows where protein_id is NaN or 'None'\n",
    "            mbpdb_results_cleaned = self.mbpdb_results.copy()\n",
    "            mbpdb_results_cleaned.dropna(subset=['search_peptide'], inplace=True)\n",
    "            mbpdb_results_cleaned = mbpdb_results_cleaned[mbpdb_results_cleaned['protein_id'] != 'None']\n",
    "\n",
    "            # Dynamically build aggregation dictionary based on available columns\n",
    "            available_columns = mbpdb_results_cleaned.columns.tolist()\n",
    "            \n",
    "            # Base aggregation for required columns\n",
    "            agg_dict = {}\n",
    "            \n",
    "            # Always include these if they exist\n",
    "            if 'peptide' in available_columns:\n",
    "                agg_dict['peptide'] = 'first'\n",
    "            if 'protein_id' in available_columns:\n",
    "                agg_dict['protein_id'] = 'first'\n",
    "            \n",
    "            # Add optional columns if they exist\n",
    "            optional_columns = {\n",
    "                'protein_description': 'first',\n",
    "                '% Alignment': 'first', \n",
    "                'species': 'first',\n",
    "                'intervals': 'first',\n",
    "                'additional_details': 'first',\n",
    "                'ic50': 'first',\n",
    "                'inhibition_type': 'first',\n",
    "                'inhibited_microorganisms': 'first',\n",
    "                'ptm': 'first',\n",
    "                'title': 'first',\n",
    "                'authors': 'first',\n",
    "                'abstract': 'first',\n",
    "                'doi': 'first',\n",
    "                'search_type': 'first',\n",
    "                'scoring_matrix': 'first'\n",
    "            }\n",
    "            \n",
    "            for col, agg_func in optional_columns.items():\n",
    "                if col in available_columns:\n",
    "                    agg_dict[col] = agg_func\n",
    "            \n",
    "            # Special handling for function column (combine unique values)\n",
    "            if 'function' in available_columns:\n",
    "                agg_dict['function'] = lambda x: list(x.dropna().unique())\n",
    "            \n",
    "            # Ensure we have at least peptide for grouping\n",
    "            if not agg_dict:\n",
    "                print(\"Warning: No expected columns found for aggregation. Using available columns as-is.\")\n",
    "                return mbpdb_results_cleaned, mbpdb_results_cleaned\n",
    "\n",
    "            # Perform the groupby and aggregation with error handling\n",
    "            try:\n",
    "                self.mbpdb_results_grouped = mbpdb_results_cleaned.groupby('search_peptide').agg(agg_dict).reset_index()\n",
    "                \n",
    "                # Flatten the 'function' list if it exists\n",
    "                if 'function' in self.mbpdb_results_grouped.columns:\n",
    "                    self.mbpdb_results_grouped['function'] = self.mbpdb_results_grouped['function'].apply(\n",
    "                        lambda x: '; '.join([str(func) for func in x if str(func) != 'nan']) if isinstance(x, list) else str(x)\n",
    "                    )\n",
    "                \n",
    "                return mbpdb_results_cleaned, self.mbpdb_results_grouped\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during MBPDB aggregation: {str(e)}\")\n",
    "                print(f\"Available columns: {available_columns}\")\n",
    "                print(f\"Aggregation dict: {list(agg_dict.keys())}\")\n",
    "                return mbpdb_results_cleaned, mbpdb_results_cleaned\n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    def create_unique_id(self, row):\n",
    "        \"\"\"Creates a unique ID for each peptide row.\"\"\"\n",
    "        # Handle Sequence - convert list to comma-separated string if needed\n",
    "        sequence = row['Sequence']\n",
    "        if isinstance(sequence, list):\n",
    "            sequence = ','.join(sequence)\n",
    "        else:\n",
    "            sequence = str(sequence).strip()\n",
    "        \n",
    "        # Create unique ID with modifications if present\n",
    "        if pd.notna(row['Modifications']):\n",
    "            unique_id = sequence + \"_\" + row['Modifications'].strip()\n",
    "        else:\n",
    "            unique_id = sequence\n",
    "        \n",
    "        # Ensure unique_id is a string and strip trailing underscores\n",
    "        unique_id = str(unique_id).strip()\n",
    "        return unique_id.rstrip('_')\n",
    "\n",
    "    def process_pd_results(self, mbpdb_results_grouped):\n",
    "        pd_results_cleaned = self.pd_results_cleaned\n",
    "        \n",
    "        # Process positions and accessions\n",
    "        #pd_results_cleaned['Positions in Proteins'] = pd_results_cleaned['Positions in Proteins'].str.split(';', expand=False).str[0]\n",
    "        #pd_results_cleaned['Master Protein Accessions'] = pd_results_cleaned['Master Protein Accessions'].str.split(';', expand=False).str[0]\n",
    "                    \n",
    "        # Handle NaN/Unknown values first\n",
    "        pd_results_cleaned['Master Protein Accessions'] = pd_results_cleaned['Master Protein Accessions'].fillna('Unknown')\n",
    "        pd_results_cleaned['Positions in Proteins'] = pd_results_cleaned['Positions in Proteins'].fillna('Unknown')\n",
    "        \n",
    "        # Create sequence column if needed\n",
    "        # Create sequence column if needed\n",
    "        if 'Sequence' not in pd_results_cleaned.columns:\n",
    "            # First create Sequence column with NaN values\n",
    "            pd_results_cleaned['Sequence'] = pd.NA\n",
    "            \n",
    "            def extract_sequence(annotated_seq):\n",
    "                if pd.isna(annotated_seq):\n",
    "                    return pd.NA\n",
    "                \n",
    "                # Case 1: [X].SEQUENCE.[X] format\n",
    "                if '.' in annotated_seq:\n",
    "                    parts = annotated_seq.split('.')\n",
    "                    if len(parts) > 1:\n",
    "                        return parts[1]\n",
    "                \n",
    "                # Case 2: Plain sequence like \"LLL\" or \"WE\"\n",
    "                return annotated_seq\n",
    "            \n",
    "            # Apply the extraction function to all rows\n",
    "            pd_results_cleaned['Sequence'] = pd_results_cleaned['Annotated Sequence'].apply(extract_sequence)\n",
    "        \n",
    "        # Create unique ID\n",
    "        pd_results_cleaned['unique ID'] = pd_results_cleaned.apply(self.create_unique_id, axis=1)\n",
    "\n",
    "        # Extract start and stop positions\n",
    "        try:\n",
    "            # Initialize start and stop columns with NaN\n",
    "            pd_results_cleaned['start'] = pd.NA\n",
    "            pd_results_cleaned['stop'] = pd.NA\n",
    "            \n",
    "            # Create mask for rows without semicolons (single positions) and not Unknown\n",
    "            valid_position_mask = (~pd_results_cleaned['Positions in Proteins'].str.contains(';', na=False) & \n",
    "                                 (pd_results_cleaned['Positions in Proteins'] != 'Unknown'))\n",
    "            \n",
    "            # Process rows with single positions\n",
    "            single_positions = pd_results_cleaned.loc[valid_position_mask, 'Positions in Proteins']\n",
    "            if not single_positions.empty:\n",
    "                extracted = single_positions.str.extract(r'\\[(\\d+)-(\\d+)\\]')\n",
    "                \n",
    "                # Convert to numeric and handle invalid values\n",
    "                pd_results_cleaned.loc[valid_position_mask, 'start'] = pd.to_numeric(extracted[0], errors='coerce')\n",
    "                pd_results_cleaned.loc[valid_position_mask, 'stop'] = pd.to_numeric(extracted[1], errors='coerce')\n",
    "            \n",
    "            # Convert to Int64 to handle missing values properly\n",
    "            pd_results_cleaned['start'] = pd_results_cleaned['start'].astype('Int64')\n",
    "            pd_results_cleaned['stop'] = pd_results_cleaned['stop'].astype('Int64')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing positions: {str(e)}\")\n",
    "        \n",
    "    \n",
    "        # Reorder columns with unique ID and Sequence first\n",
    "        remaining_cols = [col for col in pd_results_cleaned.columns \n",
    "                         if col not in ['unique ID', 'Sequence', 'Master Protein Accessions', \n",
    "                                      'Positions in Proteins', 'start', 'stop']]\n",
    "        \n",
    "        columns_order = ['unique ID', 'Sequence', 'Master Protein Accessions', \n",
    "                        'Positions in Proteins', 'start', 'stop'] + remaining_cols\n",
    "        \n",
    "        pd_results_cleaned = pd_results_cleaned[columns_order]\n",
    "                \n",
    "        # Merge with MBPDB results if available\n",
    "        if self.mbpdb_results_grouped is not None and not self.mbpdb_results_grouped.empty:\n",
    "            # First do the regular merge\n",
    "            merged_df = pd.merge(pd_results_cleaned, self.mbpdb_results_grouped, \n",
    "                                right_on='search_peptide', left_on='unique ID', how='left')\n",
    "            \n",
    "            # Second pass: handle comma-separated unique IDs\n",
    "            comma_mask = merged_df['unique ID'].str.contains(',', na=False)\n",
    "            comma_rows = merged_df[comma_mask].copy()\n",
    "            \n",
    "            for idx, row in comma_rows.iterrows():\n",
    "                # Split the unique ID\n",
    "                unique_ids = row['unique ID'].split(',')\n",
    "                \n",
    "                # Check if any part matches with search_peptide\n",
    "                matches = self.mbpdb_results_grouped[self.mbpdb_results_grouped['search_peptide'].isin(unique_ids)]\n",
    "\n",
    "                if not matches.empty:\n",
    "                    # Take the first match and update all MBPDB columns\n",
    "                    match = matches.iloc[0]\n",
    "                    for col in self.mbpdb_results_grouped.columns:\n",
    "                        #if col != 'search_peptide':  # Don't overwrite unique ID\n",
    "                        merged_df.loc[idx, col] = match[col]\n",
    "        \n",
    "            #display(HTML(\"<b style='color:green;'>The MBPDB was successfully merged with the peptidomic data matching the Search Peptide and Unique ID columns (including comma-separated IDs).</b>\"))\n",
    "        \n",
    "        else:\n",
    "            merged_df = pd_results_cleaned.copy()\n",
    "            merged_df['function'] = np.nan\n",
    "\n",
    "        \n",
    "        # Ensure columns are in correct order\n",
    "        final_column_order = columns_order + [col for col in merged_df.columns if col not in columns_order]\n",
    "        merged_df = merged_df[final_column_order]\n",
    "        \n",
    "        return merged_df\n",
    "    \n",
    "    def calculate_group_abundance_averages(self, df, group_data):\n",
    "        \"\"\"Calculates group abundance averages, organizing them with averages\"\"\"\n",
    "        # Check if all average abundance columns already exist\n",
    "        all_columns_exist = True\n",
    "        for group_number, details in group_data.items():\n",
    "            average_column_name = f\"Avg_{details['grouping_variable']}\"\n",
    "            if average_column_name not in df.columns:\n",
    "                all_columns_exist = False\n",
    "                break\n",
    "        \n",
    "        if all_columns_exist:\n",
    "            display(HTML('<b style=\"color:orange;\">All average abundance columns already exist. Returning original DataFrame.</b>'))\n",
    "            return df\n",
    "        \n",
    "        # If not all columns exist, proceed with calculations\n",
    "        average_columns = {}\n",
    "        \n",
    "        # Calculate all averages but store them separately\n",
    "        for group_number, details in group_data.items():\n",
    "            grouping_variable = details['grouping_variable']\n",
    "            abundance_columns = details['abundance_columns']\n",
    "            \n",
    "            # Convert abundance columns to numeric\n",
    "            for col in abundance_columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            \n",
    "            # Define column names\n",
    "            average_column_name = f\"Avg_{grouping_variable}\"\n",
    "        \n",
    "            average_columns[average_column_name] = df[abundance_columns].mean(axis=1, skipna=True)\n",
    "        \n",
    "        # Combine the columns in the desired order (all averages, then all SEMs)\n",
    "        new_columns = {**average_columns}\n",
    "        \n",
    "        # Add new columns to DataFrame\n",
    "        df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "        \n",
    "        #if not df.empty:\n",
    "            #display(HTML('<b style=\"color:green;\">Group average columns have been successfully added to the DataFrame.</b>'))\n",
    "        return df\n",
    "        \n",
    "    def create_column_to_groups_mapping(self, group_data):\n",
    "        \"\"\"\n",
    "        Creates a mapping dictionary where keys are column names and values are lists of \n",
    "        grouping variables that include this column.\n",
    "        \"\"\"\n",
    "        column_to_groups = {}\n",
    "        \n",
    "        # Iterate through all groups\n",
    "        for group_number, details in group_data.items():\n",
    "            grouping_variable = details['grouping_variable']\n",
    "            abundance_columns = details['abundance_columns']\n",
    "            \n",
    "            # For each column, add the current grouping variable to its list\n",
    "            for column in abundance_columns:\n",
    "                if column not in column_to_groups:\n",
    "                    column_to_groups[column] = []\n",
    "                column_to_groups[column].append(grouping_variable)\n",
    "        \n",
    "        return column_to_groups\n",
    "\n",
    "    def update_column_names_with_groups(self, df, group_data):\n",
    "        \"\"\"\n",
    "        Updates column names in the DataFrame by adding grouping information.\n",
    "        \"\"\"\n",
    "        # Create the mapping of columns to their grouping variables\n",
    "        column_to_groups = self.create_column_to_groups_mapping(group_data)\n",
    "        # Create a copy of the DataFrame to avoid modifying the original\n",
    "        df_renamed = df.copy()\n",
    "        \n",
    "        # Create renaming dictionary\n",
    "        rename_dict = {}\n",
    "        for column in column_to_groups:\n",
    "            if column in df.columns:\n",
    "                groups_str = \"; \".join(column_to_groups[column])\n",
    "                new_name = f\"{column} 'Grouped: ({groups_str})'\"\n",
    "                rename_dict[column] = new_name\n",
    "        \n",
    "        # Rename columns\n",
    "        df_renamed = df_renamed.rename(columns=rename_dict)\n",
    "        \n",
    "        if rename_dict:\n",
    "            pass\n",
    "            #display(HTML('<b style=\"color:green;\">Column names have been updated with grouping information.</b>'))\n",
    "        else:\n",
    "            display(HTML('<b style=\"color:orange;\">No columns were updated with grouping information.</b>'))\n",
    "        \n",
    "        return df_renamed\n",
    "\n",
    "    def process_data(self, group_data):\n",
    "        \"\"\"Main method to process all data.\"\"\"\n",
    "        if hasattr(self, 'pd_results') and self.pd_results is not None and not self.pd_results.empty:\n",
    "            try:\n",
    "                # Extract and process bioactive peptides\n",
    "                mbpdb_results_cleaned, self.mbpdb_results_grouped = self.extract_bioactive_peptides()\n",
    "                \n",
    "                if not hasattr(self, 'pd_results_cleaned') or self.pd_results_cleaned is None:\n",
    "                    self.pd_results_cleaned = self.pd_results.copy()\n",
    "                \n",
    "                # Process PD results and merge with MBPDB\n",
    "                merged_df_temp = self.process_pd_results(self.mbpdb_results_grouped)\n",
    "                \n",
    "                # Calculate abundance averages if group_data exists\n",
    "                if group_data:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                        df_temp = self.calculate_group_abundance_averages(merged_df_temp, group_data)\n",
    "                        \n",
    "                        # Apply the new function to update column names with grouping information\n",
    "                        final_df_temp = self.update_column_names_with_groups(df_temp, group_data)\n",
    "                else:\n",
    "                    final_df_temp = merged_df_temp\n",
    "                    display(HTML(\"<b style='color:orange;'>No group data provided. Skipping abundance calculations and column renaming.</b>\"))\n",
    "        \n",
    "                \n",
    "                # Store the final DataFrame and add protein name and species \n",
    "                final_df = self.add_protein_info(final_df_temp)\n",
    "                self._merged_df = final_df\n",
    "\n",
    "                return final_df\n",
    "\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<b style='color:red;'>Error processing data: {str(e)}</b>\"))\n",
    "                return None\n",
    "        else:\n",
    "            display(HTML(\"<b style='color:red;'>No PD results data available for processing.</b>\"))\n",
    "            return None\n",
    "    \n",
    "    def update_data(self, pd_results):\n",
    "        \"\"\"Update data and refresh filtered columns\"\"\"\n",
    "        self.pd_results = pd_results\n",
    "        \n",
    "        # Only update if we have valid data\n",
    "        if pd_results is not None and not pd_results.empty:\n",
    "            self.setup_data()\n",
    "            \n",
    "            # Update the dropdown with new filtered columns\n",
    "            self.column_dropdown.options = self.filtered_columns\n",
    "            \n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                #display(widgets.HTML('<b style=\"color:green;\">Data updated successfully. Column selection refreshed.</b>'))\n",
    "        else:\n",
    "            # Clear options if no data\n",
    "            self.column_dropdown.options = []\n",
    "            with self.output:\n",
    "                self.output.clear_output()\n",
    "                display(widgets.HTML('<b style=\"color:orange;\">No data available for column selection.</b>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20958467-4a02-4b10-afbb-1a3224430ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportManager:\n",
    "    \"\"\"Class to manage all export operations with predefined buttons\"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Create output area for status messages\n",
    "        self.status_output = widgets.Output()\n",
    "        # Create all export buttons\n",
    "        self.mbpdb_button = widgets.Button(\n",
    "            description='Download MBPDB Results',\n",
    "            icon='download',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            tooltip='Download the results from searching your peptides against the MBPDB database',\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        self.group_data_button = widgets.Button(\n",
    "            description='Download Group Definitions',\n",
    "            icon='download',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            tooltip='Download the categorical variable definitions used for data grouping and analysis',\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        self.dataset_button = widgets.Button(\n",
    "            description='Download Merged Dataset',\n",
    "            icon='download',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            tooltip='Download the complete merged dataset containing all processed data',\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        self.export_group_correlation_button = widgets.Button(\n",
    "            description='Export Sample-to-Sample Correlations',\n",
    "            button_style='info',\n",
    "            icon='download',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            disabled=True\n",
    "            )   \n",
    "        \n",
    "        self.export_replicate_correlation_button = widgets.Button(\n",
    "            description='Export Technical Replicate Correlations',\n",
    "            button_style='info',\n",
    "            icon='download',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            disabled=True\n",
    "            )\n",
    "        \n",
    "        self.export_sequence_list_button = widgets.Button(\n",
    "            description='Export Summed Peptide Results',\n",
    "            button_style='info',\n",
    "            icon='download',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            disabled=True\n",
    "        )\n",
    "\n",
    "        self.export_summed_peptide_results_button = widgets.Button(\n",
    "            description='Export Summed Peptide Results',\n",
    "            button_style='info',\n",
    "            icon='download',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            disabled=True\n",
    "        )\n",
    "\n",
    "        self.export_protein_data_button = widgets.Button(\n",
    "            description='Export Protein Analysis Results',\n",
    "            button_style='info',\n",
    "            icon='download',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            disabled=True\n",
    "        )\n",
    "\n",
    "        self.export_summed_function_data_button = widgets.Button(\n",
    "            description='Export Summed Functional Data',\n",
    "            button_style='info',\n",
    "            icon='download',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            disabled=True\n",
    "        )\n",
    "        self.correlation_type = widgets.Dropdown(\n",
    "            options=['Pearson', 'Spearman'],\n",
    "            description='Correlation:',\n",
    "            value='Pearson',\n",
    "            layout=widgets.Layout(width='300px', height='30px'),\n",
    "            disabled=True\n",
    "        )\n",
    "\n",
    "        self.log_transform = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Log10 transform data',\n",
    "            layout=widgets.Layout(width='300px', height='50px'),\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        self.export_group_correlation_button.on_click(self._handle_group_correlation_download)\n",
    "        self.export_replicate_correlation_button.on_click(self._handle_replicate_correlation_download)\n",
    "        self.export_summed_peptide_results_button.on_click(self._handle_summed_peptide_download)\n",
    "        self.export_sequence_list_button.on_click(self._handle_sequence_list_download)\n",
    "        self.export_protein_data_button.on_click(self._handle_protein_download)\n",
    "        self.export_summed_function_data_button.on_click(self._handle_summed_function_download)\n",
    "\n",
    "        # Add click handlers\n",
    "        self.mbpdb_button.on_click(self._handle_mbpdb_download)\n",
    "        self.group_data_button.on_click(self._handle_group_download)\n",
    "        self.dataset_button.on_click(self._handle_dataset_download)\n",
    "        \"\"\"\n",
    "        # Create labels for descriptions\n",
    "        self.mbpdb_desc = widgets.HTML(\n",
    "            value='<div style=\"color: #666; font-style: italic; margin: 5px 0;\">Download MBPDB search results and bioactivity data</div>'\n",
    "        )\n",
    "        self.group_desc = widgets.HTML(\n",
    "            value='<div style=\"color: #666; font-style: italic; margin: 5px 0;\">Download categorical variable definitions for data grouping</div>'\n",
    "        )\n",
    "        self.dataset_desc = widgets.HTML(\n",
    "            value='<div style=\"color: #666; font-style: italic; margin: 5px 0;\">Download the complete processed dataset</div>'\n",
    "        )\n",
    "        \"\"\"\n",
    "        # Store references to data\n",
    "        self.mbpdb_df = None\n",
    "        self.group_data = None\n",
    "        self.merged_df = None\n",
    "        \n",
    "        # Create button container with spacing\n",
    "        \"\"\"\n",
    "        self.button_container = widgets.VBox([\n",
    "            self.mbpdb_button,\n",
    "            self.group_data_button,\n",
    "            self.dataset_button,\n",
    "            self.export_group_correlation_button,\n",
    "            self.export_replicate_correlation_button,\n",
    "            self.export_summed_peptide_results_button,\n",
    "            self.export_protein_data_button,\n",
    "            self.export_summed_function_data_button\n",
    "        ],layout=widgets.Layout(width='310px'),\n",
    "        )\"\"\"\n",
    "\n",
    "    def calculate_correlation(self, x, y):\n",
    "        \"\"\"Calculate correlation based on selected method\"\"\"\n",
    "        if  self.correlation_type.value == 'Pearson':\n",
    "            return pearsonr(x, y)[0]\n",
    "        else:  # Spearman\n",
    "            return spearmanr(x, y)[0]\n",
    "        \n",
    "    def prepare_data(self, data):\n",
    "        \"\"\"Prepare data based on log transform setting\"\"\"\n",
    "        if  self.log_transform.value:\n",
    "            return np.log10(data)\n",
    "        return data\n",
    "    \n",
    "    def _convert_group_data_dict(self):\n",
    "            # Calculate within-group correlations\n",
    "            df = self.merged_df.copy()\n",
    "            group_data = self.group_data\n",
    "            \n",
    "            # Find all columns that have the 'Grouped:' pattern\n",
    "            grouped_columns = [col for col in df.columns if \" 'Grouped:\" in str(col)]\n",
    "            \n",
    "            # Create mapping for column renaming (strip the 'Grouped:' part)\n",
    "            renamed_columns = {}\n",
    "            for col in grouped_columns:\n",
    "                base_col_name = col.split(\" 'Grouped:\")[0].strip()\n",
    "                renamed_columns[col] = base_col_name\n",
    "            \n",
    "            # Rename the columns in the DataFrame to remove the 'Grouped:' part\n",
    "            df = df.rename(columns=renamed_columns)\n",
    "            \n",
    "            # Update the abundance_columns in group_data to match the renamed columns\n",
    "            updated_group_data = {}\n",
    "            for key, value in group_data.items():\n",
    "                grouping_variable = value['grouping_variable']\n",
    "                abundance_columns = value['abundance_columns']\n",
    "                \n",
    "                # Create a new list of abundance columns that match the renamed columns\n",
    "                updated_abundance_columns = []\n",
    "                for col in abundance_columns:\n",
    "                    # Find the matching column in the renamed DataFrame\n",
    "                    matching_cols = [c for c in df.columns if c == col]\n",
    "                    if matching_cols:\n",
    "                        updated_abundance_columns.append(matching_cols[0])\n",
    "                \n",
    "                updated_group_data[key] = {\n",
    "                    'grouping_variable': grouping_variable,\n",
    "                    'abundance_columns': updated_abundance_columns\n",
    "                }\n",
    "            \n",
    "            # Use the updated group data for further processing\n",
    "            group_data = updated_group_data\n",
    "            return group_data, df\n",
    "\n",
    "    def _process_bioactive_data(self):\n",
    "        \"\"\"Process bioactive peptide data for visualization\"\"\"\n",
    "        if self.merged_df is None:\n",
    "            return None\n",
    "            \n",
    "\n",
    "        df = self.merged_df.copy()\n",
    "        absorbance_cols = []\n",
    "        selected_groups = []\n",
    "        unique_function_absorbance = {}\n",
    "        # Get Absorbance columns based on selected groups\n",
    "        for _, value in self.group_data.items():\n",
    "            grouping_variable = value['grouping_variable']\n",
    "            abundance_columns = value['abundance_columns']        \n",
    "            selected_groups.append(grouping_variable)\n",
    "            if grouping_variable != abundance_columns:\n",
    "                absorbance_cols.append(f'Avg_{grouping_variable}')\n",
    "            else:\n",
    "                absorbance_cols.append(abundance_columns)\n",
    "            if 'function' not in df.columns:\n",
    "                return None\n",
    "            \n",
    "            for column in absorbance_cols:\n",
    "                            \n",
    "                # Filter and process data\n",
    "                temp_df = df[['unique ID', 'function', column]].copy()\n",
    "                temp_df = temp_df[\n",
    "                    (temp_df[column] != 0) & \n",
    "                    temp_df[column].notna() &\n",
    "                    temp_df['function'].notna()\n",
    "                ]\n",
    "                \n",
    "                if temp_df.empty:\n",
    "                    continue\n",
    "                \n",
    "                # Process functions\n",
    "                temp_df.loc[:, 'function'] = temp_df['function'].fillna('').str.split(';')\n",
    "                exploded_df = temp_df.explode('function')\n",
    "                exploded_df.loc[:, 'function'] = exploded_df['function'].str.strip()\n",
    "                exploded_df = exploded_df[exploded_df['function'] != '']\n",
    "                \n",
    "                if not exploded_df.empty:\n",
    "                    function_grouped = exploded_df.groupby('function')[column].sum()\n",
    "                    unique_function_absorbance[grouping_variable] = function_grouped.to_dict()\n",
    "\n",
    "        return unique_function_absorbance, absorbance_cols              \n",
    "    \n",
    "    def _process_functional_peptide_export_data(self):\n",
    "        \"\"\"Process data for export into Excel format\"\"\"\n",
    "        unique_function_absorbance, absorbance_cols = self._process_bioactive_data()\n",
    "        if not unique_function_absorbance:\n",
    "            return None\n",
    "            \n",
    "        # Get all groups and functions\n",
    "        groups = list(unique_function_absorbance.keys())\n",
    "        all_functions = set()\n",
    "        for group_data in unique_function_absorbance.values():\n",
    "            all_functions.update(group_data.keys())\n",
    "            \n",
    "        # Calculate function counts\n",
    "        df = self.merged_df.copy()\n",
    "        summed_function_count = {}\n",
    "        unique_function_counts = {}\n",
    "        unique_function_count_averages = {}\n",
    "        summed_function_abundance = {}\n",
    "        \n",
    "        for group, abundance_column in zip(groups, absorbance_cols):\n",
    "            if abundance_column not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Filter and process data\n",
    "            temp_df = df[['unique ID', 'function', abundance_column]].copy()\n",
    "            temp_df = temp_df[\n",
    "                (temp_df[abundance_column] != 0) & \n",
    "                temp_df[abundance_column].notna() &\n",
    "                temp_df['function'].notna()\n",
    "            ]\n",
    "            \n",
    "            # Drop duplicates and calculate counts\n",
    "            filtered_df = temp_df.drop_duplicates(subset='unique ID')\n",
    "            unique_peptide_count = filtered_df['unique ID'].nunique()\n",
    "            total_sum = filtered_df[abundance_column].sum()\n",
    "            \n",
    "            # Store the totals\n",
    "            summed_function_abundance[group] = total_sum\n",
    "            summed_function_count[group] = unique_peptide_count\n",
    "            \n",
    "            # Process functions\n",
    "            filtered_df.loc[:, 'function'] = filtered_df['function'].fillna('').str.split(';')\n",
    "            exploded_df = filtered_df.explode('function')\n",
    "            exploded_df.loc[:, 'function'] = exploded_df['function'].str.strip()\n",
    "            exploded_df = exploded_df[exploded_df['function'] != '']\n",
    "            \n",
    "            if not exploded_df.empty:\n",
    "                # Count functions\n",
    "                function_counts = exploded_df['function'].value_counts().to_dict()\n",
    "                unique_function_counts[group] = function_counts\n",
    "                \n",
    "                # Calculate averages (using 1 since we're using averaged columns)\n",
    "                function_averages = {func: count for func, count in function_counts.items()}\n",
    "                unique_function_count_averages[group] = function_averages\n",
    "        \n",
    "        # Create DataFrames for export\n",
    "        peptide_count_df = pd.DataFrame.from_dict(\n",
    "            summed_function_count,\n",
    "            orient='index',\n",
    "            columns=['Counts of peptides']\n",
    "        )\n",
    "        \n",
    "        function_count_df = pd.DataFrame.from_dict(\n",
    "            unique_function_counts,\n",
    "            orient='index'\n",
    "        ).fillna(0).astype(int)\n",
    "        \n",
    "        combined_count_df = pd.concat([peptide_count_df, function_count_df], axis=1).T\n",
    "        \n",
    "        # Create abundance DataFrames\n",
    "        peptide_absorbance_df = pd.DataFrame.from_dict(\n",
    "            summed_function_abundance,\n",
    "            orient='index',\n",
    "            columns=['Summed Absorbance']\n",
    "        )\n",
    "        \n",
    "        function_absorbance_df = pd.DataFrame.from_dict(\n",
    "            unique_function_absorbance,\n",
    "            orient='index'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        combined_absorbance_df = pd.concat(\n",
    "            [peptide_absorbance_df, function_absorbance_df],\n",
    "            axis=1\n",
    "        ).T\n",
    "        \n",
    "        # Create combined DataFrame with formatted values\n",
    "        combined_df = pd.DataFrame(\n",
    "            index=combined_absorbance_df.index,\n",
    "            columns=combined_absorbance_df.columns\n",
    "        )\n",
    "        \n",
    "        for col in combined_absorbance_df.columns:\n",
    "            for idx in combined_absorbance_df.index:\n",
    "                abundance = combined_absorbance_df.loc[idx, col]\n",
    "                count = (combined_count_df.loc['Counts of peptides', col]\n",
    "                        if idx == 'Summed Absorbance'\n",
    "                        else combined_count_df.loc[idx, col])\n",
    "                combined_df.loc[idx, col] = \"-\" if (abundance == 0 and count == 0) else f\"{abundance:.2e} ({round(count)})\"\n",
    "        \n",
    "        combined_df.rename(index={'Summed Absorbance': 'Total'}, inplace=True)\n",
    "        \n",
    "        return combined_df, combined_count_df, combined_absorbance_df\n",
    "\n",
    "    def _export_summed_peptide_data(self, data):\n",
    "        \"\"\"\n",
    "        Export peptide data to Excel with summary and replicate details.\n",
    "        \n",
    "        Args:\n",
    "            data (dict): Dictionary containing peptide analysis results\n",
    "            \n",
    "        Returns:\n",
    "            bytes: Excel file content as bytes\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create summary DataFrame\n",
    "            summary_data = []\n",
    "            for group, values in data.items():\n",
    "                summary_data.append({\n",
    "                    'Group': group,\n",
    "                    'Total_Absorbance': values['total_Absorbance'],\n",
    "                    'Abundance_SEM': values['abundance_sem'],\n",
    "                    'Unique_Peptides': values['unique_peptides'],\n",
    "                    'Count_SEM': values['count_sem']\n",
    "                })\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            \n",
    "            # Create replicate details DataFrame\n",
    "            replicate_data = []\n",
    "            for group, values in data.items():\n",
    "                # Get the replicate information\n",
    "                replicate_info = values['replicate_data']\n",
    "                \n",
    "                # Add entry for each replicate\n",
    "                for i, replicate_name in enumerate(replicate_info['abundance_columns']):\n",
    "                    replicate_data.append({\n",
    "                        'Group': group,\n",
    "                        'Replicate': replicate_name,\n",
    "                        'Total_Absorbance': replicate_info['replicate_abundances'][i],\n",
    "                        'Unique_Peptides': replicate_info['replicate_counts'][i]\n",
    "                    })\n",
    "            replicate_df = pd.DataFrame(replicate_data)\n",
    "            \n",
    "            # Create Excel file in memory\n",
    "            output = io.BytesIO()\n",
    "            with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
    "                # Write summary sheet\n",
    "                summary_df.to_excel(\n",
    "                    writer, \n",
    "                    sheet_name='Summary',\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                # Write replicate details sheet\n",
    "                replicate_df.to_excel(\n",
    "                    writer, \n",
    "                    sheet_name='Replicate Details',\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                # Auto-adjust column widths for both sheets\n",
    "                for sheet in writer.sheets.values():\n",
    "                    for column in sheet.columns:\n",
    "                        max_length = 0\n",
    "                        column = [cell for cell in column if cell.value is not None]\n",
    "                        for cell in column:\n",
    "                            try:\n",
    "                                if len(str(cell.value)) > max_length:\n",
    "                                    max_length = len(str(cell.value))\n",
    "                            except:\n",
    "                                pass\n",
    "                        adjusted_width = (max_length + 2)\n",
    "                        sheet.column_dimensions[column[0].column_letter].width = adjusted_width\n",
    "            \n",
    "            # Get the Excel file content\n",
    "            excel_content = output.getvalue()\n",
    "            output.close()\n",
    "            \n",
    "            return excel_content\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error exporting data: {str(e)}\")\n",
    "            return None\n",
    "            \n",
    "    def _export_protein_data(self):\n",
    "        \n",
    "        if self.merged_df is None:\n",
    "            return False\n",
    "        \n",
    "        # First, update the protein list to plot\n",
    "        self.pro_list = list(set(self.merged_df['protein_name']))\n",
    "        \n",
    "        df = self.merged_df.copy()\n",
    "        absorbance_cols = []\n",
    "        selected_groups = []\n",
    "        # Get Absorbance columns based on selected groups\n",
    "        for _, value in self.group_data.items():\n",
    "            grouping_variable = value['grouping_variable']\n",
    "            abundance_columns = value['abundance_columns']        \n",
    "            selected_groups.append(grouping_variable)\n",
    "            if grouping_variable != abundance_columns:\n",
    "                absorbance_cols.append(f'Avg_{grouping_variable}')\n",
    "            else:\n",
    "                absorbance_cols.append(abundance_columns)\n",
    "        df['Total_Absorbance'] = df[absorbance_cols].sum(axis=1).astype(int)\n",
    "        \n",
    "        # Filter out zero Absorbance entries\n",
    "        result_df = df[['unique ID', 'Total_Absorbance']]\n",
    "        result_df = result_df[result_df['Total_Absorbance'] == 0]\n",
    "        all_zero_list = list(result_df['unique ID'])\n",
    "        peptides_df = df[~df['unique ID'].isin(all_zero_list)]\n",
    "\n",
    "        # Process protein positions and create proteins DataFrame\n",
    "        additional_columns = ['Master Protein Accessions', 'unique ID']\n",
    "        selected_columns = additional_columns + absorbance_cols\n",
    "        \n",
    "        peptides_df.loc[:, 'Master Protein Accessions'] = peptides_df['Master Protein Accessions']\n",
    "        \n",
    "        temp_df = peptides_df.copy()\n",
    "        temp_df.loc[:, 'Protein_ID'] = temp_df['Master Protein Accessions']\n",
    "        \n",
    "        # Create proteins DataFrame with selected columns\n",
    "        self.proteins_df = temp_df.groupby('Protein_ID').agg(\n",
    "            {**{col: 'first' for col in ['Master Protein Accessions']},\n",
    "            **{col: 'sum' for col in absorbance_cols}}\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Calculate relative Absorbance for selected groups\n",
    "        for col in absorbance_cols:\n",
    "            col_sum = self.proteins_df[col].sum()\n",
    "            if col_sum > 0:  # Avoid division by zero\n",
    "                self.proteins_df[f'Rel_{col}'] = (self.proteins_df[col] / col_sum) * 100\n",
    "            else:\n",
    "                self.proteins_df[f'Rel_{col}'] = 0\n",
    "                \n",
    "        # Create sum DataFrame for selected groups\n",
    "        self.sum_df = pd.DataFrame({\n",
    "            'Sample': absorbance_cols,\n",
    "            'Total_Sum': [self.proteins_df[col].sum() for col in absorbance_cols]\n",
    "        })\n",
    "        \n",
    "        # Add protein descriptions\n",
    "        name_list = []\n",
    "        for _, row in self.proteins_df.iterrows():\n",
    "            if ',' in row['Protein_ID']:\n",
    "                strrow = row['Protein_ID'].split(',')\n",
    "                named_combo = self._fetch_protein_names('; '.join(strrow))\n",
    "            else:\n",
    "                named_combo = self._fetch_protein_names(row['Protein_ID'])\n",
    "            name_list.append(named_combo)\n",
    "        \n",
    "        # Drop the 'Protein_ID' column\n",
    "        self.proteins_df = self.proteins_df.drop(columns=['Protein_ID'])    \n",
    "        \n",
    "        self.proteins_df['Description'] = name_list\n",
    "        self.proteins_df['Description'] = self.proteins_df['Description'].astype(str).str.replace(r\"['\\['\\]]\", \"\", regex=True)\n",
    "        \n",
    "        # Calculate average Absorbance for sorting using only selected groups\n",
    "       \n",
    "        # Calculate sum of all selected columns\n",
    "        total_sum = self.proteins_df[absorbance_cols].sum().sum()\n",
    "        \n",
    "        # Calculate row sums\n",
    "        row_sums = self.proteins_df[absorbance_cols].sum(axis=1)\n",
    "        \n",
    "        # Calculate relative percentage contribution\n",
    "        self.proteins_df['avg_absorbance_all'] = (row_sums / total_sum * 100).round(2)\n",
    "        \n",
    "        # Sort proteins by abundance for consistent ordering\n",
    "        self.proteins_df = self.proteins_df.sort_values('avg_absorbance_all', ascending=False)\n",
    "                                \n",
    "        # Create a dictionary to store the actual peptide counts per group\n",
    "        self.peptide_count_totals = {}\n",
    "        \n",
    "        # Dictionary to store unique peptide counts per protein\n",
    "        self.protein_peptide_counts = {}\n",
    "        \n",
    "        # Track which peptides belong to which proteins\n",
    "        protein_to_peptides = defaultdict(set)\n",
    "        \n",
    "        # Track which peptides belong to which proteins in each group\n",
    "        protein_to_group_peptides = defaultdict(lambda: defaultdict(set))\n",
    "        \n",
    "        # Determine counts based on merged_df and add to proteins_df\n",
    "        if selected_groups and self.proteins_df is not None and df is not None:\n",
    "            # Add count columns to the proteins_df (initialize with zeros)\n",
    "            for group in selected_groups:\n",
    "                count_col = f'Count_{group}'\n",
    "                rel_count_col = f'Rel_Count_{group}'\n",
    "                # Initialize with float64 dtype\n",
    "                self.proteins_df[count_col] = pd.Series(dtype='float64')\n",
    "                self.proteins_df[rel_count_col] = pd.Series(dtype='float64')\n",
    "                # Set initial values to 0.0\n",
    "                self.proteins_df[count_col] = 0.0\n",
    "                self.proteins_df[rel_count_col] = 0.0\n",
    "            \n",
    "            # Create a mapping from accession to protein index in proteins_df\n",
    "            accession_to_idx = {}\n",
    "            accession_to_description = {}  # Map accessions to descriptions for counting\n",
    "            for idx, row in self.proteins_df.iterrows():\n",
    "                if 'Master Protein Accessions' in row and pd.notna(row['Master Protein Accessions']):\n",
    "                    accession_to_idx[row['Master Protein Accessions']] = idx\n",
    "                    accession_to_description[row['Master Protein Accessions']] = row['Description']\n",
    "                elif 'Accession' in row and pd.notna(row['Accession']):\n",
    "                    accession_to_idx[row['Accession']] = idx\n",
    "                    accession_to_description[row['Accession']] = row['Description']\n",
    "            \n",
    "            # For each group, count peptides per protein\n",
    "            for group in selected_groups:\n",
    "                # Filter peptides that are present in this group\n",
    "                group_peptides = df[df[f'Avg_{group}'] > 0]\n",
    "                \n",
    "                # Store the total number of peptides for this group\n",
    "                self.peptide_count_totals[group] = len(group_peptides)\n",
    "                \n",
    "                # Track which peptides have already been counted\n",
    "                counted_peptides = set()\n",
    "                \n",
    "                # Track warning stats\n",
    "                peptides_with_no_accession = 0\n",
    "                peptides_with_no_id = 0\n",
    "                peptides_already_counted = 0\n",
    "                peptides_with_multi_accessions = set()\n",
    "                peptides_with_no_protein_match = 0\n",
    "                \n",
    "                # Count peptides for each protein\n",
    "                for _, peptide in group_peptides.iterrows():\n",
    "                    if 'Master Protein Accessions' not in peptide or pd.isna(peptide['Master Protein Accessions']):\n",
    "                        peptides_with_no_accession += 1\n",
    "                        continue\n",
    "                        \n",
    "                    # Get unique peptide ID to track counting\n",
    "                    peptide_id = peptide.get('unique ID', None)\n",
    "                    if peptide_id is None or pd.isna(peptide_id):\n",
    "                        peptides_with_no_id += 1\n",
    "                        continue  # Skip if no unique ID\n",
    "                    \n",
    "                    # Skip if we've already counted this peptide for this group\n",
    "                    if peptide_id in counted_peptides:\n",
    "                        peptides_already_counted += 1\n",
    "                        continue\n",
    "                    \n",
    "                    accession = peptide['Master Protein Accessions']\n",
    "                    found_match = False\n",
    "                    \n",
    "                    # Check if this peptide maps to multiple proteins\n",
    "                    if ';' in accession:\n",
    "                        peptides_with_multi_accessions.add(peptide_id)\n",
    "                        accessions = [acc.strip() for acc in accession.split(';') if acc.strip()]\n",
    "                        \n",
    "                        # Only count for the first valid protein in the list\n",
    "                        for acc in accessions:\n",
    "                            if acc in accession_to_idx:\n",
    "                                idx = accession_to_idx[acc]\n",
    "                                count_col = f'Count_{group}'\n",
    "                                self.proteins_df.at[idx, count_col] += 1\n",
    "                                \n",
    "                                # Add this peptide to the protein's set for protein-specific counting\n",
    "                                protein_desc = accession_to_description.get(acc, acc)\n",
    "                                protein_to_peptides[protein_desc].add(peptide_id)\n",
    "                                protein_to_group_peptides[protein_desc][group].add(peptide_id)\n",
    "                                \n",
    "                                counted_peptides.add(peptide_id)  # Mark as counted\n",
    "                                found_match = True\n",
    "                                break  # Count only once\n",
    "                    else:\n",
    "                        # Handle direct match - only single protein\n",
    "                        if accession in accession_to_idx:\n",
    "                            idx = accession_to_idx[accession]\n",
    "                            count_col = f'Count_{group}'\n",
    "                            self.proteins_df.at[idx, count_col] += 1\n",
    "                            \n",
    "                            # Add this peptide to the protein's set for protein-specific counting\n",
    "                            protein_desc = accession_to_description.get(accession, accession)\n",
    "                            protein_to_peptides[protein_desc].add(peptide_id)\n",
    "                            protein_to_group_peptides[protein_desc][group].add(peptide_id)\n",
    "                            \n",
    "                            counted_peptides.add(peptide_id)  # Mark as counted\n",
    "                            found_match = True\n",
    "                    \n",
    "                    # Track peptides that didn't match any protein in our list\n",
    "                    if not found_match:\n",
    "                        peptides_with_no_protein_match += 1\n",
    "                        \n",
    "                # After counting all peptides for this group, calculate relative counts\n",
    "                count_col = f'Count_{group}'\n",
    "                rel_count_col = f'Rel_Count_{group}'\n",
    "                total_value = self.peptide_count_totals[group]\n",
    "                \n",
    "                # Calculate relative counts as percentages of total peptides\n",
    "                # When calculating relative counts\n",
    "                if total_value > 0:\n",
    "                    for idx in range(len(self.proteins_df)):\n",
    "                        protein_count = float(self.proteins_df.at[idx, count_col])  # Ensure float\n",
    "                        rel_value = (protein_count / total_value) * 100\n",
    "                        self.proteins_df.at[idx, rel_count_col] = rel_value\n",
    "\n",
    "                \n",
    "                # Display warning about peptides mapping to multiple proteins\n",
    "                warning_html = '<div style=\"color: orange; margin: 5px 0;\"><b>Warning:</b> Peptide counting stats for group {0}:<br>'\n",
    "                \n",
    "                if peptides_with_no_accession > 0:\n",
    "                    warning_html += f'â¢ Skipped {peptides_with_no_accession} peptides with no accession<br>'\n",
    "                    \n",
    "                if peptides_with_no_id > 0:\n",
    "                    warning_html += f'â¢ Skipped {peptides_with_no_id} peptides with no unique ID<br>'\n",
    "                    \n",
    "                if peptides_already_counted > 0:\n",
    "                    warning_html += f'â¢ Skipped {peptides_already_counted} duplicate peptides (already counted)<br>'\n",
    "                    \n",
    "                if len(peptides_with_multi_accessions) > 0:\n",
    "                    warning_html += f'â¢ Found {len(peptides_with_multi_accessions)} peptides mapping to multiple proteins<br>'\n",
    "                    warning_html += f'  (Each counted only once for the first matching protein)<br>'\n",
    "                    \n",
    "                if peptides_with_no_protein_match > 0:\n",
    "                    warning_html += f'â¢ {peptides_with_no_protein_match} peptides had no matching protein in the protein list<br>'\n",
    "                    \n",
    "                total_peptides = len(group_peptides)\n",
    "                warning_html += f'â¢ Total peptides processed: {total_peptides}, successfully counted: {len(counted_peptides)}'\n",
    "                warning_html += '</div>'\n",
    "                \n",
    "                #display(HTML(warning_html.format(group)))\n",
    "\n",
    "        # Calculate the number of unique peptides per protein\n",
    "        for protein, peptides in protein_to_peptides.items():\n",
    "            self.protein_peptide_counts[protein] = len(peptides)\n",
    "\n",
    "        # Create a copy of the proteins DataFrame for protein sample distribution calculation\n",
    "        working_df = self.proteins_df.copy()\n",
    "        \n",
    "        # Calculate protein distributions across samples (for both counts and absorbance)\n",
    "        self.protein_sample_distribution = {}\n",
    "        \n",
    "        # Calculate data for major proteins (based on pro_list)\n",
    "        major_proteins = []\n",
    "        if hasattr(self, 'pro_list') and self.pro_list:\n",
    "            major_proteins = self.pro_list\n",
    "            \n",
    "        # Add \"Minor Proteins\" data structures to hold aggregated values\n",
    "        minor_proteins_data = {\n",
    "            'counts': {group: 0 for group in selected_groups},\n",
    "            'count_relative': {group: 0 for group in selected_groups},\n",
    "            'absorbance': {group: 0 for group in selected_groups},\n",
    "            'absorbance_relative': {group: 0 for group in selected_groups},\n",
    "            'unique_peptide_count': 0,\n",
    "            'total_value': 0,\n",
    "            'total_absorbance': 0,\n",
    "            'total_count': 0\n",
    "        }\n",
    "        \n",
    "        # Counts to track minor proteins' peptides\n",
    "        minor_proteins_peptides = set()\n",
    "        \n",
    "        # Process each protein\n",
    "        for _, row in working_df.iterrows():\n",
    "            protein_name = row['Description']\n",
    "            \n",
    "            # Skip if protein name is empty or NaN\n",
    "            if pd.isna(protein_name) or not protein_name:\n",
    "                continue\n",
    "            \n",
    "            # Initialize data structure for this protein\n",
    "            protein_data = {\n",
    "                'counts': {},\n",
    "                'count_relative': {},\n",
    "                'absorbance': {},\n",
    "                'absorbance_relative': {},\n",
    "                'unique_peptide_count': 0\n",
    "            }\n",
    "            \n",
    "            # Get count values for each group\n",
    "            count_values = {}\n",
    "            absorbance_values = {}\n",
    "            \n",
    "            for group in selected_groups:\n",
    "                # Get count values from proteins_df\n",
    "                count_col = f'Count_{group}'\n",
    "                if count_col in row:\n",
    "                    count_values[group] = row[count_col]\n",
    "                else:\n",
    "                    count_values[group] = 0\n",
    "                \n",
    "                # Get absorbance values\n",
    "                absorbance_col = f'Avg_{group}'\n",
    "                if absorbance_col in row:\n",
    "                    absorbance_values[group] = row[absorbance_col]\n",
    "                else:\n",
    "                    absorbance_values[group] = 0\n",
    "            \n",
    "            # Get the actual count of unique peptides for this protein (across all groups)\n",
    "            if protein_name in protein_to_peptides:\n",
    "                protein_data['unique_peptide_count'] = len(protein_to_peptides[protein_name])\n",
    "            \n",
    "            # Store the count and absorbance values\n",
    "            protein_data['counts'] = count_values\n",
    "            protein_data['absorbance'] = absorbance_values\n",
    "            \n",
    "            # Calculate totals as sums across groups\n",
    "            protein_total_count = sum(count_values.values())\n",
    "            protein_total_absorbance = sum(absorbance_values.values())\n",
    "            \n",
    "            protein_data['total_count'] = protein_total_count\n",
    "            protein_data['total_absorbance'] = protein_total_absorbance\n",
    "            \n",
    "            # Calculate relative distributions\n",
    "            # Count relative distribution - percentage of this protein's total count in each group\n",
    "            if protein_total_count > 0:\n",
    "                for group, count in count_values.items():\n",
    "                    protein_data['count_relative'][group] = (count / protein_total_count) * 100\n",
    "            else:\n",
    "                for group in selected_groups:\n",
    "                    protein_data['count_relative'][group] = 0\n",
    "            \n",
    "            # Absorbance relative distribution\n",
    "            if protein_total_absorbance > 0:\n",
    "                for group, absorbance in absorbance_values.items():\n",
    "                    protein_data['absorbance_relative'][group] = (absorbance / protein_total_absorbance) * 100\n",
    "            else:\n",
    "                for group in selected_groups:\n",
    "                    protein_data['absorbance_relative'][group] = 0\n",
    "            \n",
    "            # Add backward compatibility\n",
    "            use_count = hasattr(self, 'abs_or_count') and ('count' in getattr(self, 'abs_or_count').value.lower() \n",
    "                                                        if hasattr(getattr(self, 'abs_or_count'), 'value') else True)\n",
    "            \n",
    "            if use_count:\n",
    "                protein_data['total'] = protein_total_count\n",
    "                protein_data['values'] = count_values\n",
    "                protein_data['relative'] = protein_data['count_relative']\n",
    "            else:\n",
    "                protein_data['total'] = protein_total_absorbance\n",
    "                protein_data['values'] = absorbance_values\n",
    "                protein_data['relative'] = protein_data['absorbance_relative']\n",
    "            \n",
    "            # Check if this is a major or minor protein\n",
    "            if major_proteins and protein_name not in major_proteins:\n",
    "                # This is a minor protein - add its data to the minor proteins aggregated data\n",
    "                for group in selected_groups:\n",
    "                    minor_proteins_data['counts'][group] += count_values[group]\n",
    "                    minor_proteins_data['absorbance'][group] += absorbance_values[group]\n",
    "                \n",
    "                # For minor proteins, track both the sum and the unique peptide count\n",
    "                if protein_name in protein_to_peptides:\n",
    "                    minor_proteins_peptides.update(protein_to_peptides[protein_name])\n",
    "                \n",
    "                minor_proteins_data['total_count'] += protein_total_count\n",
    "                minor_proteins_data['total_absorbance'] += protein_total_absorbance\n",
    "            else:\n",
    "                # This is a major protein - store its individual data\n",
    "                self.protein_sample_distribution[protein_name] = protein_data\n",
    "        \n",
    "        # Set unique peptide count for minor proteins\n",
    "        minor_proteins_data['unique_peptide_count'] = len(minor_proteins_peptides)\n",
    "        \n",
    "        # Calculate relative distributions for minor proteins\n",
    "        if minor_proteins_data['total_count'] > 0:\n",
    "            for group in selected_groups:\n",
    "                minor_proteins_data['count_relative'][group] = (minor_proteins_data['counts'][group] / minor_proteins_data['total_count'] * 100)\n",
    "        \n",
    "        if minor_proteins_data['total_absorbance'] > 0:\n",
    "            for group in selected_groups:\n",
    "                minor_proteins_data['absorbance_relative'][group] = (minor_proteins_data['absorbance'][group] / minor_proteins_data['total_absorbance'] * 100)\n",
    "        \n",
    "        # Add backward compatibility for minor proteins\n",
    "        if use_count:\n",
    "            minor_proteins_data['total'] = minor_proteins_data['total_count']\n",
    "            minor_proteins_data['values'] = minor_proteins_data['counts']\n",
    "            minor_proteins_data['relative'] = minor_proteins_data['count_relative']\n",
    "        else:\n",
    "            minor_proteins_data['total'] = minor_proteins_data['total_absorbance']\n",
    "            minor_proteins_data['values'] = minor_proteins_data['absorbance']\n",
    "            minor_proteins_data['relative'] = minor_proteins_data['absorbance_relative']\n",
    "        \n",
    "        # Print debug info for key proteins\n",
    "        debug_proteins = ['Beta-casein']\n",
    "        for protein in debug_proteins:\n",
    "            if protein in self.protein_sample_distribution:\n",
    "                data = self.protein_sample_distribution[protein]\n",
    "                print(\n",
    "                    f\"\\nProtein: {protein}\\n\"\n",
    "                    f\"Total count across all groups: {data['total_count']}\\n\"\n",
    "                    f\"Unique peptide count: {data.get('unique_peptide_count', 'N/A')}\\n\"\n",
    "                    f\"Total absorbance: {data['total_absorbance']:.2e}\\n\"\n",
    "                    f\"Sample distribution (% of protein's total):\\n\"\n",
    "                    f\"Count: {', '.join([f'{g}: {v:.1f}%' for g, v in data['count_relative'].items()])}\\n\"\n",
    "                    f\"Absorbance: {', '.join([f'{g}: {v:.1f}%' for g, v in data['absorbance_relative'].items()])}\\n\"\n",
    "                    f\"Sum of count percentages: {sum(data['count_relative'].values()):.1f}%\"\n",
    "                )\n",
    "                \n",
    "        return True\n",
    "    \n",
    "    def _fetch_protein_names(self, accession_str):\n",
    "        \"\"\"\n",
    "        Fetch protein names from the proteins dictionary.\n",
    "        Returns a list of protein names, using the full protein name.\n",
    "        \"\"\"\n",
    "        names = []\n",
    "        for acc in accession_str.split('; '):\n",
    "            if acc in self.protein_dict:\n",
    "                # Use the full protein name instead of splitting it\n",
    "                name = self.protein_dict[acc]['name']\n",
    "                names.append(name)\n",
    "            else:\n",
    "                names.append(acc)\n",
    "        return names\n",
    "    \n",
    "    def _export_group_correlation_analysis(self, df, group_data):\n",
    "        \"\"\"\n",
    "        Calculate and export correlation analysis to Excel.\n",
    "        Returns bytes of Excel file content.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate cross-group correlations\n",
    "            correlation_results = []\n",
    "            avg_columns = {\n",
    "                group_info['grouping_variable']: f\"Avg_{group_info['grouping_variable']}\"\n",
    "                for group_info in group_data.values()\n",
    "                if f\"Avg_{group_info['grouping_variable']}\" in df.columns\n",
    "            }\n",
    "            \n",
    "            # Create Excel writer buffer\n",
    "            buffer = io.BytesIO()\n",
    "            with pd.ExcelWriter(buffer, engine='openpyxl') as writer:\n",
    "                # Cross-group correlations\n",
    "                for (group1, col1), (group2, col2) in combinations(avg_columns.items(), 2):\n",
    "                    mask = (df[col1] > 0) & (df[col2] > 0)\n",
    "                    if mask.sum() > 1:\n",
    "                        values1 = self.prepare_data(df.loc[mask, col1])\n",
    "                        values2 = self.prepare_data(df.loc[mask, col2])\n",
    "                        correlation = self.calculate_correlation(values1, values2)\n",
    "                        correlation_results.append({\n",
    "                            'Group 1': group1,\n",
    "                            'Group 2': group2,\n",
    "                            'Correlation': round(correlation, 3),\n",
    "                            'Number of Peptides': mask.sum()\n",
    "                        })\n",
    "                \n",
    "                # Create and write cross-group correlation sheet\n",
    "                if correlation_results:\n",
    "                    cross_group_df = pd.DataFrame(correlation_results)\n",
    "                    cross_group_df.to_excel(writer, sheet_name='Cross-Group Correlations', index=False)\n",
    "                \n",
    "                # Calculate summary statistics\n",
    "                summary_stats = {\n",
    "                    'Average Correlation': round(np.mean([r['Correlation'] for r in correlation_results]), 3),\n",
    "                    'Min Correlation': round(min([r['Correlation'] for r in correlation_results]), 3),\n",
    "                    'Max Correlation': round(max([r['Correlation'] for r in correlation_results]), 3),\n",
    "                    'Total Comparisons': len(correlation_results)\n",
    "                }\n",
    "                \n",
    "                # Write summary statistics\n",
    "                pd.DataFrame([summary_stats]).to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "            return buffer.getvalue()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in group correlation analysis: {str(e)}\")\n",
    "    \n",
    "    def _export_replicate_correlation_analysis(self):\n",
    "        \"\"\"\n",
    "        Calculate and export replicate correlation analysis to Excel.\n",
    "        Returns bytes of Excel file content.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            group_data, df = self._convert_group_data_dict()\n",
    "            within_group_correlations = {}\n",
    "            for key, value in group_data.items():\n",
    "                grouping_variable = value['grouping_variable']\n",
    "                abundance_columns = value['abundance_columns']\n",
    "                \n",
    "                data = df[abundance_columns].copy()\n",
    "                data = data[data.gt(0).all(axis=1)]  # Filter for rows where all values > 0\n",
    "                \n",
    "                if len(data) > 1:\n",
    "                    data = self.prepare_data(data)\n",
    "                    \n",
    "                    # Calculate correlation matrix\n",
    "                    method = 'pearson' if self.correlation_type.value == 'Pearson' else 'spearman'\n",
    "                    correlation_matrix = data.corr(method=method)\n",
    "                    \n",
    "                    # Get lower triangle only to avoid redundancy\n",
    "                    lower_triangle = correlation_matrix.where(\n",
    "                        np.tril(np.ones(correlation_matrix.shape), k=-1).astype(bool)\n",
    "                    )\n",
    "                    \n",
    "                    # Create pairs and get correlation values\n",
    "                    pairs = []\n",
    "                    values = []\n",
    "                    for i in range(len(abundance_columns)):\n",
    "                        for j in range(i):\n",
    "                            pair_name = f\"{abundance_columns[j]} vs {abundance_columns[i]}\"\n",
    "                            pairs.append(pair_name)\n",
    "                            values.append(round(lower_triangle.iloc[i,j], 3))\n",
    "                    \n",
    "                    within_group_correlations[grouping_variable] = pd.Series(values)\n",
    "    \n",
    "            # Create Excel file\n",
    "            buffer = io.BytesIO()\n",
    "            with pd.ExcelWriter(buffer, engine='openpyxl') as writer:\n",
    "                if within_group_correlations:\n",
    "                    # Create summary sheet with all groups\n",
    "                    combined_correlation_df = pd.concat(within_group_correlations, axis=1)\n",
    "                    \n",
    "                    # Calculate summary statistics\n",
    "                    min_values = combined_correlation_df.min().round(3)\n",
    "                    max_values = combined_correlation_df.max().round(3)\n",
    "                    mean_values = combined_correlation_df.mean().round(3)\n",
    "                    \n",
    "                    summary_df = pd.DataFrame({\n",
    "                        'Min': min_values,\n",
    "                        'Max': max_values,\n",
    "                        'Average': mean_values\n",
    "                    }).T\n",
    "                    \n",
    "                    # Combine and write summary\n",
    "                    combined_with_summary = pd.concat([combined_correlation_df, summary_df], axis=0)\n",
    "                    combined_with_summary.to_excel(writer, sheet_name='Summary')\n",
    "                    \n",
    "                    # Create individual sheets for each group\n",
    "                    for key, value in group_data.items():\n",
    "                        grouping_variable = value['grouping_variable']\n",
    "                        if grouping_variable in within_group_correlations:\n",
    "                            values = within_group_correlations[grouping_variable]\n",
    "                            pairs = []\n",
    "                            for i in range(len(value['abundance_columns'])):\n",
    "                                for j in range(i):\n",
    "                                    pairs.append(f\"{value['abundance_columns'][j]} vs {value['abundance_columns'][i]}\")\n",
    "                            \n",
    "                            group_df = pd.DataFrame({\n",
    "                                'Pair': pairs,\n",
    "                                'Correlation': values\n",
    "                            })\n",
    "                            group_df.to_excel(writer, sheet_name=grouping_variable, index=False)\n",
    "            \n",
    "            return buffer.getvalue()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in replicate correlation analysis: {str(e)}\")\n",
    "\n",
    "    def summed_peptide_results(self):\n",
    "        group_data, df = self._convert_group_data_dict()\n",
    "            \n",
    "        # Initialize dictionary to store results for all groups\n",
    "        total_peptide_results_dict = {}\n",
    "        \n",
    "        # Use consistent reference to merged dataframe\n",
    "        filtered_df = df.copy()\n",
    "\n",
    "        # Process each group from the simplified group data structure\n",
    "        for _, value in group_data.items():\n",
    "            grouping_variable = value['grouping_variable']\n",
    "            abundance_columns = value['abundance_columns']\n",
    "                \n",
    "            # Calculate total abundance and SEM from the abundance columns\n",
    "\n",
    "            #valid_abundance_cols = [f\"Avg_{col}\" for col in abundance_columns \n",
    "            #                    if f\"Avg_{col}\" in filtered_df.columns]\n",
    "            valid_abundance_cols = abundance_columns\n",
    "            if not valid_abundance_cols:\n",
    "                print(f\"Warning: No valid abundance columns found for group {grouping_variable}\")\n",
    "                continue\n",
    "                        \n",
    "            # Filter for non-zero, non-null values in any abundance column\n",
    "            temp_df = filtered_df[['unique ID'] + valid_abundance_cols].copy()\n",
    "            \n",
    "            # Convert abundance columns to numeric, forcing non-numeric values to NaN\n",
    "            for col in valid_abundance_cols:\n",
    "                temp_df[col] = pd.to_numeric(temp_df[col], errors='coerce')\n",
    "            \n",
    "            # Additional filtering for valid data\n",
    "            valid_data_mask = (\n",
    "                temp_df[valid_abundance_cols].notna().any(axis=1) & \n",
    "                (temp_df[valid_abundance_cols] != 0).any(axis=1) &\n",
    "                temp_df['unique ID'].notna()\n",
    "            )\n",
    "            temp_df = temp_df[valid_data_mask]\n",
    "            \n",
    "            if temp_df.empty:\n",
    "                print(f\"Warning: No valid data for group {grouping_variable}\")\n",
    "                # Add empty results to maintain group in output\n",
    "                total_peptide_results_dict[grouping_variable] = {\n",
    "                    'unique_peptides': 0,\n",
    "                    'total_Absorbance': 0,\n",
    "                    'total_sem': 0,\n",
    "                    'abundance_sem': 0,\n",
    "                    'count_sem': 0,\n",
    "                    'replicate_data': {\n",
    "                        'abundance_columns': valid_abundance_cols,\n",
    "                        'replicate_counts': [0] * len(valid_abundance_cols),\n",
    "                        'replicate_abundances': [0] * len(valid_abundance_cols)\n",
    "                    }\n",
    "                }\n",
    "                continue\n",
    "                \n",
    "            # Rest of the function remains the same...\n",
    "            # Calculate peptide counts for each replicate\n",
    "            replicate_counts = []\n",
    "            for col in valid_abundance_cols:\n",
    "                count = temp_df[temp_df[col].notna() & (temp_df[col] != 0)]['unique ID'].nunique()\n",
    "                replicate_counts.append(count)\n",
    "            \n",
    "            # Calculate mean count and SEM across replicates\n",
    "            if len(replicate_counts) > 1:\n",
    "                count_sem = np.std(replicate_counts, ddof=1) / np.sqrt(len(replicate_counts))\n",
    "            else:\n",
    "                count_sem = 0\n",
    "                \n",
    "            # Calculate abundance statistics\n",
    "            abundances = temp_df[valid_abundance_cols].values.astype(float)\n",
    "            peptide_means = np.nanmean(abundances, axis=1)\n",
    "            total_abundance = np.nansum(peptide_means)\n",
    "            \n",
    "            # Calculate SEM for abundance\n",
    "            peptide_sems = np.nanstd(abundances, axis=1) / np.sqrt(abundances.shape[1])\n",
    "            total_sem = np.sqrt(np.nansum(peptide_sems ** 2))\n",
    "\n",
    "            # Calculate total count for group\n",
    "            all_unique_peptides = temp_df[\n",
    "                (temp_df[valid_abundance_cols] > 0).any(axis=1)\n",
    "            ]['unique ID'].nunique()\n",
    "            \n",
    "            # Store results for this group\n",
    "            total_peptide_results_dict[grouping_variable] = {\n",
    "                'unique_peptides': all_unique_peptides,\n",
    "                'total_Absorbance': total_abundance,\n",
    "                'total_sem': total_sem,\n",
    "                'abundance_sem': total_sem,\n",
    "                'count_sem': count_sem,\n",
    "                'replicate_data': {\n",
    "                    'abundance_columns': valid_abundance_cols,\n",
    "                    'replicate_counts': replicate_counts,\n",
    "                    'replicate_abundances': [temp_df[col].replace(0, np.nan).sum() for col in valid_abundance_cols]\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return total_peptide_results_dict\n",
    "                        \n",
    "    def extract_sequences_by_name(self):\n",
    "        # Create an empty dictionary to store the results\n",
    "        result_dict = {}\n",
    "        \n",
    "        # Get the actual column names from the DataFrame\n",
    "        available_columns = self.merged_df.columns.tolist()\n",
    "        \n",
    "        # Iterate through each group in the group_data dictionary\n",
    "        for group_id, group_info in self.group_data.items():\n",
    "            # Get the grouping variable name and abundance columns\n",
    "            grouping_variable = f\"Avg_{group_info['grouping_variable']}\"\n",
    "            \n",
    "            # Filter sample_ids to only include columns that exist in the DataFrame\n",
    "            if grouping_variable in available_columns:\n",
    "                # Create a mask for peptides that have non-zero values\n",
    "                # Since we're working with a single column, we don't need axis=1\n",
    "                mask = self.merged_df[grouping_variable] > 0\n",
    "                \n",
    "                # Extract the unique IDs for peptides that match this mask\n",
    "                unique_peptides = self.merged_df.loc[mask, 'unique ID'].unique().tolist()\n",
    "                \n",
    "                # Store the list of unique peptides in the result dictionary using the grouping variable name\n",
    "                result_dict[group_info['grouping_variable']] = unique_peptides\n",
    "            else:\n",
    "                print(f\"Warning: No valid columns found for grouping variable {group_info['grouping_variable']}\")\n",
    "                continue\n",
    "        \n",
    "        # Convert the dictionary to a DataFrame\n",
    "        # First, find the maximum length of any list in the dictionary\n",
    "        max_length = max(len(peptides) for peptides in result_dict.values())\n",
    "        \n",
    "        # Create a new dictionary with padded lists\n",
    "        padded_dict = {k: v + [None] * (max_length - len(v)) for k, v in result_dict.items()}\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        result_df = pd.DataFrame(padded_dict)\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def _trigger_download(self, content, filename, mime_type):\n",
    "        \"\"\"Helper method to trigger file download\"\"\"\n",
    "        if isinstance(content, str):\n",
    "            content = content.encode('utf-8')\n",
    "            \n",
    "        b64_data = base64.b64encode(content).decode('utf-8')\n",
    "        file_data = f\"data:{mime_type};base64,{b64_data}\"\n",
    "        \n",
    "        with self.status_output:\n",
    "            self.status_output.clear_output(wait=True)\n",
    "            display(HTML(f\"\"\"\n",
    "                <div id=\"download_{filename}\">\n",
    "                    <a id=\"download_link_{filename}\" \n",
    "                       href=\"{file_data}\" \n",
    "                       download=\"{filename}\"\n",
    "                       style=\"display: none;\"></a>\n",
    "                    <script>\n",
    "                        document.getElementById('download_link_{filename}').click();\n",
    "                        setTimeout(() => {{\n",
    "                            document.getElementById('download_{filename}').remove();\n",
    "                        }}, 1000);\n",
    "                    </script>\n",
    "                </div>\n",
    "            \"\"\"))\n",
    "            display(HTML(f'<div style=\"color: green\">Successfully downloaded {filename}</div>'))\n",
    "\n",
    "    def _handle_mbpdb_download(self, b):\n",
    "        \"\"\"Handle MBPDB results download\"\"\"\n",
    "        try:\n",
    "            if self.mbpdb_df is not None and 'function' in self.mbpdb_df.columns:\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filename = f\"MBPDB_SEARCH_{timestamp}.tsv\"\n",
    "                content = self.mbpdb_df.to_csv(sep='\\t', index=False)\n",
    "                self._trigger_download(content, filename, 'text/tab-separated-values')\n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                self.status_output.clear_output(wait=True)\n",
    "                display(HTML(f'<div style=\"color: red\">Error downloading MBPDB results: {str(e)}</div>'))\n",
    "\n",
    "    def _handle_group_download(self, b):\n",
    "        \"\"\"Handle group data download\"\"\"\n",
    "        try:\n",
    "            if self.group_data:\n",
    "                # Convert enumerated format to simplified format\n",
    "                simplified_data = {}\n",
    "                for _, group_info in self.group_data.items():\n",
    "                    group_name = group_info['grouping_variable']\n",
    "                    abundance_cols = group_info['abundance_columns']\n",
    "                    simplified_data[group_name] = abundance_cols\n",
    "                \n",
    "                # Create filename with timestamp\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filename = f\"Categorical_variable_definitions_{timestamp}.json\"\n",
    "                \n",
    "                # Convert to JSON with indentation\n",
    "                content = json.dumps(simplified_data, indent=4)\n",
    "                \n",
    "                # Trigger download\n",
    "                self._trigger_download(content, filename, 'application/json')\n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                self.status_output.clear_output(wait=True)\n",
    "                display(HTML(f'<div style=\"color: red\">Error downloading group data: {str(e)}</div>'))\n",
    "\n",
    "    def _handle_dataset_download(self, b):\n",
    "        \"\"\"Handle Merged Dataset download\"\"\"\n",
    "        try:\n",
    "            if self.merged_df is not None:\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                filename = f\"Merged_Dataframe_{timestamp}.csv\"\n",
    "                content = self.merged_df.to_csv(index=False)\n",
    "                self._trigger_download(content, filename, 'text/csv')\n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                self.status_output.clear_output(wait=True)\n",
    "                display(HTML(f'<div style=\"color: red\">Error downloading dataset: {str(e)}</div>'))\n",
    "                \n",
    "    def _handle_replicate_correlation_download(self, b):\n",
    "        \"\"\"Handle correlation export button click\"\"\"\n",
    "        try:\n",
    "            if self.merged_df is None or self.group_data is None:\n",
    "                display(HTML('<div style=\"color: red; padding: 10px;\">No data available for correlation analysis.</div>'))\n",
    "                return\n",
    "            \n",
    "            # Get the Excel content\n",
    "            excel_content = self._export_replicate_correlation_analysis()\n",
    "            \n",
    "            # Generate filename\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            correlation_type = self.correlation_type.value.lower()\n",
    "            transform_type = \"log10\" if self.log_transform.value else \"raw\"\n",
    "            filename = f\"correlation_analysis_{correlation_type}_{transform_type}_{timestamp}.xlsx\"\n",
    "            \n",
    "            self._trigger_download(excel_content, filename, 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')    \n",
    "                \n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                self.status_output.clear_output(wait=True)\n",
    "                display(HTML(f'<div style=\"color: red; padding: 10px;\">Error exporting correlations: {str(e)}</div>'))\n",
    "   \n",
    "    def _handle_group_correlation_download(self, b):\n",
    "        \"\"\"Handle correlation export button click\"\"\"\n",
    "        try:\n",
    "            with self.status_output:\n",
    "                self.status_output.clear_output(wait=True)\n",
    "                \n",
    "                if self.merged_df is None or self.group_data is None:\n",
    "                    display(HTML('<div style=\"color: red; padding: 10px;\">No data available for correlation analysis.</div>'))\n",
    "                    return\n",
    "                \n",
    "                # Get the Excel content\n",
    "                excel_content = self._export_group_correlation_analysis(\n",
    "                    self.merged_df,\n",
    "                    self.group_data\n",
    "                )\n",
    "                \n",
    "                if excel_content is None:\n",
    "                    display(HTML('<div style=\"color: red; padding: 10px;\">No correlation data generated.</div>'))\n",
    "                    return\n",
    "                    \n",
    "                # Generate filename\n",
    "                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                correlation_type = self.correlation_type.value.lower()\n",
    "                transform_type = \"log10\" if self.log_transform.value else \"raw\"\n",
    "                filename = f\"group_correlations_{correlation_type}_{transform_type}_{timestamp}.xlsx\"\n",
    "                \n",
    "                # Create base64 encoded string for download\n",
    "                b64_content = base64.b64encode(excel_content).decode()\n",
    "                \n",
    "                self._trigger_download(excel_content, filename, 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')\n",
    "                \n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                self.status_output.clear_output(wait=True)\n",
    "                display(HTML(f'<div style=\"color: red; padding: 10px;\">Error exporting correlations: {str(e)}</div>'))\n",
    "    def _handle_sequence_list_download(self, b):\n",
    "        \"\"\"Handle data export with automatic download\"\"\"\n",
    "        sequences_by_name_df = self.extract_sequences_by_name()\n",
    "        if self.merged_df is not None:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f'List_of_Peptides_by_Sequences_{timestamp}.csv'\n",
    "            csv_content = sequences_by_name_df.to_csv(index=False)\n",
    "            self._trigger_download(csv_content, filename, 'text/csv')\n",
    "        else:\n",
    "            print(\"Please generate the analysis first.\")\n",
    "            \n",
    "    def _handle_summed_peptide_download(self, b):\n",
    "        summed_results = self.summed_peptide_results()\n",
    "        if summed_results is not None:\n",
    "            excel_content = self._export_summed_peptide_data(summed_results)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"summed_peptide_results_{timestamp}.xlsx\"\n",
    "        self._trigger_download(excel_content, filename, 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')\n",
    "   \n",
    "    def _handle_protein_download(self, b):\n",
    "        self._export_protein_data()\n",
    "        \"\"\"Handle data export with automatic download\"\"\"\n",
    "        if self.proteins_df is not None:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f'protein_absorbance_analysis_{timestamp}.csv'\n",
    "            csv_content = self.proteins_df.to_csv(index=False)\n",
    "            self._trigger_download(csv_content, filename, 'text/csv')\n",
    "        else:\n",
    "            print(\"Please generate the analysis first.\")\n",
    "    \n",
    "    def _handle_summed_function_download(self, b):\n",
    "        \"\"\"Handle summed function data download\"\"\"\n",
    "        try:\n",
    "            # Process the data\n",
    "            combined_df, combined_count_df, combined_absorbance_df = self._process_functional_peptide_export_data()\n",
    "\n",
    "            # Create Excel file in memory\n",
    "            output = io.BytesIO()\n",
    "            with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
    "                combined_df.to_excel(writer, sheet_name='combined', index=True)\n",
    "                combined_count_df.to_excel(writer, sheet_name='count', index=True)\n",
    "                combined_absorbance_df.to_excel(writer, sheet_name='absorbance', index=True)\n",
    "            \n",
    "            # Get the value of the BytesIO buffer\n",
    "            excel_data = output.getvalue()\n",
    "            \n",
    "            # Generate filename\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"Processed_mbpdb_results_{timestamp}.xlsx\"\n",
    "            \n",
    "            self._trigger_download(excel_data, filename, 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')\n",
    "                \n",
    "        except Exception as e:\n",
    "            with self.export_output:\n",
    "                self.export_output.clear_output(wait=True)\n",
    "                display(HTML(f'<div style=\"color: red; padding: 10px;\">Error exporting data: {str(e)}</div>'))\n",
    "\n",
    "    def update_data(self, mbpdb_df=None, group_data=None, merged_df=None, protein_dict=None):\n",
    "        \"\"\"Update data and enable/disable buttons accordingly\"\"\"\n",
    "        self.mbpdb_df = mbpdb_df\n",
    "        self.group_data = group_data\n",
    "        self.merged_df = merged_df\n",
    "        self.protein_dict = protein_dict\n",
    "\n",
    "        # Enable/disable buttons based on data availability\n",
    "        # depends on functional data\n",
    "        #self.mbpdb_button.disabled = not (mbpdb_df is not None and 'function' in mbpdb_df.columns)\n",
    "        #self.export_summed_function_data_button.disabled = not (mbpdb_df is not None and 'function' in mbpdb_df.columns)\n",
    "        # depends on group data\n",
    "        self.export_sequence_list_button.disabled = not bool(group_data)\n",
    "        self.export_summed_peptide_results_button.disabled = not bool(group_data)\n",
    "        self.export_group_correlation_button.disabled = not bool(group_data)\n",
    "        #self.export_replicate_correlation_button.disabled = not bool(group_data)\n",
    "        self.group_data_button.disabled = not bool(group_data)\n",
    "        self.log_transform.disabled = not bool(group_data)\n",
    "        self.correlation_type.disabled = not bool(group_data)\n",
    "        # depends on merged data\n",
    "        self.dataset_button.disabled = merged_df is None\n",
    "        self.export_protein_data_button.disabled = merged_df is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b506a35-a853-46ee-b193-3fce79decf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessingController:\n",
    "    def __init__(self, workflow):\n",
    "        self.workflow = workflow  # Store reference to workflow\n",
    "        self.export_manager = ExportManager()\n",
    "        self.data_transformer = self.workflow.data_transformer\n",
    "        self.combiner = CombineAverageDataframes(\n",
    "            self.workflow.data_transformer,\n",
    "            self.workflow.group_processor,\n",
    "            self.workflow.protein_handler\n",
    "        )  # Initialize combiner here\n",
    "        self.merged_df = None\n",
    "        \n",
    "        \n",
    "        # Create processing button\n",
    "        self.process_button = widgets.Button(\n",
    "            description=' Generate/Update Data',\n",
    "            button_style='success',\n",
    "            disabled=True,\n",
    "            icon='refresh',\n",
    "            layout=widgets.Layout(width='300px'),\n",
    "            tooltip='Click to start data processing'\n",
    "        )\n",
    "                \n",
    "        # Create separate output areas\n",
    "        self.process_output = widgets.Output()\n",
    "        self.export_output = widgets.Output()\n",
    "        self.search_output = widgets.Output()\n",
    "        self.stats_output = widgets.Output()\n",
    "        self.grid_output = widgets.Output()\n",
    "        \n",
    "        # Set up button callbacks\n",
    "        self.process_button.on_click(self._on_process_clicked)\n",
    "        \n",
    "        # Initialize export manager with disabled buttons\n",
    "        self.export_manager.update_data(None, None, None)\n",
    "        \n",
    "        # Initialize instructions HTML message\n",
    "        self._initialize_instructions()   \n",
    "        # Initialize tab headers with default \"pending\" status\n",
    "        self.create_headers_for_tabs()\n",
    "\n",
    "        # Set up observers for group data changes\n",
    "        self.workflow.group_processor.group_uploader.observe(self._check_enable_process_button, names='value')\n",
    "        self.workflow.group_processor.add_group_button.on_click(self._check_enable_process_button)\n",
    "        self.workflow.group_processor.no_group_button.on_click(self._check_enable_process_button)\n",
    "        # Add observers for changes that would affect the step 4 message\n",
    "        #self.workflow.data_transformer.mbpdb_uploader.observe(self._check_mbpdb_data_change, names='value')\n",
    "        #self.workflow.data_transformer.mbpdb_results_from_search_placeholder.observe(self._check_mbpdb_data_change, names='value')\n",
    "        \n",
    "\n",
    "    def _check_enable_process_button(self, change=None):\n",
    "        \"\"\"Check if process button should be enabled based on group data\"\"\"\n",
    "        # Enable button if group data exists\n",
    "        has_group_data = bool(self.workflow.group_processor.group_data)\n",
    "        self.process_button.disabled = not has_group_data\n",
    "   \n",
    "    def display_interactive_results(self, df):\n",
    "        \"\"\"Display interactive grid with row search functionality\"\"\"\n",
    "        if df is None:\n",
    "            with self.stats_output:\n",
    "                self.stats_output.clear_output()\n",
    "                display(HTML(\"<b style='color:red;'>No data to display</b>\"))\n",
    "            return\n",
    "        \n",
    "        # Create search widget\n",
    "        search_widget = widgets.Text(\n",
    "            placeholder='Search for data in rows...',\n",
    "            description='Search:',\n",
    "            layout=widgets.Layout(width='300px', height='30px',overflow='hidden'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        def get_column_category(col):\n",
    "            \"\"\"Determine category for each column\"\"\"\n",
    "            if col.startswith('Avg_'):\n",
    "                return 'Average Abundance'\n",
    "            elif hasattr(self.data_transformer, 'mbpdb_results') and \\\n",
    "                not self.data_transformer.mbpdb_results.empty and \\\n",
    "                col in self.data_transformer.mbpdb_results.columns:\n",
    "                return 'MBPDB Search Results'\n",
    "            else:\n",
    "                return 'Peptidomic Data'\n",
    "\n",
    "        # Create multi-level columns while preserving order\n",
    "        column_tuples = [(get_column_category(col), col) for col in df.columns]\n",
    "        \n",
    "        df_display = df.copy()\n",
    "        df_display.columns = pd.MultiIndex.from_tuples(column_tuples)\n",
    "\n",
    "        def create_grid(df_to_display):\n",
    "            grid = DataGrid(\n",
    "                df_to_display, \n",
    "                selection_mode='cell', \n",
    "                editable=False,\n",
    "                layout=widgets.Layout(height='600px')\n",
    "            )\n",
    "            grid.auto_fit_columns = True\n",
    "            grid.base_row_size = 25\n",
    "            grid.base_column_size = 150\n",
    "            grid.auto_fit_params = {'area': 'column', 'padding': 10}\n",
    "            return grid\n",
    "        \n",
    "        def on_search_change(change):\n",
    "            search_term = change['new'].strip()\n",
    "            \n",
    "            # Update stats output\n",
    "            with self.stats_output:\n",
    "                self.stats_output.clear_output()\n",
    "                \n",
    "                if search_term:\n",
    "                    # Filter data based on search term\n",
    "                    str_df = df_display.astype(str)\n",
    "                    mask = str_df.apply(\n",
    "                        lambda row: row.str.contains(search_term, case=False, na=False).any(),\n",
    "                        axis=1\n",
    "                    )\n",
    "                    filtered_df = df_display[mask]\n",
    "                    display(HTML(f'<b style=\"color:green;\">Found {len(filtered_df)} matching rows out of {len(df_display)} total rows</b>'))\n",
    "                else:\n",
    "                    filtered_df = df_display\n",
    "                    display(HTML(f'<b style=\"color:orange;\">No search term entered. Displaying all {len(df_display)} rows</b>'))\n",
    "            \n",
    "            # Always update grid output regardless of search term\n",
    "            with self.grid_output:\n",
    "                self.grid_output.clear_output()\n",
    "                if search_term:\n",
    "                    display(create_grid(filtered_df))\n",
    "                else:\n",
    "                    display(create_grid(df_display))\n",
    "        \n",
    "        # Connect search widget to callback\n",
    "        search_widget.observe(on_search_change, names='value')\n",
    "\n",
    "        # Clear previous outputs\n",
    "        self.search_output.clear_output()\n",
    "        self.stats_output.clear_output()\n",
    "        self.grid_output.clear_output()\n",
    "        \n",
    "        # Display search interface\n",
    "        with self.search_output:\n",
    "            display(search_widget)\n",
    "        \n",
    "        # Display initial stats\n",
    "        with self.stats_output:\n",
    "            display(HTML(f'<b style=\"color:blue;\">Ready to search {len(df_display)} rows of data</b>'))\n",
    "        \n",
    "        # Initialize grid display\n",
    "        with self.grid_output:\n",
    "            display(create_grid(df_display))\n",
    "\n",
    "    def _on_process_clicked(self, b):\n",
    "        # Clear all outputs\n",
    "        self.process_output.clear_output()\n",
    "        self.search_output.clear_output()\n",
    "        self.stats_output.clear_output()\n",
    "        self.grid_output.clear_output()\n",
    "\n",
    "        group_data = self.workflow.group_processor.group_data\n",
    "        with self.process_output:\n",
    "            # Pass the actual data_transformer, not the workflow\n",
    "            self.combiner = CombineAverageDataframes(\n",
    "                self.workflow.data_transformer,\n",
    "                self.workflow.group_processor, \n",
    "                self.workflow.protein_handler\n",
    "            )\n",
    "            self.merged_df = self.combiner.process_data(group_data)\n",
    "            \n",
    "            if self.merged_df is not None:\n",
    "                # Check if functional data exists in the processed data\n",
    "                has_functional_data = (hasattr(self.merged_df, 'function') and \n",
    "                                    not self.merged_df['function'].isna().all())\n",
    "                has_groups = bool(group_data)\n",
    "\n",
    "                # Iterate through each key in the group_data dictionary\n",
    "                has_no_reps = True  # Start with assumption that there are no replicates\n",
    "                for group_id, group_info in group_data.items():\n",
    "                    # Get the grouping variable name and abundance columns\n",
    "                    abundance_columns = group_info['abundance_columns']\n",
    "                    if len(abundance_columns) > 1:\n",
    "                        has_no_reps = False\n",
    "                        break\n",
    "\n",
    "                # Check protein mapping status\n",
    "                protein_mapping_submitted = False\n",
    "                has_multiple_proteins = False\n",
    "                \n",
    "                # Check if protein mapping was submitted\n",
    "                if hasattr(self.workflow.protein_handler, 'submit_button') and \\\n",
    "                hasattr(self.workflow.protein_handler, 'pd_results_cleaned'):\n",
    "                    protein_mapping_submitted = True\n",
    "\n",
    "                    \n",
    "                    df = self.merged_df\n",
    "                    \n",
    "                    # Check Master Protein Accessions column\n",
    "                    if 'Master Protein Accessions' in df.columns:\n",
    "                        has_multiple_in_master = df['Master Protein Accessions'].str.contains(';', na=False).any()\n",
    "                    else:\n",
    "                        has_multiple_in_master = False\n",
    "                        \n",
    "                    # Check Positions in Proteins column\n",
    "                    if 'Positions in Proteins' in df.columns:\n",
    "                        has_multiple_in_positions = df['Positions in Proteins'].str.contains(';', na=False).any()\n",
    "                    else:\n",
    "                        has_multiple_in_positions = False\n",
    "                        \n",
    "                    has_multiple_proteins = has_multiple_in_master or has_multiple_in_positions\n",
    "                \n",
    "                # Update step 4 message to show success\n",
    "                self._update_step4_message(\n",
    "                    processed=True,\n",
    "                    has_functional_data=has_functional_data,\n",
    "                    has_groups=has_groups,\n",
    "                    has_no_reps=has_no_reps,\n",
    "                    protein_mapping_submitted=protein_mapping_submitted,\n",
    "                    has_multiple_proteins=has_multiple_proteins\n",
    "                )\n",
    "                \n",
    "                \n",
    "                # Enable export buttons after successful processing\n",
    "                self.export_manager.update_data(\n",
    "                    mbpdb_df=self.workflow.data_transformer.mbpdb_results if hasattr(self.workflow.data_transformer, 'mbpdb_results') else None,\n",
    "                    group_data=group_data,\n",
    "                    merged_df=self.merged_df,\n",
    "                    protein_dict=self.workflow.data_transformer.protein_dict if hasattr(self.workflow.data_transformer, 'protein_dict') else None,\n",
    "                )\n",
    "                \n",
    "                self.display_interactive_results(self.merged_df)\n",
    "            else:\n",
    "                # Update step 4 message to show failure\n",
    "                self._update_step4_message(\n",
    "                    processed=False,\n",
    "                    has_functional_data=False,\n",
    "                    has_groups=bool(group_data),\n",
    "                    has_no_reps=False,\n",
    "                    protein_mapping_submitted=False,\n",
    "                    has_multiple_proteins=False\n",
    "                )\n",
    "                \n",
    "                display(HTML(f'<b style=\\\"color:red;\\\">Error: No data was processed.</b>'))\n",
    "                # Keep export buttons disabled\n",
    "                self.export_manager.update_data(None, None, None, None)\n",
    "\n",
    "    def _on_export_clicked(self, b):\n",
    "            \"\"\"Handle export button click\"\"\"\n",
    "            with self.export_output:\n",
    "                self.export_output.clear_output(wait=True)\n",
    "                \n",
    "                # Update export manager with current data\n",
    "                self.export_manager.update_data(\n",
    "                    mbpdb_df=self.workflow.data_transformer.mbpdb_results if hasattr(self.workflow.data_transformer, 'mbpdb_results') else None,\n",
    "                    group_data=self.workflow.group_processor.group_data,\n",
    "                    merged_df=self.merged_df,\n",
    "                    protein_dict=self.workflow.data_transformer.protein_dict if hasattr(self.workflow.data_transformer, 'protein_dict') else None,\n",
    "                )\n",
    "                \n",
    "                # Display the export manager\n",
    "                self.export_manager.display()\n",
    "                \n",
    "    def _initialize_instructions(self):\n",
    "        \"\"\"Initialize step four instructions with dynamic updating capability\"\"\"\n",
    "        self.stepfour_base_message = \"\"\"\n",
    "            <div style='padding: 10px; background-color: #f8f9fa; border-left: 5px solid #007bff; margin: 10px 0;'>\n",
    "                <h3>Step 4: Process & Export Data</h3>\n",
    "                <p>Click the <b>Generate/Update Data</b> button to process your peptidomic data with the settings you've configured:</p>\n",
    "                \n",
    "                <details>\n",
    "                    <summary><b>What happens when you generate data</b> (click to expand)</summary>\n",
    "                    <ul style='list-style-type: circle;'>\n",
    "                        <li>Combine your peptidomic data with MBPDB bioactivity information</li>\n",
    "                        <li>Calculate group averages based on your defined categories</li>\n",
    "                        <li>Apply protein mapping decisions to resolve multiple protein hits</li>\n",
    "                        <li>Organize data with standardized column naming</li>\n",
    "                    </ul>\n",
    "                </details>\n",
    "                \n",
    "                <p>Use the <b>Explore & Export Data</b> tabs below to interactively view and download your processed data in various formats.</p>\n",
    "                \n",
    "                <details>\n",
    "                    <summary><b>Available options after processing</b> (click to expand)</summary>\n",
    "                    <ul style='list-style-type: circle;'>\n",
    "                        <li>Merged Dataset: View and download the complete dataset with all peptide data</li>\n",
    "                        <li>Functional Data: Export bioactivity information</li>\n",
    "                        <li>Study Design: Export your study variable definitions</li>\n",
    "                        <li>Correlative Analysis: Calculate and export correlation statistics</li>\n",
    "                        <li>Peptide Summary: Export peptide counts and abundance values by group</li>\n",
    "                        <li>Protein Analysis: Export protein-level distribution across experimental groups</li>\n",
    "                        <li>List of Sequences: Export the peptide sequences by names for each experimental groups</li>\n",
    "                    </ul>\n",
    "                </details>\n",
    "                {status_message}\n",
    "            </div>\n",
    "        \"\"\"\n",
    "        # Start with no status message \n",
    "        self.stepfour_output_html_message = self.stepfour_base_message.format(status_message=\"\")\n",
    "        \n",
    "        self.stepfour_status_output = widgets.Output(\n",
    "            layout=widgets.Layout(\n",
    "                max_width='1000px',\n",
    "                width='100%'\n",
    "            )\n",
    "        )\n",
    "        with self.stepfour_status_output:\n",
    "            display(HTML(self.stepfour_output_html_message))\n",
    "\n",
    "    def _update_step4_message(self, processed=False, has_functional_data=False, has_groups=False,\n",
    "                            has_no_reps=False, protein_mapping_submitted=False, has_multiple_proteins=False): \n",
    "        \"\"\"Update the step four message based on the current state\"\"\"\n",
    "        background_color = \"#f8f9fa\"  # Default light gray background\n",
    "        border_color = \"#007bff\"      # Default blue border\n",
    "        \n",
    "        if processed:\n",
    "            # Data has been processed - show success with green background\n",
    "            background_color = \"#e8f5e9\"  # Light green background\n",
    "            border_color = \"#4caf50\"      # Green border\n",
    "        \n",
    "        # Generate dynamic processing steps list with checkmarks for completed steps\n",
    "        if processed:\n",
    "            steps_html = \"\"\"\n",
    "            <ul style='list-style-type: none;'>\n",
    "                <li>â <b>Combined peptidomic data</b> into a unified <b>Merged Dataset</b></li>\n",
    "            \"\"\"\n",
    "            \n",
    "            # Add checkmark for MBPDB integration if functional data exists\n",
    "            if has_functional_data:\n",
    "                steps_html += \"\"\"\n",
    "                <li>â <b>Merged functional data</b> with peptidomic data</li>\n",
    "                \"\"\"\n",
    "            else:\n",
    "                steps_html += \"\"\"\n",
    "                <li>â¤ <i>No functional data available to integrate</i></li>\n",
    "                \"\"\"\n",
    "                \n",
    "            # Add checkmark for group averages if groups exist\n",
    "            if has_groups:\n",
    "                if has_no_reps:\n",
    "                    steps_html += \"\"\"\n",
    "                    <li>â¤ <i>No group definitions available for averaging, abudnance columns are replicated for \"Avg_\" columns</i></li>\n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    steps_html += \"\"\"\n",
    "                    <li>â <b>Calculated group averages</b> based on defined categories</li>\n",
    "                    <li>â <b>Standardized column naming</b> with grouping information saved to abudnace columns and \"Avg_\" columns added</li>\n",
    "                    \"\"\"\n",
    "            else:\n",
    "                steps_html += \"\"\"\n",
    "                <li>â¤ <i>No group definitions available for averaging</i></li>\n",
    "                <li>â <b>Standardized basic column naming</b></li>\n",
    "                \"\"\"\n",
    "            \n",
    "            # Customize protein mapping message based on status\n",
    "            if has_multiple_proteins:\n",
    "                if protein_mapping_submitted:\n",
    "                    steps_html += \"\"\"\n",
    "                    <li>â¤ <i>Multiple protein are still mapped to shared peptides</i></li>\n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    steps_html += \"\"\"\n",
    "                    <li>â¤ <b>Used default protein mapping</b> <i>Multiple protein are still mapped to shared peptides</i></li>\n",
    "                    \"\"\"\n",
    "            else:\n",
    "                if protein_mapping_submitted:\n",
    "                    steps_html += \"\"\"\n",
    "                    <li>â <b>Protein Mapping Processed</b>  no shared alignment detected - all peptides have unique protein assignments</li>\n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    steps_html += \"\"\"\n",
    "                    <li>â <b>Used default protein mapping</b> no shared alignment detected - all peptides have unique protein assignments</li>\n",
    "                    \"\"\"\n",
    "\n",
    "                \n",
    "            steps_html += \"\"\"\n",
    "            </ul>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # Not processed yet, show pending steps\n",
    "            steps_html = \"\"\"\n",
    "            <ul style='list-style-type: none;'>\n",
    "                <li>â¤ Combine your peptidomic data into a unified dataset</li>\n",
    "            \"\"\"\n",
    "            \n",
    "            if has_functional_data:\n",
    "                steps_html += \"\"\"\n",
    "                <li>â¤ Integrate MBPDB bioactivity information with peptidomic data</li>\n",
    "                \"\"\"\n",
    "            else:\n",
    "                steps_html += \"\"\"\n",
    "                <li>â¤ <i>No functional data available to integrate</i></li>\n",
    "                \"\"\"\n",
    "                \n",
    "            if has_groups:\n",
    "                steps_html += \"\"\"\n",
    "                <li>â¤ Calculate group averages based on defined categories</li>\n",
    "                <li>â¤ Standardize column naming with grouping information</li>\n",
    "                \"\"\"\n",
    "            else:\n",
    "                steps_html += \"\"\"\n",
    "                <li>â¤ <i>No group definitions available for averaging</i></li>\n",
    "                <li>â¤ Standardize basic column naming</li>\n",
    "                \"\"\"\n",
    "            \n",
    "            # Customize protein mapping message based on detected state\n",
    "            if has_multiple_proteins:\n",
    "                if protein_mapping_submitted:\n",
    "                    steps_html += \"\"\"\n",
    "                    <li>â¤ Apply protein mapping decisions to resolve multiple protein hits</li>\n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    steps_html += \"\"\"\n",
    "                    <li>â¤ Use default protein mapping for peptides with multiple protein assignments</li>\n",
    "                    \"\"\"\n",
    "            else:\n",
    "                steps_html += \"\"\"\n",
    "                <li>â¤ Check for multiple protein mappings (none detected currently)</li>\n",
    "                \"\"\"\n",
    "                \n",
    "            steps_html += \"\"\"\n",
    "            </ul>\n",
    "            \"\"\"\n",
    "        # Generate available exports list based on data availability\n",
    "        export_html = \"\"\"\n",
    "        <p><b>Available export options:</b></p>\n",
    "        <ul style='list-style-type: none;'>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Always available if processed\n",
    "        if processed:\n",
    "            export_html += \"\"\"\n",
    "            <li>â <b>Merged Dataset:</b> Complete dataset with all peptide data</li>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            export_html += \"\"\"\n",
    "            <li>â¤ <b>Merged Dataset:</b> Complete dataset with all peptide data</li>\n",
    "            \"\"\"\n",
    "        \n",
    "        # Functional data exports\n",
    "        if processed and has_functional_data:\n",
    "            export_html += \"\"\"\n",
    "            <li>â <b>Functional Data:</b> Functional information avalibile</li>\n",
    "            <li>â <b>Function Summary:</b> Summarized functional categories</li>\n",
    "            \"\"\"\n",
    "            self.export_manager.mbpdb_button.button_style = 'info'\n",
    "            self.export_manager.mbpdb_button.disabled = False\n",
    "            self.export_manager.export_summed_function_data_button.button_style = 'info'\n",
    "            self.export_manager.export_summed_function_data_button.disabled = False\n",
    "        else:\n",
    "            if has_functional_data:\n",
    "                export_html += \"\"\"\n",
    "                <li>â¤ <b>Functional Data:</b> No functional information avalible</li>\n",
    "                <li>â¤ <b>Function Summary:</b> Summarized functional categories</li>\n",
    "                \"\"\"\n",
    "            else:\n",
    "                export_html += \"\"\"\n",
    "                <li>â <b>Functional Data:</b> No bioactivity information available</li>\n",
    "                <li>â <b>Function Summary:</b> No functional categories available</li>\n",
    "                \"\"\"\n",
    "                self.export_manager.export_summed_function_data_button.button_style = 'danger'\n",
    "                self.export_manager.export_summed_function_data_button.disabled = True\n",
    "                self.export_manager.mbpdb_button.button_style = 'danger'\n",
    "                self.export_manager.mbpdb_button.disabled = True\n",
    "        # Group-based exports\n",
    "        if processed and has_groups and not has_no_reps:\n",
    "            export_html += \"\"\"\n",
    "            <li>â <b>Study Design:</b> Your defined study variable groups</li>\n",
    "            <li>â <b>Peptide Summary:</b> Counts and abundance values by group</li>\n",
    "            <li>â <b>Correlation Analysis:</b> Statistics between groups</li>\n",
    "            <li>â <b>Correlation Analysis:</b> Statistics between replicates</li>\n",
    "            <li>â <b>Protein Analysis:</b> Protein distribution across experimental groups</li>\n",
    "            <li>â <b>Sequence Lists:</b> Peptide sequences by experimental groups</li>\n",
    "            \"\"\"\n",
    "            self.export_manager.export_replicate_correlation_button.disabled = False\n",
    "            self.export_manager.export_replicate_correlation_button.button_style = 'info'\n",
    "\n",
    "        else:\n",
    "            if has_groups and has_no_reps:\n",
    "                export_html += \"\"\"\n",
    "                <li>â <b>Study Design:</b> Your defined study variable with out groups or replicates declared</li>\n",
    "                <li>â¤ <b>Peptide Summary:</b> Counts and abundance valuesn, SEM data was not generated due to no replicate data</li>\n",
    "                <li>â <b>Correlation Analysis:</b> Statistics between groups</li>\n",
    "                <li>â <b>Correlation Analysis:</b> Statistics between replicates was not generated due to no replicate data</li>\n",
    "                <li>â <b>Protein Analysis:</b> Protein distribution across experimental groups</li>\n",
    "                <li>â <b>Sequence Lists:</b> Peptide sequences by experimental groups</li>\n",
    "                \"\"\"\n",
    "                self.export_manager.export_replicate_correlation_button.disabled=True\n",
    "                self.export_manager.export_replicate_correlation_button.button_style = 'danger'\n",
    "\n",
    "\n",
    "            else:\n",
    "                export_html += \"\"\"\n",
    "                <li>â <b>Study Design:</b> No study variables defined</li>\n",
    "                <li>â <b>Peptide Summary:</b> Requires study variables</li>\n",
    "                <li>â <b>Correlation Analysis:</b> Statistics between groups</li>\n",
    "                <li>â <b>Correlation Analysis:</b> Statistics between replicates</li>\n",
    "                <li>â <b>Protein Analysis:</b> Requires study variables</li>\n",
    "                <li>â <b>Sequence Lists:</b> Requires study variables</li>\n",
    "                \"\"\"\n",
    "        \n",
    "        export_html += \"\"\"\n",
    "        </ul>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add status message based on processing state\n",
    "        if processed:\n",
    "            status_html = \"\"\"\n",
    "            <div style='background-color: #d4edda; padding: 8px; border-radius: 5px; margin-top: 10px;'>\n",
    "                <p style='color: green; margin: 0;'><b>â Data processing completed successfully!</b> Navigate the tables below to view and export your data.</p>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "            #if not has_functional_data and not has_groups:\n",
    "            #    status_html += \"\"\"\n",
    "            #    <p style='color: orange; margin: 0;'>Note: Limited export options available due to missing functional data and study variables.</p>\n",
    "            #    \"\"\"\n",
    "            #elif not has_functional_data:\n",
    "            #    status_html += \"\"\"\n",
    "            #    <p style='color: orange; margin: 0;'>Note: Functional data exports unavailable (no functional data was provided or matched the list of sequences).</p>\n",
    "            #    \"\"\"\n",
    "            #elif not has_groups:\n",
    "            #    status_html += \"\"\"\n",
    "            #    <p style='color: orange; margin: 0;'>Note: Group-based exports unavailable (no study variables defined).</p>\n",
    "            #    \"\"\"\n",
    "            \n",
    "            #status_html += \"\"\"\n",
    "            #</div>\n",
    "            #\"\"\"\n",
    "        else:\n",
    "            if has_groups:\n",
    "                status_html = \"\"\"\n",
    "                <div style='background-color: #fff3e0; padding: 8px; border-radius: 5px; margin-top: 10px;'>\n",
    "                    <p style='color: #ff9800; margin: 0;'><b>â ï¸ Ready to process data.</b> Click the Generate/Update Data button to proceed.</p>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            else:\n",
    "                status_html = \"\"\"\n",
    "                <div style='background-color: #fff3e0; padding: 8px; border-radius: 5px; margin-top: 10px;'>\n",
    "                    <p style='color: #ff9800; margin: 0;'><b>â ï¸ Please define study variables in Step 3 before processing data.</b></p>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "        \n",
    "        # Create the complete HTML with dynamic background color\n",
    "        message_html = f\"\"\"\n",
    "            <div style='padding: 10px; background-color: {background_color}; border-left: 5px solid {border_color}; margin: 10px 0;'>\n",
    "                <h3>Step 4: Process & Export Data</h3>\n",
    "                <p>Click the <b>Generate/Update Data</b> button to process your peptidomic data:</p>\n",
    "                \n",
    "                <div style='margin-left: 15px;'>\n",
    "                    <p><b>Processing steps:</b></p>\n",
    "                    {steps_html}\n",
    "                </div>\n",
    "                \n",
    "                <div style='margin-left: 15px;'>\n",
    "                    {export_html}\n",
    "                </div>\n",
    "                \n",
    "                {status_html}\n",
    "            </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update the message with the new status\n",
    "        self.stepfour_output_html_message = message_html\n",
    "        \n",
    "        # Update the display\n",
    "        with self.stepfour_status_output:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(self.stepfour_output_html_message))\n",
    "    \n",
    "    def create_headers_for_tabs(self):\n",
    "        # Create descriptive headers for each tab\n",
    "        self.merged_dataset_header = \"<p><u>Dataset Viewer & Exporter:</u><br> Download the complete merged dataset with all peptide data, bioactivity information, and group averages. Use the search box to filter and explore your data interactively.</p>\"\n",
    "        \n",
    "        self.functional_data_header = \"<p><u>Bioactivity Data:</u><br> Download information about peptide bioactivity from the MBPDB database, including both raw search results and summarized functional data with quantitative abundance and count values.</p>\"\n",
    "        \n",
    "        self.study_definitions_header = \"<p><u>Study Design:</u><br> Export your study variable definitions showing how samples were organized into experimental groups for analysis and visualization.</p>\"\n",
    "        \n",
    "        self.correlation_data_header = \"<p><u>Correlative Analysis:</u><br> Calculate and export correlation statistics between sample groups or technical replicates. Configure correlation type and data transformation options before exporting.</p>\"\n",
    "        \n",
    "        self.summed_peptide_header = \"<p><u>Peptide Summary:</u><br> Export summary data showing peptide counts and total abundance values for each experimental group, including both group averages and individual replicate details.</p>\"\n",
    "        \n",
    "        self.protein_contribution_header = \"<p><u>Protein Analysis:</u><br> Export protein-level analysis showing the relative contribution and distribution of proteins across your experimental groups based on both peptide counts and abundance values.</p>\"\n",
    "        \n",
    "        self.sequence_list_header = \"<p><u>Peptide Sequence List:</u><br> Export list of detected peptide unique IDs (sequnces + modifications) by your experimental groups for venn diagram generation.</p>\"\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display the user interface\"\"\"\n",
    "        self.create_headers_for_tabs()\n",
    " \n",
    "        tab_children = [\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(self.merged_dataset_header),\n",
    "                self.export_manager.dataset_button,\n",
    "                widgets.HTML(\"<br><u>Search & Explore Dataset</u>\"),\n",
    "                self.search_output,\n",
    "                self.stats_output,\n",
    "                self.grid_output\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(self.functional_data_header),\n",
    "                self.export_manager.mbpdb_button,\n",
    "                self.export_manager.export_summed_function_data_button\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(self.study_definitions_header),\n",
    "                self.export_manager.group_data_button\n",
    "            ]),\n",
    "            widgets.VBox([                \n",
    "                widgets.HTML(self.correlation_data_header),\n",
    "                widgets.HBox([  \n",
    "                    self.export_manager.correlation_type,\n",
    "                    self.export_manager.log_transform\n",
    "                ], layout=widgets.Layout(margin='0 0 15px 0')),\n",
    "                self.export_manager.export_group_correlation_button,\n",
    "                self.export_manager.export_replicate_correlation_button \n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(self.summed_peptide_header),\n",
    "                self.export_manager.export_summed_peptide_results_button\n",
    "            ]),\n",
    "            widgets.VBox([\n",
    "                widgets.HTML(self.protein_contribution_header),\n",
    "                self.export_manager.export_protein_data_button\n",
    "            ]),            \n",
    "            widgets.VBox([\n",
    "                widgets.HTML(self.sequence_list_header),\n",
    "                self.export_manager.export_sequence_list_button\n",
    "            ]),\n",
    "        ]\n",
    "\n",
    "        tab = widgets.Tab(\n",
    "            children=tab_children,\n",
    "            layout=widgets.Layout(width='auto')  # Adjust width as needed\n",
    "        )\n",
    "\n",
    "        tab_titles = [\n",
    "            \"Merged Dataset\",\n",
    "            \"Functional Data\",\n",
    "            \"Study Definitions\",\n",
    "            \"Correlation Data\",\n",
    "            \"Peptide Summations\",\n",
    "            \"Protein Contribution\",\n",
    "            \"List of Sequences\"\n",
    "        ]\n",
    "        for i, title in enumerate(tab_titles):\n",
    "            tab.set_title(i, title)\n",
    "\n",
    "        self.button_container = tab\n",
    "\n",
    "        main_display = widgets.VBox([\n",
    "            self.stepfour_status_output,\n",
    "            self.process_button,\n",
    "            self.process_output,\n",
    "            self.button_container,\n",
    "            self.export_manager.status_output,\n",
    "        ], layout=widgets.Layout(\n",
    "            width='1000px',\n",
    "            max_width='1000px',\n",
    "            padding='5px',\n",
    "            margin='10px 0',\n",
    "            overflow='hidden'\n",
    "        ))\n",
    "\n",
    "        display(main_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77144b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c57da2b5edd9419f84db306d60511c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(layout=Layout(max_width='1000px', width='100%')), VBox(children=(HTML(value='\\n         â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0aff26f1ab4409ab0263abc7596f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(layout=Layout(max_width='1000px', width='100%')), RadioButtons(description='Process peptâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae7f8c08e92456a8d272c60f257a7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(layout=Layout(max_width='1000px', width='100%')), GridspecLayout(children=(VBox(childrenâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6717f4bf0d744d198aba9b7e7456b339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(layout=Layout(max_width='1000px', width='100%')), Button(button_style='success', descripâ¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize workflow and controller\n",
    "userinput_workflow = ProcessingWorkflow()\n",
    "userinput_workflow.display()\n",
    "\n",
    "process_and_export_controller = DataProcessingController(userinput_workflow)\n",
    "process_and_export_controller.display()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1Vt5dV-pTHRWngML-2nDQAR6_P_KFsIx4",
     "timestamp": 1712158917217
    },
    {
     "file_id": "1l7fpCQepyE1pJq2O5QfOHVv9a4VFpr-B",
     "timestamp": 1712094574841
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
